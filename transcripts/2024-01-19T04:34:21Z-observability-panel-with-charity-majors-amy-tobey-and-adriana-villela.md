# Observability Panel with Charity Majors, Amy Tobey, and Adriana Villela.

Published on 2024-01-19T04:34:21Z

## Description

The OTel End User Working Group is kicking off 2024 with this all-star panel moderated by Ana Margarita Medina, featuring ...

URL: https://www.youtube.com/watch?v=Fuy3W5bro9k

## Summary

In this YouTube panel discussion on observability, hosted by an unnamed moderator, experts Adriana Vela (ServiceNow Cloud Observability), Charity Majors (Honeycomb.io), and Amy Tobey (Equinix) delve into the evolving landscape of observability within software development. They explore definitions of observability, emphasizing its role as a property of socio-technical systems rather than a standalone project, and discuss the transition from traditional metrics to more integrated, high cardinality observability tools. Key points include the importance of meaningful data, the necessity for engineers to engage with observability throughout the software lifecycle, and the potential for AI to enhance human capabilities in this field. The panelists also touch on misconceptions around observability, the challenges of sampling and data overload, and the need for better collaboration among engineering, QA, and support teams. Their collective wish for the future includes a deeper understanding of Service Level Objectives (SLOs) and a more holistic integration of observability into development practices.

# Observability Panel Transcript

[Music]

Hello, everyone! We're very excited to have a panel today around the topic of **observability** for the observability community. Here with us today, we have **Adriana**, **Amy**, and **Charity**. We hope to have a fun, engaging conversation. 

### Introductions

**Adriana:**  
Hi, I’m Adriana Vela. I work with Anna at ServiceNow Cloud Observability, formerly known as Lightstep. I'm a Senior Staff Developer Advocate and have been doing the developer thing for almost two years. Before that, I oscillated between individual contributor engineering and management roles. I love observability!

**Charity:**  
I’m Charity Majors, co-founder and CTO of Honeycomb.io. I identify as an Ops engineer, and although it’s been a long time since I held the pager, I still feel like I’ve got half my life on call. I started my first pager rotation when I was 19, so I’ve definitely paid my dues.

**Amy:**  
Hi, I’m Amy Toby, a Senior Principal Engineer at Equinix, the largest data center company in the world. I’ve been doing observability since pretty much my first job back in the late '90s. My current work involves bringing open Telemetry into our operational stack at Equinix Metal, helping us pump metrics out to Honeycomb.

It’s exciting to have you all on the panel today! 

### Definition of Observability

Let’s start with one of the spicy questions: What is your definition of observability? Charity, would you like to kick it off?

**Charity:**  
Sure! Back in the days when observability was still new, we pushed for a technical definition—high cardinality, high dimensionality, explorability, and all that jazz. Nowadays, everyone seems to do observability, even if it’s just gathering some telemetry data. I’ve come around to the idea that observability is a property of a sociotechnical system. It exists on a continuum. 

I think there’s a noticeable difference between observability 1.0 and 2.0. Observability 1.0 relies on metrics, logs, and traces, where you gather data multiple times but don’t connect them effectively. You end up with a lot of data, but no meaningful insights because it’s all siloed. In contrast, observability 2.0 is based on a single source of truth, allowing for high cardinality data and better exploration of that data. 

**Amy:**  
I mostly agree with Charity. In my experience developing a new networking product, we’ve had to include observability features as customers demand them. They’re looking for insights about hyper-complex systems without having to hire expensive people to figure it all out. They want high-quality telemetry that’s easy to consume.

**Adriana:**  
I like the definition I heard recently from Hazel Weekly: “Observability is the ability to ask questions and get meaningful answers.” It emphasizes the importance of collecting relevant data. If you gather a bunch of useless data, you won’t be able to figure out what’s going on. Observability is a design problem, and it takes effort to determine what data is meaningful to you.

### The Role of AI in Observability

Let’s shift gears a bit. How do you think AI can help teams adopt observability?

**Charity:**  
I believe AI can augment humans in the observability pipeline rather than replace them. The opportunity lies in automating correlation and surfacing insights to humans. Instead of eliminating alerts, we can help people do more with less context and expertise.

**Amy:**  
Absolutely! AI can help reduce the cognitive load by summarizing data and highlighting potential issues. It’s about letting computers do what they do best so that humans can focus on what we do best—adding meaning to the data.

**Adriana:**  
I agree! AI can alleviate some of the workload, allowing us to focus on more complex problems. It’s about using AI to support decision-making, not replace it.

### Current Challenges in Observability

What do you think is the current largest challenge in observability?

**Amy:**  
Sampling is a big challenge. We’re generating a wealth of data, and managing that data so it remains meaningful and doesn’t overwhelm our systems is critical.

**Charity:**  
I think many people don’t realize how bad their current state is. They often don’t understand how much better it could be. There’s a need for a mindset shift.

**Adriana:**  
Organizations often get in their own way. They understand the benefits of observability but fail to grasp its implementation fully, leading to missed opportunities.

### Misconceptions About Observability

What do you think is the biggest misconception around observability right now?

**Adriana:**  
People often think observability will solve all their problems without any additional work. They expect magical results without realizing the effort that goes into it.

**Amy:**  
That need for understanding doesn’t disappear; it just becomes more efficient. 

**Charity:**  
Many see observability as a standalone project rather than an integral part of the software development lifecycle. It’s interwoven with everything we do in software development.

### The Future of Observability

As we wrap up, what do you see in store for observability in the upcoming year?

**Amy:**  
I hope to see a deeper understanding of SLAs and SLOs among teams. It’s crucial for effective communication and management.

**Adriana:**  
I want to see more developers and project planners engage with observability, making it a part of the development conversation.

**Charity:**  
I wish for more people to feel the grounding and confidence that comes from seeing telemetry feedback. Observability should enhance our experience in software development.

Thank you all for the amazing insights you’ve shared today! We're looking forward to our next discussion in June. 

Stay warm out there!

[Music]

## Raw YouTube Transcript

[Music] here we are hello hello hello everyone we're very excited to have a panel today around the topic of observability plan for the observability community here with us today we have Adriana Amy and charity joining us we hope to have a fun build conversation would you mind starting out Adriana and introducing yourself who you are and what you do oh yeah so I am Adriana Vela I work with Anna at service now Cloud observability formerly known as lightstep say that three times fast um I uh yeah I'm a senior staff developer Advocate um been doing the devil thing for what almost two years but before that was you know oscillating between like IC um engineering and and management roles so yay and I love observability I knew you before you were a Deo before you in observability yeah yeah yeah when I was just like trying to figure out what the hell this stuff was all [Laughter] about time definitely has passed by charity what about you uh charity major co-founder and CTO of honeycomb.io I identify as an Ops engineer and probably always will even though it's been a long time since I held the pager but I feel like I started I feel like I you know I picked up my first page of rotation when I was 19 so I feel like I've got still like half my life on call so I still I I can still call myself an operations engineer you paid your dues I think so what about you Amy uh hi I'm Amy Toby I am a senior principal engineer at equinex which is a the largest data center company in the world um and I've been doing observability since pretty much my first job as well starting back on Solara systems in like 98 99 um S&M and and then into naio stuff I'm responsible for some things in naos I'm sorry um and like it have been doing it all along these days I do a little bit more kind of work in soot technical space but initial work at equinix was bringing open Telemetry into our entire equinix metal operational stack and pumping those uh metrics out to Honeycomb nice very exciting work that all of you are doing and creating Community around observability and open Telemetry and I get to call you all friends which is super exciting to have you all today on the panel sweet so I think we're going to start with one of the spicy questions and I'm going to kick it off with charity what is your definition of observability the correct one of course we'll see yes spice you know okay so back in the days when nobody else gave a about observability the housian days of I'm saying that even though we almost went out of business many times you know I think we really tried to push this this this idea that there was a technical definition for observability you know that it was high cardinality high dimensionality explorability and all these things and nowadays you know like every everyone and their cat does observability it's it's like if you have any Telemetry you do observability and you know what actually I've come around to this I'm all right I'm all right with this I feel like it's actually a great definition to say that observability is a property of soci technical system so it exists in a Continuum right uh I'm I'm totally down with that because I think that it it it gives a lot of Grace to people who are you know in the early days of their Journey like I don't want to be the person who like well actual you're not really doing observ like nobody likes that guy so I don't want to be that guy um that said I do think that there's a real kind of Step function difference in let's call it observability 1.0 toing that buil built off The Primitives of you know metrics logs and traces um where every time you gather the data you have to you have to gather again in a different format and pay for it again so like you're Gathering it once for metrics again for love again for traces again for APM again for profiling again for security you know like talk about a cost crisis and observability tooling like no wonder right uh and you're very limited in each one of those by The Format that you happen to gathered in and none of them can be connected to each other the only thing that connects them is the poor human sitting in the middle guessing right like guessing eyeball us you know so let's call that 1.0 and then observability like 2.0 tooling I think is based on a single Le single source of truth right these arbitrarily wide structured data blobs you could you can derive metrics you can dve traces you can dve all these things but you've got one source of Truth it handles high cardinality it handles you know it's a more explorable sort of interface so you don't have these static dashboards you have interface where you can ask questions and follow breadcrumb go and then what and then what um so that's my very long-winded answer I think that there's kind of two generations of observability tooling out out there right now and I think that they lead to two very different outcomes like one of the I know I'm taking a very long time here I'm almost done I promise I think that one of the most one of the soot technical factors that you really associate with observability 1.0 is it's for it's like a checklist thing you know you add it at the end before you put stuff in production like cool I can monitor this um and for observably 2.0 I think like the fundamental truth of it is it it underlies the entire software development life cycle and your ability to hook up these fast feedback loops rests on how well you can you know ask questions and explore your data and have this sort of constant conversation going with your code awesome what about you Amy I think I agree with most of what charity said but I've been I've been learning a lot from my my work lately in kind of developing a new networking product and one of the feature sets that we have to include in that today in a modern Network as a service kind of product is observability features and customers demand this and we've been scratching at this like what how much do they need like what how much should we build how much should we hand off to either you know service now or honeycomb you know and let those tools do what they're best at and so what what we've been finding with customers is like that that operational part that charity mentioned right that they are tired of this world where they have to go hire very expensive people who are often cranky um who who who can basically carry that mental burden of tying all the context together with the metrics and and divining what the heck is going on with the network and so what they're asking and what they're demanding from us and all of the other Cloud vendors is how give us the observability Telemetry which is mostly what they ask for because when we talk to customers like most the time when folks were talking to don't want to talk about cardinality um so they're asking for like how do we get all the stuff we need so we can feed it to our my my feelings about a apps aside but this is a common request feed it to our AI Ops tools feed it to our our our other observability systems that we already have so that we can start to leverage the skills we already have to get those insights but it always drives to how do I get insights about this hyper complex system so that we can fix it and get back to business and some sometimes folks don't even want to do the heavy lifting they're just like do it for nobody does right because like almost none of our customers any of us here their primary business is not Telemetry it's undifferentiated lifter they want they got a business to run they don't want to be messing with this stuff so they want me to provide really high quality Telemetry um and then they want you all to make that all super easy to consume yeah that's very true I love that it makes so much sense what is your definition AD so I think I I started out with you know Chari OG definition of observability which carried me well throughout the years and then I I heard one recently from uh Hazel Weekley what which I quite like which is observability is the ability for you to ask questions and get meaningful answers um which I really like because it's you know it's it's up to you as to what what is Meaningful right so what what data are you collecting that's meaningful to you and I think that's really important um in in our in our world um I will still say that I I think like Trace first for observability no matter what um but but I think like we really need to focus on on the meaningful data aspect of observability because like sure you can like instrument the crap out of a system and it's emitting a bunch of useless garbage right you know garbage in garbage out so you don't have good data that you're instrumenting well good luck trying to figure out what the hell is going on you can have the best observability tool out there um it won't do you any good so you're you're going to need um to have I it it takes a bit of you know there's there's that learning curve of of what's what's important to you what should you be collecting to be able to do the thing observability is a design problem like in a in a huge way like once you've s like I feel like we spent spent a long time like just sort of like figuring out the underlying Plumbing which is just a prerequisite to what really matters is this is a design problem because there's so much data and I think Amy and I share very uh similar uh thoughts on AI Ops and and like but like there's two kinds of tools right there are tools that um that pave over complexity and there are tools that try to help equip you to understand it and I will always be in the in the second Camp because at the end of the day we are legally liable for the software that we put out into the world somebody somewhere is going to have to understand that and the harder you've made it the more magical you've made it um the harder that Day of Reckoning is going to be definitely interestingly enough I I think we're we're seeing that a little bit um with um like open Telemetry right like open Telemetry is awesome it's you know this awesome standard uh that pretty much everyone has has aop opted vendor wise um but then now we're we're looking Beyond like that initial adoption of open Telemetry and like people really using it out in the field and and making sure I think open telemetry's biggest challenge I think in in the near future is going to be making making sure that it's something that is easy for onboarding because I think that can be really offputting um for organizations and and that can be really scary and then that leads to the you know you instrument code I don't want to instrument my own code kind of mentality I I think that the the place that the the metrics go really matters though because we talked about a Ops where I see folks really reaching for these tools um charity called it Paving over so what we've done in the network world for decades now and it's still the the case at most organizations is we have all these devices generating gobs of alerts and metrics nobody knows what any of it's doing there's there's a sense that we should be able to correlate all these events so like some Port fails on a a core rou or all the stuff behind it disappears we should be able to roll all that up and I'm using the S the should word the way my therapist calls it the s word right like we we believe these things that should happen but the real world is is is much harder than that so like these these alerts just flood through and people fight these alerts sometimes they've been fighting their whole career trying to get on top of this alert stream and so they reach for these tools to make sense of the morass that they built for themselves because they're not really empowered to go back to the Telemetry and like dial back the cardinality or retransform how they generate those metrics to get higher you know higher cardinality that they can process at the end um often they don't even have choices about what toet Tre they get because they're buying network devices that offer you get your SNMP or you're G and that's what you get oh my God yeah oh God world out there of Legions of people that are just fighting through the noise still um and it hasn't changed for them whereas they're in the software world we keep advancing the state-ofthe-art but it's it's going to be a challenge still to bring a lot of the classic systman and network administrators who are used to this world Along on that journey I feel like one of the characteristics that I often think about when we're talk when we're thinking about like transitioning from the One point0 World to the two point0 World which is going to take for forever um no doubt but but I feel like it's it's it's part of it is generating from a push world to a pull world where we're used to you like the only way you could make sense of like a naous based system is by looking at the cluster of things that just alerted you and go like ah it's probably that right and so you rely on you know all of these alerts but that's so noisy and it does not scale right but like as a as an Ops person I grew up thinking you only look at production when you get alerted you don't have to look at it otherwise and I feel like you know the the tradeoff that we we have to learn to make is okay we're only going to alert you when users are impacted right we set slos these are our agreements with ourselves each other and the world this is the level of service we are going to give you and we alert when user be when when the when when the outage is bad enough that users are being infected but in order to get to that world you have to agree that you're going to go look at your Telemetry affirmatively because most problems will never rise to the level of waking you up God I mean I hope right like most of the things most of the codes you write has a subtle bugs it affect a few people or it's a small thing right so like you in order to have a system in order to have a hygienic system that is not just like a hairball that the cat coughed up you know we can't just keep like in the old days like we were shipping code every day that we didn't really understand these systems we've never really understood and then we wonder why it's giant trash fire right like and the way that we start improving that is by you know not just making you know the Telemetry richer and better but also by committing to closing that loop as a software engineer my job is not done in when the test pass my job is done when I have looked at my Telemetry in production and gone it's doing what I expect it to do nothing else looks weird right so like that's asking it's like I like the technical debt metaphor here because it's asking you to like put in a little bit of effort up front in order to avoid the you know there's this great graph that shows that the cost of finding and fixing bugs and goes up exponentially from the moment that you write them so like you backspace good for you good job right you find it in your test good for you the next best time to find it is right after you deployed that after that God knows how long it's going to take days months years who knows but like the accumulation of that is what makes this this job a nightmare for so many people yeah it's so true like I I I think back to like you know my my old days of of having to hand off deployment instructions to like our Ops folks who theun they had no clue what was you know like what it was they were deploying and and you know I had to pray that my instructions were correct and then I had to pray that they read my instructions correctly and then they would deploy it to Pride and these quote unquote successful deployment like things went correctly was considered a successful project the machine we're done right right yeah and and and so I love the idea of like no like because otherwise it becomes a hit and run deployment right you know or driveby drive by deployment rather is is a better way of putting it where where like you know it's like deployed all right great um let's just leave it to fate but I I think like you know paying attention what's going on as soon as you deploy like there's something to be said because otherwise because you as the person writing the code you have context you had that nobody else in the world has or will ever have you know exactly what you're trying to do why you're doing it what you tried that worked what it what didn't work what the variables were named what the f like you know all this stuff and if you can close that Loop before you have to concept switch another it's so powerful like these feedback loops are what socio Technical Systems are all about like this is the one job of technical leadership in my opinion whether you're an IC or a director or VP one job is these feedback loops to make them as tight and short and functional as possible I think that like nailed down of like engineering responsibility has been so important and observability really brings it to a front but a lot of people are just not paying enough attention or like not willing to do it they're just like my job is to code and that's it and maybe I'm on call twice a year but it's someone else's problem there's not much we can do about that though like a lot of times right like if you are an engineering leader and you're trying to build a dynamic powerful engineering organization you have to either lead those people to care or lead them out right like because like the reality is is like there's the customers that y'all want that that have that engineering leadership and culture of like giving a crap yeah and then there's the vast majority of organizations out there where the most of the people who are writing the code that runs our world are trying to put bread on their table and so as soon as they are done with the with the specification that they were given for their job they e that code into to get and they run because like why should they care why should they metrics but this is this is a self-fulfilling prophecy because you know what makes people like check out tune the out not being connected to the results of their labor like there's something that's like what is it Dan pink says we all want in our job is autonomy Mastery and purpose the purpose of your work if you're so disconnected from it yeah that's all there is to it I'm like I'm not trying to suggest that this is easy or can be done in a one stop you know whatever but I do think that every step we make along this path that tightens up the feedback loops that connects people more with the consequences of their code is good and valuable and pays off interesting oh sorry go ahead I was going to say it's even letting Engineers understand customers like I've worked at many shops and with so many engineers that like that customer impact doesn't matter to them or they're unaware of how the customers are properly using their software or they themselves are not even using the software that they're building that's why like dog fooding is so important I go into help teams all the time where they're you know they're just churning this is like the most common thing that I go work help teams with right they just churning they're not making progress like usually I can guess that without even talking to anybody I can just like look at their J for two minutes and be like yeah they're turning so let's go figure out why right and then you go look and I lost my train of thought I'm sorry people are not connected to their work they're churning right I still forgot where I was going to connect that it's okay it's okay we can move on I I wanted to add one one thought to the mix because you know we we the the overarching theme here is you know like they they deploy and then they go or they write the code and then that's that's it and and I mean ultimately it we are failing the promise of devops at the end of the day right exact we did nothing to solve or to alleviate the problem because we are still continuing to do that yes we've got cicd pipelines that's awesome we've got automation but we still have not fulfilled the promise of devops at the end of the day and I think it's interesting though because I think observability is kind of you know forcing us to face that once again right um because now there like these are the consequences of now we know I I think the opportunity here is now that we have the data to show the business how the consequences of bad Engineering Management are harming our customers like this we didn't have back when we started devops right like we had no freaking idea how how our failed deploys or any of this were impacting our business right we were just guessing we're throwing dark birds and and like some of us were fairly accurate like yeah this is what's happening in the go talk to a customer and they would confirm it great now we can go do devops but there's tons of shops where they just never close that yeah right and then you could bring the data in though and this is one of the things I've been slowly doing right is I go talk to a team be like well the first thing you need to do is go talk to your product manager and I'll help them to find some freaking slos so you know what the acceptable abuse of your customers is because most people don't even know yeah let's start observing that let's start let's put in stick in s on it we don't have to alert or nothing just start looking getting the data now I can go back to the business and I can go tell your director of engineering or your VP or even your SVP and go yell at them be like look at look at this data you need to go invest in this team and change how they work because this is what your customers are going through and and I have actually data to show it and that's that's a game changer that changes the discussion it really does I side not I always get so much energy talking to Amy because like all these things I feel like I'm just like sitting here thinking about like she's out there in the field just doing like over and over and over and I'm just like oh my God you're so cool what talk sound cool but the dayto day is like meetings and waiting for the opportunity go like hey here yeah but the impact I mean I I just I just think it's so cool but like you say we we're failing the promise of devops I actually feel like devops is like it's like it's like the the the Band-Aid that we have on a self inflicted wounds that never should have happened like we never should have separated them from Ops like there's no possible World in which having one set of people write the code and another set of people understand the code makes any sense like and like I get it we were doing the best we could at the time you know we're like this is too complex we got to split this up somehow but like you know I I feel like the entire the entire idea of having different people do these jobs and and it's a little bit unfortunate I think that we've kind of settled on going oh well it's because op sucks and we're all going to be developers like whatever like okay fine I mean most of the time when this happens it's it's like cultural inertia because most of the corporate world they live in this world where they think about okay I have some people who make decisions about the product and then I go and I encapsulate that in some kind of requirements or design docs or whatever and I ship it off overseas to be manufactured so some other people do the labor some stuff shows up in a warehouse I probably never even see it and it and it goes out to distribute blah blah blah blah blah like this is like most the businesses in the world today are some kind of form of this kind of dis breaking the labor away from the design of it and and this is actually I think changing radically out even outside of the software world where where Now product design is speeding up you see kind of things coming faster so now they're starting to look at what we're doing in the devops and going like oh maybe there was something there maybe expertise matters maybe the idea that there's kind of work that isn't knowledge work is  yeah I like that I I think one one thing that um is is also a you know a is to some effect a consequence of of this whole idea of separation of concerns right which um in you see especially in like large Enterprises like when I when I worked at at like a small like me it was like small to mediumsized Startup um you know we I had access to like the freaking PR database and then like I joined a bank right after right after I wrapped up that job and then they're like oh yeah we've got like a DBA for uat and a DBA for prod I'm like H you don't even want you to log into your laptop if yeah yeah that's right to do your job yeah that's changed a lot you know we borrowed so much of that stuff from the accounting world where things like you know the the concept of you shouldn't have the same person file the expense report and approve the expense report totally makes sense you know and I think that there are some Concepts that you know translate nicely and most of them really don't um I actually the most recent talk that I wrote was called um modern engineering is not incompatible with highly secure or like or or compliance environments or something like that yeah and it's just like because it's been and it was super like I've been wanting somebody to write this talk for like 10 years and I finally was like all right this is not my area Jes somebody's got to do this so I sat down and wrote it and I did a bunch of research and it was so interesting um because you know this is all probably old news to y'all but I I I learned a lot about how just like you know it's really Engineers should not have to think about security and compliance in their day-to-day work like if you have to think about it all the time something is very very wrong it should be baked into the architecture so that by default you do the right thing and you can use the accounting and like logging and like auditing principles of like your cicd system and like Engineers should just be do you know doing the work you really only when you need to like spin up a new data source or like build something new then you pull your security folks in you make those decisions from the get-go but like you know it it's just like the entire frame and what and what's so unfortunate is I think that security has this really unfortunate you know the security through security they laugh about but they all do it but it's like they think that talking about how they do things well is going to be bad um and it's not like I think that the only way we're going to make progress in this as an industry is if people start to realize that all their competitors are doing this well and they're about to be outcompeted by the fact that everyone else in the world is is so able to like write off for like a modern Team without being bogged down in the security theater so definitely I know you all touched about Ai and I wanted to to talk to ask a question about it how do you think AI can help like teams adopt observability or like how is it that AI can actually have some benefits to the observability world I think I I've been saying about AI for a few years now right like I think the real opportunities lie in joint cognitive systems research where the way we're using AI isn't necessary to replace humans in in the pipeline it's to augment humans in the pipeline and so for observability I really feel that that the strongest opportunities for AI are in kind of one kind of automated attention on doing correlation basically like Auto autocorrelation and and surfacing that to humans not this idea that I'm gonna eliminate all my cascading alerts through this because that that's a Fool's errand that we've been chasing for at least my entire career um I don't think AI really changes that game but the if we start to think about it instead of this crazy like we're going to make all the alerts go away but instead I'm going to help your people do more with less right less context less expertise um maybe less work um obvious sudden now it gets super duper powerful because that's the hardest problem to solve is how do I reason about this hyper complex system and draw out insights from it that's where I think AI can help by by augmenting the human not replacing it I couldn't agree more it's really about the inputs so much more than the outputs like we are all familiar with like the problems of spam filters right like false positives or brutal you know very high possibility for failure but like there's so much we can do you know honeycom we've been talking about bringing everyone up to the level of the best debugger right increasingly like the hard part is getting detail like these soci Technical Systems are made of just as much of our brains as they are the data like an example I often give is like the New York Times And The Washington Post if you just switch their people overnight nothing would work because so much of the system is in those people's heads and the more we can bring it out of the people's heads and put it somewhere that other people can add to it and ask questions of it you know then it it it like pools our knowledge and our resources like one of the biggest things that blew my mind after becoming a CTO was how many engineering Executives out there trust their vendors more than they trust their employees and they're constantly susceptible to vendors who come up and like so give me $10 million and your people will never have to understand your systems again we will tell you what to look at and we'll tell you what it means and that is and because because people come and go and vendors last forever but like come on like that is just not going to work but there are absolutely things that we can do to make your people more productive uh you know better you know to get better data in to like pull your knowledge I I like I wrote that down joint cognitive research because I think that I think that that's so true you know at honeycom we actually we built a little AI thing Bob which is using uh chat GPT to when you when you deploy some code you can ask questions about it using natural language just like what's slow you know which is super useful because using a query like Explorer is hard for everyone but like just being like what slow what changed is super valuable and it helps those Engineers that like just go around the room and it like like ask a question to the person next to them and it's like you're now winning back time in a way so much time yeah you know I I I agree with with everyone on on the stance on AI because I I think there's this overarching fear um out there where they're like everyone's like AI is gonna take our jobs no I hope so I I don't think we're at Skynet levels uh yet but but also I think like we like personally I like the idea of having you know something take away some of the cognitive load so I can focus on the cooler things and and also take away some of the biases that I might have um assuming that the AI is is is trained without the biases right I guess that's the cave um but you know I I like the idea of like something saying to me hey this is where you look further um you might be interested in blah so so you look and you're like sorry AI you're wrong but thanks for pointing it out anyway um and I think that that can be hugely helpful to just I don't know just take a load off right definitely or like you're getting paged about something it's a database you're not super familiar with and it's like hm something like this happened last year around this time and here's what they did to resolve it would this be useful to you yes exactly it's just your little help on your shoulder that's just telling you all the goodness that could be going on in your system anytime exactly anytime I think about something that I or people spend a lot of time trying to get information out whether that's like combing through data or trying to find stuff in the history or something that's an AI problem like they're so good at that let computers do what computers do best so that humans can do what we do best and I know that like there's this fear that we all have about um you know change and stuff and some jobs are absolutely going to be lost um and but many more will be changed and I think that the way that we make this good for people is not by resisting it fearing it but by being thoughtful about how we how we like involve it and how we adopt it and you know making a change Grace obviously now we're GNA start talking about government policies which is anyway but like but like yeah like Amy said I hope part of my job gets automated away because I could be using the cycles of something else well so there's something else I want to maybe shift to from when you were talking charity one of your favorite topics which is the unknown unknowns because I think AI can eat up most of the known knowns right like we go and ask ask the chat bot and be like hey this seems like it's happened before has this happened before and oh look here's all instent reports and what happened before those are known knowns we we kind of know what to do I think the place where today's efforts anyway we'll see if it changes but I don't think it's going to change drastically in the next decade um are are not are still not aimed at figuring out the the unknown unknown right this is where I think humans are still going to have an edge for a very long time um is basically like we'll use the AI to collate the data to do all the the undifferentiated lifting but it's still going to be people that peer into that space between the data and go I wonder if there's a packet law somewhere in you know deep in this network that's never happened before we've never seen it before and now you have a novel insight and that's the place where today's AI really like it can kind of make things that seem like novel insights because it's it's approximating what a human would do but those actual really tough novel insights I think are going to remain in our domain for a long time they they always come out with these demos that show this these like leaps of things you're like oh I never would have thought to look at that but like those are impressive because they're rare and and at the end of the day if it was the right thing you someone still has to go and like the thing that I keep coming back to that humans are good at is is attaching meaning to things right like any computer can tell you if there's a spike or how big it was but like only a person can come out and go this one really means something because of that that context that the computer won't necessarily always remember what do you think is the current largest challenge in observability I'll start with Amy sampling ask me again in five years it'll probably still be sampling like it's just you know we're generating a wealth of data um yeah you know there's a few places where we could probably tune things up and and generally we're not sampling a lot today but some of the systems that we want to turn on we're going to have to because they do a bunch of this is kind of circling back on like that Tech de under the covers like as soon as we turn on tracing in some of these systems we see that they're doing a bunch of busy work that doesn't make any sense but the first thing that happens is it tips over our our collectors and and overloads our our Upstream account and then we got to go turn it off and so we don't actually get to figure out what the heck is going on so I think like sampling and really just kind of getting control of all of the data we're generating that might not ever have any value and that might is super important because it's it's a by no means an easy problem to solve it's probably the hardest problem in observability right now it's like just how do we get this to a rational on of data so that my cost is rational so I can do more observability get deeper insights into my systems but you got to pay the bill and so like this is continually gonna be our struggle that's the answer of a super user true what do you think is the largest challenge shity um the people don't understand how badly they have it now and how much better it could be people who have spent their careers like slugging it out in the salt mines using metrics managing the overhead of metrics not having high cardinality data dancing around between metrics and logs and they genuinely have no idea that the way they're doing it is the hard way and they don't they don't actually they nobody's optimistic in computers because why would you be optimistic about computers but like until people see their data in our world it doesn't they don't understand how much easier it could be how much better it could be and this there's this mental model you know and and observ 2.0 is dramatically easier than 1.0 because it's just interesting shove it in interesting shove it in there's no like custom metrics to measure or or like manage and farm over time there's none but like the difficulty is that it's a mental model shift that people have to and and I am confident that 10 years from now 15 years from now uh people will be you know using the systems that you know we kind of sketched out a few years ago for observability to understand their systems because the rate of complexity is just going up you know too high we might be out of business by then like we might have been way too early but I am extremely confident that there will be no other way to understand your assistance 10 years from now but using something like this that's fair they are definely not getting any simpler or not and and ironically what that leads to is this increasingly this increas ing Reliance on Heroes and martyrs because if there's nothing knitting together all of your sources of the data except the people with the intuition and the scar tissue who can make good guesses you have to you don't have any other choice I call them loadbearing humans in my God because like when I run into that that's how I talk to the management tree about it I'm like you have a loadbearing human this one person is doing this task and if they go on a vacation or win the lottery we are going to have a bad time right and that's the way to surface the risk is like you wouldn't put a single you know firewall you would always do redundant right like so we need to make at least make that pun and that's usually a good way to start getting over the line and get people realizing that that loadbearing humans are holding the world together yes all over the world and even as our rhetoric has gotten better about this and our understanding has gotten better about this there's still these these Tech it's like the finger in the Dy so to speak you know the little Amsterdam yeah dude just like plugging the Dyke with his God I I got to stop using anecdote so terrible you say out loud but but like these people exist and then they're doing a hero's job but they shouldn't have to and it's bad for everyone it's also putting a lot of trust and retention oh my God very fragile yeah ad what about you what do you think is the biggest challenge um so I think the biggest challenges uh organizations getting in their own way um when it comes to observability um either because they don't they kind of get observability in much the same way that everyone kind of got like agile and devops is like important but they kind of totally missed the point at the same time and so we're you know we've made like a little bit of progress but also not um and and on a similar vein like with open Telemetry again it's like oh I understand the benefits of of observability let's get open telemetry into the OR and then again like not understanding like what they should do with that um either because you know of a vendors um who who oversell right where I we have a tracing tool we can totally take your traces and put them on a little Island by themselves with no food or water and nobody ever freaking looks at them because they're little island yeah we'll totally do that for you we'll take your money too thank you they'll be safe true exactly never use them yeah I you know we we we get uh especially in in large organizations um we we get very um tempted or or or drawn in by by the shiny shiny new thing that seems too good to be true and you're sold on the vaporware or like some reads an article about blah and they speak to a salesperson about at blah company and then all of a sudden they walk in all staryy so I feel like these are such huge barriers to to actual like actually getting done for observability True what would you say is the biggest misconception around observability right now and I'll start with you ad biggest misconception um I think right now I would say like people thinking that it'll solve all their problems without putting in the work I think there's a lot of like people assuming there's a lot of like magical fairy dust that comes in with observability and then when they realized that it's extra work it was like oh crap what about you Amy I I think I'll extend what Adriana said right like it's that extra work of like the the need for understanding doesn't disappear ideally as we get the data presentation into a better place it gets easier to achieve but the need for people to understand the world around them and be able to synthesize new information from from the observability data um that's that doesn't go away it just gets more efficient what do you think about a charity I think I think the biggest misconception I see is just people thinking that observability is its own sort of Standalone project that you can buy or do or check off when in fact it's so interwoven with the development of software iterating on software owning deciding what features you're going to build you know I mean deciding figuring out how your product is working figure out what your users are actually doing why you should care about it where you should spend your time like it has its tentacles everywhere and there are lots of things that we do that make observability better or worse have nothing to do with anything that's marked observability and this is both I think it should be both encouraging and reassuring to people and also a little bit depressing and demoralizing because but like it just it CS for a more holistic view of like this is not I don't think you could have like an observability project that you could just check off it's it requires us to get better at understanding the mission of building something which I think we're doing bit by bit over the years like we're understanding more about how to build software that is better for our users that is better for our Engineers that is more linked to the business you know and and like you were talking earlier about you know there there was this great discussion was kicked off by McKenzie of all people about measuring the work of software whether whether or not it's measurable you know and of course they're completely wrong and which meant that K Beck gave him a Smackdown and Gade did gave him a Smackdown and there's a great discussion about how you measure software development uh but in one of the pieces um someone said that people have got to start realizing that building software is not like building construction it's like it's more like it's more like the design phase and you would never say that a blueprint is better because it has more blue lines right which gets to your point earlier Amy about how this all work is knowledge work right and so I where was I going with all this I don't know I feel like it is it is indivisible from the act of getting better at delivering software so if I I think that a lot God I keep I I swear I've taken up 50% of the air time in this and I really apologize I made must have had too much coffee today but I feel like so many software orgs out there have kind of given up on saying that they're trying to get better at delivering software and so so instead they they they set these goals themselves kind of nibble around the edges we're going to get better at observ villing year we're going to get better at cicd we're going to get better at blah blah blah blah blah but like the great leaps forward that you make that you have to make in order to keep up with complexity because you can't just hire your way linearly out of this you have to make these big leaps and that requires taking a a much you know a much more bird eye view and being like in order to deliver software better this year it requires that we make these investments in observability we make these investments in testing and and and I feel like in conclusion in conclusion building software is an amazing job I thank my my lucky atheist Stars every day that I get to be in this industry not like picking weeds and getting dirt under my finger but like sitting here at a computer solving puzzles and playing with my friends and and I feel like I wish that everyone in software could have as magical of an experience in clear as I have had and I think that observability is a big part of how we get there that's true that actually brings me to my next question as we're getting ready to close up what do you think is in store for observability for this upcoming [Music] year the first person to jump in is okay I'm not gonna pick on one of you oh I haven't been keeping up on all the road maps and stuff but I guess so what is your wish what is your wish that this year brings fors for Ability I guess the thing that I'm working on is a lot of folks still don't really get slos and sis um as much as I think it will help both Engineering Management and Engineers on the ground just communicate better about what they we're all trying to accomplish like it's if I had a wish like it would be to drive kind of that understanding deeper in and just into our industry just so now so I can go into a room and talk about the power that's available here and just like having the metrics to understand and and not just get stared at like I just nailed a fish to the wall right like just you know it would just be nice to to not have to start at zero over and over um but I'll keep doing it until we're there that's fair I personally want to see more people like besides like you know we we we often um associate I guess observability with with like more the Y side of things and it is so not just that right I mean it's it's everybody's problem and so I want to see more conversations where like we get more developers giving a crap about observability and and not just developers like folks who are like planning Tech projects like software releases whatever like make sure it's part of make sure it's part of their language make sure that we Empower our our qas to use everybody misses that give it to your support team gosh like what a opportunity yeah well well even your your QA right like they're they're testing your software and they could use observability to figure out what the hell is wrong and tell developers hey this is why it's wrong you know um so that I want to see I want to see more of that um and and I think it it you know basically it puts us closer to you know Charities observability 2.0 where it's like more part more of a holistic part of the sdlc right I and I think that's where it needs to be so that it's it's you know part of it's baked in baked into how we deliver software and how we code software and and all that good stuff definitely everybody does a better job when they can see where they're going and see what they're doing when they are getting feedback back from the system for sure Amy I had this sticker that says that slos are apis for our engineering teams and and I completely agree like if if you don't have an agreement that you've all agreed upon you're down in the weeds negotiating over every single decision you make about how to spend your time like it's absurdly inex it's absurdly expensive um I I don't I don't know I expect that what's going to happen in 2024 is that everything's going to get even more muddled and people are getting even more confused and um and it's fine it's it's it's entropy it's how it goes um I guess my wish for Christmas would be that um my wish would be that more people just have that feeling in their gut that you get when you see Telemetry feedback to you about what you've done and just go that feeling that you get this like you feel more grounded you feel more certain you feel more confident what you've done observability is the way that we can move swiftly and with confidence and I feel like this is something we all know in our heads we need to learn in our bodies and the only way that we can learn it in our body is by seeing it and experiencing so I would wish that everyone had that experience most definely I love that I love that to keep that in store for 2024 as like find that fuzzy feeling when you start seeing your logs metrics start showing up on your vendor definitely well thank you all very much for the amazing insights you've been able to provide today on our panel around observability this group is going to be coming back later in June to have a little other chat around observability stay tuned to hear about those details later with that we'll like to sign off and say thank you all for watching stay warm out there [Music] yay

