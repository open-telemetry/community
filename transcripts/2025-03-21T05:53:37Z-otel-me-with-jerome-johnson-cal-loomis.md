# OTel Me...with Jerome Johnson  &amp; Cal Loomis

Published on 2025-03-21T05:53:37Z

## Description

Join us as we catch up with Relativity from our first convo with them back in 2022 -- How has the development of OpenTelemetry ...

URL: https://www.youtube.com/watch?v=DrD35XxTDsY

## Summary

In this episode of "Hotel Me," hosts Ree and Adriana, joining from Vancouver and Toronto respectively, discuss open telemetry with guests Cal and Jerome from Relativity, a company that automates legal e-discovery processes. The conversation focuses on Relativity's tech stack, their early adoption of open telemetry, and the challenges and successes they faced in implementing it. They detail their complex observability platform, which serves over 1500 services globally and processes around 10-15 terabytes of data daily. The guests emphasize the cultural shift towards observability within their organization, noting increased acceptance and benefits of open telemetry over time. They also provide feedback on the project's documentation, expressing a desire for improved standardization of telemetry data and greater support for downstream consumers. The episode concludes with mentions of upcoming events and resources related to open telemetry.

# Hotel Me Episode Transcript

[Music] 

Hello! Welcome everyone to a new episode of **Hotel Me**, formerly known as the End User Q&A. I'm very excited to be here. If you have been here before, I am Ree, and I have Adriana here with me. I am coming to you live from Vancouver, Washington, not BC. Adriana, where are you coming from?

I am coming to you live from Toronto, Canada. So, we are at opposite ends of the coast, which is pretty neat.

For those of you who are familiar with the flow, hang tight. We are going to bring our guests on pretty soon. For those of you who are new, welcome! Here’s how this is going to go: we will bring on our two special guests from an end user organization shortly. We will have a quick introduction, and they will introduce us to their tech stack and how long they’ve been using OpenTelemetry. This will be interesting because they were actually one of the first guests we talked to when we started this segment back in 2022. I’m pretty excited to follow up with them and see how far they’ve come in their journey, what struggles they’ve had along the way, and what they’ve figured out.

Then we’ll get into some time for them to give us feedback about the project. We should also have some time at the end for any audience questions. If you have questions that come up as we go, feel free to pop them in the chat, whether you're on LinkedIn or YouTube, and we will get to them as appropriate during the conversation. If not, we will wait until the end.

And with that, I believe we can bring our guests on. Oh, and Adriana, do you want to talk about our resources just a little bit?

Oh, yeah! So, our resources... I believe this will take us to [OpenTelemetry.io](https://opentelemetry.io). We have a page in the Hotel site all about our End User SIG. If you want to learn more about the End User SIG, we welcome you to join. I believe there's a link on there for joining our Slack channel on CNCF Slack. If you're already on CNCF Slack, please come find us; we welcome everyone. People come along to ask questions. We might not have all the answers, but we can definitely direct you to where you need to be.

We would also love to know where you are listening from, so feel free to put that in the chat as well. We just like to know where people are tuning in from.

The guests we have for you today are Jerome Johnson and Calumnus from Relativity. Hello!

Hi, everyone!

Howdy, howdy!

We would love to learn about you. Can you tell us a little bit about your role in the company and what Relativity is focused on? You can go ahead, Cal. Alphabetical order and whatnot.

Oh, okay! Excellent. Very democratic. 

So, a little bit about Relativity: Relativity is a company that predominantly has a SaaS product providing services to the legal community. We automate essentially all the processes around legal e-discovery, including the exchange of documents and all of that. It is a SaaS platform, and we have a lot of companies around the world that actually use it. As part of the observability team, we are really tuned into how our SaaS project is actually working at any particular point in time.

As for me, I started with Relativity about four almost five years ago now. I was brought in basically to upgrade the way we do telemetry. As Ree mentioned, we were one of the first ones to have a chat with you folks and one of the first adopters of OpenTelemetry in production. So, we were very early adopters of OpenTelemetry. I’m an architect here. Now, I’ll let Jerome introduce himself.

Hello! As Cal said, we’re on the observability team. My name is Jerome. I’ve been at the company for about 10 years or so now, but I’ve only been on the observability team, specifically the one dealing with OpenTelemetry, for a little over a year now. Hopefully, I can bring some perspective on how easy it is to use from the internal side, and I’m looking forward to chatting with you guys.

Cool! That's so exciting. This is a great opportunity for us to start digging in. Can either of you or both of you give us some insights into your tech stack and architecture?

Yeah, I could talk about that a bit. Like I said, we were an early adopter of OpenTelemetry. Before that, we had a completely bespoke system for collecting telemetry from a lot of different sources. In addition to that, we did use New Relic at the time, but we also had a lot of other vendors in there too. So, it was kind of a nightmare from an operational standpoint to troubleshoot and debug because you had to go to lots of different sources to sort of put all that telemetry together in your head.

At the time when I was hired, the company made a decision to try to unify everything. We looked around to see what would be the best platform for doing that, and the one that checked all the boxes, even though it was really immature, was OpenTelemetry. So, that was the one we sort of pointed to do that.

Now, internally we have a very complicated platform. We have over 1,500 different services running on our platform, which is a huge mix of things. We are predominantly a Windows and .NET shop, so most of that is actually Windows and .NET. But we have a mix of basically every other language in the world, so we have a very complicated tech stack and need to pull in the telemetry from all those different things.

Most of the complexity in our observability platform, which I’ll probably mention at various points, is just being able to pull those signals in from lots of different sources. We really prefer that people use the OpenTelemetry SDKs now, and we push that as hard as we can. However, we still have people using our old platform, which is Relativity APM, and we translate that into OpenTelemetry now. We pull logging from lots of different places, and that’s probably the part which is most difficult and most heterogeneous in terms of the platform itself.

We have a whole fleet of OpenTelemetry collectors that we run in 20 different regions around the world. Each one of those is load-balanced, so we’re running anywhere between 200 and probably 800 individual collectors around the world to pull all this telemetry in. We then send that to various data stores for operations. This is really New Relic at this point, which is where most of our engineers go to see what the telemetry is doing. We have a subset of people who use LaunchDarkly for future releases, so we send a set of telemetry there. We also archive all of our telemetry for compliance reasons for a year and for forensic analysis. 

We have a reporting platform that uses Azure Blob Storage as its backend to make reporting for managers and things like that. This makes it look simpler than it is; it’s a lot more gnarly than this diagram lets on. But we do have a large scale: 20+ regions around the world and 1,500 services all feeding into this is a fairly complicated platform.

Dang, that was really cool! I have so many questions. I do actually have a lot of follow-up questions, but I also want to get into why OpenTelemetry. I know you mentioned that your team was early adopters back in 2022. How did you come upon OpenTelemetry, and why did you decide that it made sense for you?

I think the thing that ticked all the boxes for us was that we came from a place where we were having to maintain our own observability libraries. Given that we were .NET and then trying to diversify into lots of different languages, it meant that we were having to translate that thing again and again. On top of that, we had a mix of our own libraries for some stuff and a mix of things from New Relic.

One of the complicated parts was that every time we wanted to move something or change something, it meant code changes. That’s something we wanted to avoid going forward. So, we wanted to standardize on something that was ideally open source, that we could include in our services one time and maintain that without having to change the code every time we needed to modify where we put things. OpenTelemetry was the only one that ticked those boxes for us at the time. 

The only concern we had was that it was a new project, so there was a lot of risk in going that direction. It wasn’t an easy decision; we discussed this a long time internally. But we decided that it was worth the risk to go ahead and adopt it because it looked like it was going to meet our needs. Basically, we wanted to isolate our codebase from what we do with the telemetry afterwards, which was the selling point for us.

That is very awesome! I don’t know if Jerome has anything to add.

Oh, yeah! I can give some perspective on what it was like before we had OpenTelemetry. I was serving on the performance team for several years, and it was very challenging to analyze specific applications because, as Cal mentioned, it was kind of the wild west on how we handled metrics. 

Every time we had to analyze a specific application, we would have to do a deep dive into what telemetry they were putting out. Usually, it would take half the time just trying to understand what the product was, and then the latter half would be coming up with a solution. Since joining the observability team, I've been amazed at how much cleaner it is to understand what Relativity is doing as a whole.

As a follow-up question, we are always curious about collector setup in the SIG. What kind of setup do you use for your fleet of collectors? Can you give us an idea of how many collectors you have and how they are managed?

In terms of how things are usually deployed, we use a gateway setup. All of our collectors are basically in a gateway setup, so people send their telemetry to us, we process it, and send it on. Over the years, we’ve had more or less complicated pipelines, but in general, we have a single set of collectors with a common configuration that handles all of that telemetry.

We are running in over 20 regions, and each region has between 10 and 100 instances, depending on the load. So, we’re running at any point in time somewhere between 200 and 500 individual collectors, all running in Kubernetes. We are very focused on compute and Kubernetes, so that’s where almost all these things are running.

In terms of deployment, we handle all of our deployments via Helm charts. We use Helm charts to deploy everything to our Kubernetes clusters. We take care to ensure that all of our configurations are isolated. Our collectors, except in very few cases, do not reach out to external services. We manage the configurations locally to ensure they are as robust as possible.

One of the reasons we went with OpenTelemetry is that we have some internal attribute enrichments to do specifically for our product, and the way we use things. We implement those inside the collector itself, so we have custom processors that enhance the data in various ways. We validate it and drop data that contains things that shouldn’t be there, for instance. We do a lot of validation on the fly as things go through the collectors before we write it out to our various data stores.

I think that covers most of your questions. Did I miss anything?

I think you covered it well. A follow-up question: since you deploy your collectors in Kubernetes via Helm charts, have you used the OpenTelemetry operator to manage any of your collectors? Is that something you’ve played with?

We have not. The OpenTelemetry operator sort of came out after we were already doing things. We don’t have any particular reason for not using it; we consider it tech debt. We had the Helm chart beforehand, and we’ve just continued doing that. We’re kind of used to managing things that way.

We will have an internal shift in our deployment tooling coming up, so that might be an opportunity to switch how we manage things. But at the moment, we don’t have any experience with the OpenTelemetry operator.

Got it. Another similar question: since you manage a fleet of collectors, have you experimented with the OPAMP protocol for managing them?

We haven’t. We’ve tried very hard to make our fleet essentially standalone, so even for configurations and things like that, we don’t reach out to anything else. We deployed static config maps for that. It was a choice we made because we have such a large, distributed deployment; we wanted to be as resilient against networking problems as possible. Even for configuration changes, we push out new configs rather than using a centralized method.

Do you have a dedicated team responsible for maintaining the collector pipeline, or is it just whoever is on the observability team?

Yes, we do have an observability team split between Poland and here. Our other major engineering office is in Krakow, Poland. Each team has around five people at the moment, and collectively we are responsible for maintaining those pipelines and configurations. 

We try to avoid shared responsibility because shared responsibility often means no one is responsible. 

As the project has matured, it sounds like you have a pretty good setup working well for you now. How has the implementation evolved over the years? Have you added additional instrumentation signals?

It’s interesting in that the deployment hasn’t really changed in terms of architecture since the beginning. It’s been the same architecture the entire time. We’ve made tweaks here and there and simplified things as new features have come out, but by and large, it has stayed the same.

We’ve gone through the alpha and beta phases, and things are starting to become stable now. We haven’t noticed any significant changes that we needed to make in the platform. That being said, we have added new signals. Initially, it was just metrics and traces; logs came in afterward. We had logs coming into our system even earlier than that, so we’ve folded those changes into the platform as they come along, and it hasn’t been terribly painful.

In terms of the community, we haven’t seen any negative consequences from the alpha and beta phases. We’ve had to update occasionally, but those updates are usually pretty easy. Jerome has been handling a lot of the following for the collector. We have our own build of the collector, which we try to keep in sync with the external one, and he can probably provide some feedback on that process.

Oh yeah! To be honest, updating the collectors has been a relatively painless process. There have been a few changes that we’ve had to discuss as a team, but overall, pushing out changes has been simple, especially with how detailed the change logs have been. They clearly communicate any breaking changes or deprecations.

I have a follow-up on building your own collectors. I think you might be the ones who have built your own collector. What was your experience doing that, especially with the documentation provided on the OpenTelemetry site? 

We started with our own build of the OpenTelemetry collector because we needed custom receivers, processors, and exporters. Initially, it was very painful; we had to clone the entire repository and make our changes. Once the collector builder came along, it made the process much easier.

Now we have our own repository with a single config file for the collector builder that specifies what we pull in from contrib and the main OpenTelemetry collector. The only thing we have in our repo is our custom stuff. The builder has streamlined the process significantly, and once it was available, everything became easier.

We did that right at the beginning, so I can’t comment on the current documentation. However, the builder works seamlessly for us now. 

Regarding building your own components, how was that experience? Was it tricky? 

I think the biggest hurdle was learning Go. We weren’t initially familiar with it, but now we have good experience with Go, so it’s not an obstacle anymore. In terms of building our own components, we started by copying something that worked and modifying it, which was fairly straightforward. 

The documentation has improved since then, making it easier to start building. However, there are still areas where I would like better documentation, like internal telemetry and how to hook into it better. We had to reverse engineer some aspects, but overall, it was manageable.

We have an audience question from Sumit: How much data size do you consume, and have you found the need to reduce it via tail sampling or other methods?

We typically write between 10 and 15 terabytes a day. The split between logs, traces, and metrics is not even, but we take all of those in. We’ve never had issues with scale; we can handle whatever is thrown at us due to autoscaling on all our collectors.

The place where we’ve had to reduce data is mainly for budget reasons since we push our data into New Relic and pay for what we push there. We manage the volume of data we send to New Relic mainly for budget reasons rather than technical ones. In those cases, we do sample down the traces and manage logs by log levels. Metrics, however, we tend to leave untouched, although we do look into cases of abuse.

Since your organization is supportive of the observability journey and OpenTelemetry adoption, can you talk more about the culture of observability?

When I started, I was brought in to change how we do observability, and a major part of that has been OpenTelemetry. It’s been nice to see that the technical aspects have been the easiest parts. The tech has just worked well.

The harder part has been the cultural changes. There has been some resistance to migration because we have a large code base. However, as people gain experience with OpenTelemetry and see the benefits, there has been less resistance over time. People are starting to lean into OpenTelemetry as a platform.

One thing that has enabled people to do nice new interesting things is that we can write to different places easily. For example, we write telemetry to LaunchDarkly, which allows us to use their release guardian feature. This feature automatically rolls back flags based on telemetry data, which is powerful because it integrates another platform seamlessly into our telemetry without changing our code fundamentally.

Thank you for sharing that! In the interest of time, I want to give you space to share additional feedback that we could share with the project maintainers. You mentioned the issue with conventions for internal data schemas; can you provide more detail on that?

I think it’s important to note that I am fully in support of OpenTelemetry. The maintainers are doing a fantastic job both on the documentation and the code level. This feedback isn’t necessarily about the collector implementations but about our own journey.

As we generate more telemetry, it becomes increasingly useful for various downstream consumers, such as our AI division and managers who need reporting. Standardizing data is becoming more important. We enforce a global schema for the attributes coming into the system, but how this fits with semantic conventions and data contracts is something we are thinking hard about.

Discussions around semantic conventions, the Elastic Common Schema, and how to create data contracts with OpenTelemetry data will become increasingly important. It would be great to have common themes and tools in the community regarding this.

The tech stack is really solid, so I don’t have much feedback on that. Looking forward, it’s crucial to standardize the data produced and make it more useful for downstream consumers.

Thank you! Now, let’s catch up on some audience questions. Doug on YouTube wants to know if cloud providers will offer OpenTelemetry as a boring text, similar to SMTP or SFTP. Is that a crazy idea?

The first question we ask any vendor now is whether they support OTLP. We treat that as our standard protocol, and it’s a negative mark against them if they don’t support it. So, I don’t think that’s a pipe dream; pushing vendors in that direction is a good thing. We have consistently given feedback to Azure that we want Azure Monitor to produce OpenTelemetry signals.

Manas has a question about OpenTelemetry tutorials. We will respond directly with some resources for you in the LinkedIn chat. 

Another question: Did you ever feel there were classes of telemetry data that OpenTelemetry didn’t collect well, requiring you to add another collection method like eBPF?

We had to develop some custom receivers and things that weren’t covered beforehand. One early need was for a webhook into OpenTelemetry, which now exists. The biggest gap we’ve had internally is front-end telemetry. We are currently working on a project to collect more front-end telemetry using the OpenTelemetry JavaScript SDK.

In terms of other telemetry types, we haven’t had significant issues. Most things we needed to collect were either OTLP or could be captured with tools like Splunk, Fluent Bit, etc.

Perfect! Thank you. Our time is just about up. Any last parting words?

Just thanks for inviting us! If anyone wants to reach out, I am available on Slack, so feel free to contact me directly if you have questions or if discussing what we’ve done would be helpful.

Thank you, Cal and Jerome, for being on and sharing your journey with us. We will link to our OpenTelemetry SIG and user channel so you can find us there. You can also reach out to Cal directly via CNCF Slack.

We have a link for you, Manas, that will provide resources for OpenTelemetry documentation. 

Before we wrap up, I want to mention that Hotel Community Day is coming up on June 25th, and we are currently accepting talk proposals. If you would like to submit a talk, we encourage you to submit a topic for OpenTelemetry Day in Colorado, which is part of Open Source Summit North America.

Also, if you are going to or are already in Europe and planning to attend KubeCon EU in London in less than two weeks, we would love to see you there! Adriana and I will be speaking at the event, and we have a blog post about the event that outlines all the OpenTelemetry-specific talks. 

The recordings will be available on the CNCF YouTube channel about one to two weeks after the event. So, if you can make it in person, that’s great, but if not, you can catch the recordings. 

If you are at KubeCon EU, we will be hanging out at the Hotel Observatory off and on, so come say hi. The observatory is always fun, with many OpenTelemetry contributors and maintainers around. 

There will also be project updates at KubeCon about OpenTelemetry, so I strongly encourage folks to join that session to learn what's new and exciting in OpenTelemetry. 

If you enjoyed this recording but have a friend who couldn't make it, you can replay it on our LinkedIn or share the link to this live recording. It will also be available on our OpenTelemetry YouTube channel.

Thank you all so much, and we will see you next time! 

[Music]

## Raw YouTube Transcript

[Music] Hello. Welcome everyone to a new episode of Hotel Me, formerly known as the end user Q&A. I'm very excited to be here. Um, if you have been here before, I am Ree and I have Adriana here with me. I am coming to you live from Vancouver, Washington, not BC. Um, Adriana, where are you coming from? I am coming to you live from Toronto, Canada. So, we're at opposite ends of the coast, which is pretty We are. Yeah. And so, for those of you who are familiar with the flow, um, hang tight. We are going to bring our guests on pretty soon. Um, for those of you who are new, welcome. So, how this is going to go is we are going to bring on our two special guests from a an end user organization on shortly. We're going to have a quick introduction. Um, they'll introduce us to their tech stack and how long they've been using open telemetry. This one's gonna be a really interesting one because they were actually one of the first guests we talked to when we started this segment um back in 2022. So, I'm pretty excited to follow up with them and see, you know, how far they've come in their journey, what, you know, what kind of struggles they've had along the way and um what they've figured out. Um and then we'll get into some time for them to give us some feedback about the project. And we should also have some time at the end for any audience members to ask questions. Um, if you do have questions that come up as we go, feel free to pop them in the chat, whether you're on LinkedIn or YouTube, and we will get to them as appropriate um during the conversation. If not, we will wait until the end. And with that, I believe we can bring our guests on. Oh, and Adrian, do you want to talk about our resources just a little bit? Oh, yeah. So, uh, our resources, um, I believe this will I'm going to scan the QR code myself because I don't remember what this does. Um, this will take us to this takes us to open telemetry.io. Um, and we have a page in the hotel uh, site all about our um, all about our end user SIG. If you want to learn more about the end user SIG, we welcome you to join. I believe there's a a link on there also for joining our uh Slack channel on CNCF Slack. So, if you're already on CNCF Slack, um please come find us. We welcome everyone. Um people come along to ask questions. We might not have all the answers, but we can definitely direct you to where you need to be. Oh, and also we would love to know where you are listening from. So feel free to put that in the chat as well so we can we just like to know where people are listening from. And so the guests we have for you today are Jerome Johnson and columnist from relativity. Hello. Hi everyone. Howdy howdy. Hello. Would you we would love to um learn about you and can you tell us a little bit about your role in the company and kind of what relativity is focused on? You can go ahead, Cal. Alphabetical order and whatnot. Oh, okay. All right. Excellent. Very democratic. Yeah. So, a little bit about relativity. So, Relativity is a company that has a predominantly a SAS product that uh provides services to the legal community. So, we automate essentially all the uh processes around legal eiscocovery. So, exchange of documents and all of that. Um it is a SAS platform. We have a lot of companies around the world that actually use it and uh we as observability are really tuned toward how how the cloud our our SAS project is actually working at any particular point in time. Um as far as me I um I started with relativity about four almost five years ago now. I was brought in basically to upgrade uh the way we do telemetry and as Ree mentioned we were one of the probably the first first ones to have a chat with with you folks but also one of the first adopters of open telemetry in production. So we were very very early adopters of of open telemetry. Um I'm an architect here. Um and then I'll let Jerome introduce himself. Hello. As Cal said, we're on the observability team. My name is Jerome. Um, I've been at the company for about 10 years or so now. Uh, but, uh, I've only been on the observability team, specifically the one that's dealing with open telemetry for a little over a year now. So hopefully I can bring some perspective on like how easy it is to use from an on the internal side and how nice it is to so looking forward to chatting it up with you guys. Cool. That's so exciting. Well, um I think this is a great opportunity for us to uh start digging in and can um can either of you or both of you give us some insights into your tech stack and architecture? Yeah, so I could talk about that a bit and uh you know, so like I said, we were an early adopter of open telemetry. Uh what we had before that was an entirely bespoke I would say uh system for collecting telemetry from a lot of different sources. Um, in addition to that, um, we we did use New Relic at the time, but we also had a lot of other vendors in there, too. So, it was kind of a nightmare from an operational standpoint to actually do any uh troubleshooting and debugging because you had to go to lots of different sources to to sort of put all that telemetry together in your head. Um, at the time when I was hired, we we uh the company made sort of a decision to try to unify everything. We looked around to see what would be the best platform for doing that. Um and the one that sort of checked all the boxes even though it was really immature was open telemetry. So that was the one which we sort of pointed to to do that. Now internally we have a very complicated platform. Um and maybe this is time to bring up the diagram on on what we actually do now with um um with open telemetry. We have 1500 plus different services running on our platform and that is a huge mix of things. We are predominantly a Windows andnet shop. So most of that is actually Windows and .NET. Um but we have a mix of basically every other language in the world. Uh so we have a very very complicated tech stack and we need to pull in the telemetry from all those different things. So most of the complexity in our observability platform and I'll probably mention at various points. So if I say RELI, that is our observability platform that's built over open telemetry. Um most of the complexity is just being able to pull those signals in from lots of different sources and you see most of them here. We really prefer that people use the open telemetry SDKs now and we push that as hard as we can. But we still have people using our old platform which is relatively APM which we translate into open telemetry now. We have things like SNMP. uh we pull logging from lots and lots of different places and that's probably the part which is most most difficult and most uh heterogeneous in terms of the platform itself. We have a whole fleet of uh open telemetry collectors that we run in 20 20 different regions around the world. Each one of those is load balanced. So we're running we're running anywhere between uh depending on the time of day and all that between 200 and probably 800 individual collectors around the world to to pull all this telemetry in. Um and then we we send that to various uh data stores for operations. This is really new relic at this point. Um so that's where most of our uh engineers go to actually see what the what the telemetry is doing. We have a subset of people who use launch darkly and use that for future releases. So we send some set of telemetry there. Um and then we actually archive all of our telemetry for compliance reasons for a year and for forensic analysis and stuff like that. Uh we also have a reporting platform that uses the Azure blob storage basically as as its back end to to make that reporting for managers and things like that. Um so this makes it look simpler than it is. It's a lot more uh gnarly than than this diagram uh uh lets on. But we do have a large scale. I mean 20 plus regions around the world, 1500 services all feeding into this is is a fairly complicated platform. Dang, that was really cool actually. Um questions. So many questions. I do actually have a lot of um followup questions, but I also want to um get into why open telemetry like how like when exactly I know you mentioned, you know, kind of back in 2022 you you know your team was early adopters. Um how did you come upon open telemetry and like why did you decide like oh this makes sense for us? Yeah. So you know I think the thing which which sort of ticked all the boxes for us we we came from a place where we were having to maintain our own observability libraries um and given the fact that we werenet and then trying to diversify into lots of different languages uh meant that we were having to translate that thing again and again and again. Um and on top of that we also had uh well we were using New Relic at the time and we had a lot of the New Relic agents in the mix as well. So we had this this weird mix of our own libraries for some stuff. We had uh a mix of of things from open from New Relic uh with the uh with the agents and all of that kind of stuff. Um and we had a lot of people who were writing to New Relic directly from from their code as well uh as well as to other sources. So one of one of the things which was really complicated on our side was every time we wanted to move something or we wanted to change something it meant code changes. Um and that's something we wanted to avoid going forward. So we wanted to standardize on something which was ideally open source that we could include in our uh in our services one time and maintain that and then change what where we put things but didn't didn't have to change the code every time. So we're really looking for um you know something that would give us stability in terms of the codebase but allow us the flexibility to send things to other places as we needed to do that and open telemetry was the only one which sort of ticked those boxes for us at the time and the only worry we really had at the point was that it was it was a new project. Um there was a lot of risk in going that direction. uh we decided in the end it it wasn't an easy decision. We we actually discussed this a long time internally but we decided that it was worth the risk to to go ahead and adopt that because it looked like it was going to take tick the boxes for what we wanted to do there. Um you know so basically it's it's to isolate basically our code base from from the from what we do with the telemetry afterwards which was sort of the selling point for what we wanted to do there. That is very awesome. Um I don't know I don't know if Jerome has anything. Uh oh yeah I yeah just to add on to that um I can give somewhat of a perspective to what it was like before we had open telemetry like um I was serving on the performance team for a good number of years and um it was very challenging to do uh analysis of specific applications just because like Cal had mentioned things were it was kind of the wild west on how we handled metrics like performance metrics and monitoring are are somewhat look similar, a little different, but enough to where like every time we had to analyze a specific application, we would have to do a deep dive on a set application like what telemetry are you guys putting out? Oh, you're not putting out any at all. And then we'd have to like dig into the code and that would really muck up like how much time we could actually like drill down into what the core problem of said application performance uh perspective was. Usually it would take half the time we would do an analysis it would be like trying to understand what the product was and then actually coming up with a solution would be the the la latter 50 or 40% they have. So um since joining uh observability like and even like even though Cal showed a simplified version of um our reli stack and uh it's a like you said it's a bit more complicated once you drill down into it. I was amazed just how like how much cleaner it is to just understand like what our what relativity is doing as a whole. So, uh take that with what you want. That's very cool. Um now as a as a follow-up question, um one of the things that we are always curious about in the SIG is collector setup. Um what kind of you you mentioned I I believe um Cal mentioned that you guys manage a fleet of collectors. Um how do you manage that fleet of collectors? How many can you give us an idea of how many collectors? Is there a particular setup that you use? Like we we would love to know. Yeah. So, you know, I think um in terms of how things are usually deployed, I think you call it a gateway setup. So, all of our collectors are basically in a gateway setup. Um so, people send their telemetry to us, we process it and send it on. Um over the years, we've had more or less complicated pipelines, but in general, we have essentially a single set of collectors with a common configuration that handles all of that uh all of that uh telemetry. We um so in terms of the numbers of things so we are running over 20 plus regions each uh each region has somewhere between depending on the load between 10 and probably 50 to 100 uh instances themselves. So we're running at any point in time somewhere between 200 and 500 individual collectors. These are all running in Kubernetes. Um we are uh very focused on compute and Kubernetes. So that's where almost all these things are running. In terms of the deployment itself, we actually handle all of our deployments um uh via Helm charts. So we use Helm charts basically to deploy these all out to the um to the um uh Kubernetes clusters. Uh we do take pains to make sure that essentially all of our configurations are isolated. So our collectors except in a very few cases do not reach out to external services. We manage the configurations all locally to make sure that they are as available and uh as as available and robust as possible, right? Um so that's kind of how we do all of that. In terms of the pipelines themselves, um one of the other reasons why we went with open telemetry is we have some internal uh I would say attribute enrichments that we do specifically to our product and the way we use things. We actually implement those inside of the uh collector itself. Um so we have some custom processors which en enhance the data in various ways. We actually validate it. We actually drop data which is not uh which contains things which shouldn't be there for instance. Um so we do a lot of validation on the fly as things go through the the collectors themselves before we write it out to our various data stores. Um I think I I think that hit most of the points you're asking for. Did I miss any of the uh the things you're asking? I think I think you got most of them. And and yeah, actually one one follow-up question I had was uh also since you you mentioned you deploy your your collectors in Kubernetes via Helmcharts, have you used um the hotel operator to manage any of your collectors? Is that something you played with? We have not. The the hotel operator sort of came out after we were already doing stuff. So in fact, we uh we don't uh but not for any particularly good reason. No. Okay. Okay. Fair enough. So, it's it's that we had uh you consider it sort of tech debt. You know, we had the Helm chart and all of that beforehand. Um so, we've just continued doing that and we're kind of used to managing things that way. Um we actually will have an internal shift in in our deployment uh tooling uh coming up for the organization. So, that might be an that might be an opportunity to switch over how we're actually managing things. But at the moment, yeah, we're not we don't have any experience with the uh open telemetry operator at the moment. Got it. Got it. And I I uh another similar question. Um since you do meet manage a fleet of collectors, um have you played around with the opamp protocol for for doing that? We've not again again we've tried very hard to make our our fleet essentially uh standalone. So even for configurations and things like that, we don't actually reach out to anything else. We really deployed uh sort of static config maps for that. Um that that was basically a choice that we made um just to because we have such a large deployment and because it's distributed so widely, we wanted to make sure we were as um resilient against networking problems as possible. So even for things like configuration changes, we we push out new new configs for those things rather than than doing something that Got it. Got it. Do you have a It's not to not to say that's a bad idea. It's just it's a choice. It's a choice that we made. Of course. Of course. That makes sense. That makes sense. What about um maintaining the collector pipeline? Who is there a dedicated team? Is that just whoever is on the observability team or how do you maintain the local configurations and the pipelines? Yeah, so we do have an observability team. Um, so we are uh split between Poland and here um our other big engineering office is actually in Krakow, Poland. Um, so we have two uh two observability teams split between the two regions. Uh, each one has I think around five people at the moment. Um but collectively we are responsible for maintaining those pipelines and that configuration. Um almost I would say almost entirely we are responsible for that. There are a few areas where we share responsibility with certain places where we pull in configurations. One of them is our Kubernetes team. um they actually define part of our configuration for which metrics we actually pull into the global system rather than just keeping it local to their Prometheus instances. But by and large we are responsible for that entire configuration. Um and we try to avoid shared responsibility there just because we want to make sure that uh you know shared responsibility is always means no one's responsible. So we try to really avoid that. So, as you know, the project has matured um and it sounds like you've got a pretty good setup that work is working well for you guys right now. Um how has the implementation evolved over the years, you know, as um things have become GA? Have you added additional instrumentation signals? Um how has that looked as you've kind of grown alongside the project? Yeah. So, um yeah, it's actually an interesting question um in the sense that the thing that surprised me overall is that the deployment that we have hasn't really changed in terms of its architecture since the beginning. It's been the same architecture the entire time. Now, we've made tweaks here and there and we've simplified things, you know, as new features have come out and that kind of stuff, but by and large, it's stayed the same. and and that I think is amazing given the fact that it was really sort of pre-alpha when we started. Um so we've gone through you know we sort of gone through the uh the alpha beta you know and things are starting to become stable now and we've not really noticed uh any really heavy changes that we've needed needed to make in the platform. Now that being said I mean we have added new signals. So at the beginning of course it was metrics and traces that were were there at the beginning. Logs came in afterwards. Um we had logs coming into our system even earlier than that. So we we had some of our own receivers to do some of that before it was an official signal. So we've we've sort of folded those changes into the platform as they as they come along and it hasn't really been terribly painful in doing that. Um, so you know, as far as the community goes, the, you know, the the alpha beta stable kind of thing, you know, we've not seen really any negative consequences from that. We've had to update on occasion. Um, but by and large, those updates are pretty easy. and and Jerome I mean his he's been doing a lot of the following for the collect we have our own uh build of the collector uh which of course we try to keep in sync with the external one and Jerome has been doing quite a lot of that so you can probably give some feedback on how difficult that process is and and where we've run into issues. Oh yeah, I'm sure. I mean, for the most part, um, uh, it's, and I don't know if this speaks to open telemetry or the automation we put around it, but as far as just updating the collectors, it's been a relatively painless process. I mean, sure, um, there have been a few changes that we've had to uh, kind of team uh, just file down on as a team and like, hey, what path do we want to go down? But um as far as just like pushing out those changes, they've been pretty simple, especially with how elaborate and um detailed the white papers or the change logs have been out of open telemetry just explicitly telling hey these libraries or modules you've been using uh they here's an uh we're introducing a breaking change or an API is uh going to be deprecated. Uh but overall it's been uh quite e uh use of ease experience and um uh getting more people in our team to actually utilize that process has been uh fairly easy as well. Um, I I have a followup actually on on um on on building your your collectors because I think uh of I think out of all the people we've talked to, I think you guys might be the ones that have done uh have built your own collector, which makes me very excited. Um did you um in in terms of building like your your own dro um did you have like what was your experience around that like with as far as like the documentation provided on the hotel site because that that's another thing that we we want to hear from from um practitioners of OTEL is how how usable is is the documentation and and have you noticed an improvement in the documentation? So even um building like building your your own collector, how useful was that documentation? Did you have to do a little extra like Sherlock Holmesing to figure stuff out? And and also um were you able to uh build like a nice streamlined process for building the collector that uh uh like did you I guess have to go outside of the confines of of what was already documented to do that? Yeah. So um I'm not sure I'm going to be able to provide necessarily documenta you know feedback on the documentation but the process as a whole I mean we one of the things we started with was we had to start with our own uh our own build of the open open telemetry collector mainly because we do have custom custom receivers we have custom processors we have custom exporters as well and we kind of needed that at the beginning to cover some of our some of our use case so we were forced basically at the beginning to to build our own collector, right? Um, at the beginning it was very painful. It was basically us cloning the entire repository and then making the changes we needed to build it. Um, once once the collector builder came along um that made that whole process so much easier and it was it was a great addition to that uh that process. Um right now basically we have moved to a situation where we have our own repository. Um and basically we have the single config file for the uh for the collector builder that just tells us what we pull in from from contrib and from the uh the main open telemetry collector and the only thing we have in our repo at this point is our own custom stuff. Um so once the once the builder was there everything since then has been really really easy and smooth to to build our own stuff. Um we did that right at the beginning. So before there was documentation so um and it's worked since then. So I I can't say you know whether the current documentation is good there or not but we use the builder and it it works seamlessly for us. And uh just as a followup because you mentioned you you built your own components. Um, how how was that experience of of building your own components? Was that um was that tricky? Like what was the um uh because I that that's definitely something that um I I would say personally feels a little bit lacking in the hotel documentation for from building like your own receivers, processors, exporters kind of thing. How how is that for you? Yeah. So I think in terms of the team itself um I think the biggest hurdle there was was learning go we we weren't uh we weren't um so I think from the team standpoint that was sort of the biggest uh initial obstacle for that um all of us I think have some good experience with go uh now so I don't think that's an obstacle anymore um in terms of how to do it um we again were doing this very early. So it came came at a time when it was basically well let's take one which we know works copy it over and then make the changes make the changes you know that we need to do to make our own stuff right. Um since then since then the documentation has gotten much better and it's much easier to sort of start with with those things and get them done. Um, but you know the I must say the the process of just copying something that works and moving it over and making the changes you need that also worked pretty easily as well. So you know I don't think there's a huge for an experienced programmer I don't think it's a huge barrier to do those things. Um now there are some things which I would love to have better documentation on like in terms of internal telemetry how to you know how to hook in better to the internal telemetry and and that kind of stuff. Um we sort of worked it out uh you know by uh reverse engineering what was there but some some of the things like that would be much more helpful if uh they were better documented for instance. Thanks. Oh, I was just going to say um yes, I remember um that when we were catching up um a bit yesterday and we definitely want to get into more of like the feedback that you have for the project. Um there was a an audience question that I think is interesting from Sumit. uh how much data size do you consume and have you found the need to reduce it for example via tail sampling or other methods and from application logs and security logs perspective. Yeah. So uh in terms of the data that we push through the system we are somewhere around uh we write typically somewhere between 10 and 15 terabytes a day. So that's sort of the the uh the volume of the data that we we consume. Um the in terms of the split between logs and and and traces and metrics, it's not quite an even slip between them, but you know, it's we we we uh take all of those things in, right? Um so it's it's not so different between the between the three of them. Um in terms of the platform itself, for the stuff that flows through the collectors, we've never had an issue with scale. uh we've been able to take whatever people have thrown at us uh because we have autoscaling on all of our collectors and everything else. Um we've not run into any issues at that scale and there are times when that scale goes up by a factor or two because someone's doing something stupid. So you know uh so in terms of of tech and all of that things have just scaled correctly. The place where we've had to reduce data is mainly because we do push this data into New Relic and we pay for what we push into New Relic and we do have budget constraints. So we do manage essentially the volume of data we push into New Relic mainly for budget reasons rather than technical reasons. Um and in those cases yes we do sample down the traces. That's what gets sampled down most heavily in in the things that we we pull in. We obviously manage our logs by log levels. So we really do have dynamic ways of changing the log levels we use for that. Um typically coming from the Kubernetes platform as well. Um and metrics we tend to leave untouched. Um but there are cases where people abuse that and and we and we go after people. It tends to be rather than a proactive thing. It tends to be reactive. uh we see that uh we do look at who's who's pushing out data um and then go after them if we think that they're doing something unreasonable. Um but I will say in terms of tech, we've not run into any issues with scaling which I think is really amazing uh for for this for open telemetry in general. Um and most of the most of the things have been really uh based on budget rather than rather than the actual technical stack. Um, I wanted to uh ask a follow-up question to one of the comments that you made because actually overall um because the the vibe I get from from speaking with both of you guys is that your organization is very much into like very much supportive of of like the observability journey um and and open telemetry adoption. Can you talk a little bit more about like this culture of observability? Uh yeah, so you know the I was brought in basically when I when I started to to change the way we do observability at the at the organization you know and and a major part of that has been open telemetry and it's been really nice to see that the technical aspects of that have been have been actually the easiest parts uh you know the tech has just worked. uh you know not to say that there aren't hiccups and all of that but um you know the the tech stack is actually working really well and the data flows that we have in place I think have been uh have been really good. The harder part of this has been the cultural changes which you mentioned a bit. um and changing the way the organization deals with observability. Um so there's the you know resistant to migration. So you know there there are migrations there and people are resistant to changing code and we have a large code base. So that's one of the reasons why we still have some of our old libraries still producing some telemetry. Um so there there are res there has been resistance there. Um, and the the nice thing I've seen over the last few years is that as people get more and more experienced with the open telemetry and see what it can do for them and how it, you know, sort of simplifies the way they generate the telemetry, there's been less and less resistance going forward, which is which is a nice change to see, right? Um, so people are starting to see the benefits of this and they're starting to really, you know, uh, lean into open telemetry as a platform for that. Um the other thing I've seen on the other side of things is because we can write to various different places very easily. Um being able to do that has uh has sort of enabled people to to do nice new interesting things. And you know one of the things which is you know on the diagram I showed at the beginning was one of the places we write telemetry is launch darkly. Um, and they have a really cool feature where you can say, um, uh, it's called release guardian, and it's a way of actually, uh, changing a flag and having launch darkly ch make those changes automatically and progressively roll it out based on the telemetry it's seeing, right? So, we send spans there and if the spans look like we're getting a lot more errors or that the latency is getting larger or things like that, it will automatically roll back. And the fact that we could actually roll that out by just a configuration change on the open on the observability side is really cool because we've you know incorporated an entire another platform with our telemetry without having to change anything fundamental in our in our code and that uh for me is really really powerful and I think as people see things like that I think they're they're more convinced that this is a good choice. That's great. Um oh sorry go ahead. I don't know just going to give another before story as well. Um like like I mentioned uh I was on the performance team initially and we did have uh like Cal said like uh people are very uh hesitant for change. Uh like to give you some perspective like we did uh we had an initiative uh in the performance sphere to try to get more people to start using more performance telemetry and that dragged out for like almost a year and a half with teams like moaning and complaining all the way through uh just because of how non-industry standard it was and even at the end of it people weren't 100% sure like how much benefit it was bringing but Um right but to bring that to back to today we have another initiative uh along with open telemetry and we've gotten feedback basically just to get more telemetry uh maturity and coverage throughout all of our systems and services and we've gotten feedback from teams like almost immediately like how much they're benefiting from it as they mature their own uh application telemetry. uh and uh we we've seen like just like uh like response times to like issues that have come up uh that uh have brought about this because of open telemetry. So it's at least from someone who's been on both halves of this, it's like night and day. That's really exciting and you actually answered the question I was about to ask you. So that's great. Um Reese, do you have anything else that you wanted to touch upon? Um, I'll say quite a bit, but um, in the interest of time, um, you know, I did want to give some space for you to share additional feedback that we could share with the project maintainers. Um, I know you talked a little bit about, um, the issue with, um, conventions for internal data schemas. Um, so if you want to go into a little bit more about that and anything else that you would like to see improvements in or feature requests, things like that. Yeah. So, um, yeah. So, first let me preface this by, you know, I I think probably from what I've said already, you know, I am 100% in with open telemetry and I think it's a great platform and what the maintainers are doing is fantastic both on the documentation level and the code level, right? So, Um, that's all great. Um, and and this isn't feedback necessarily uh in terms of the actual uh collector implementations or open telemetry project as a whole, but I think it's sort of feedback on I think where we're going in our own journey and probably that has some bearing on other people as well as as we're generating more and more telemetry and it's becoming more and more useful for people. We have a lot of downstream consumers now. So we have an entire AI division who's now looking at our telemetry. We have people managers who are trying to do reporting off of this and getting standard data is becoming more and more important. Um one of the one of the things we built initially and that one of the reasons why we had to have a custom processor is we do have standards for the attributes which come into the system. So we enforce a sort of a global schema on everyone even though that's extensible. Um now how that fits in with the semantic conventions, how that fits in with you know the elastic common uh uh elastic common schema um in general how this fits in with sort of data contracts and how you have contracts between the producers of the data and the consumers of it. Um all those things are things we're thinking very hard about right now and we don't have good ideas on how to you know apply that across the board um and good ways and the level at which we need to do that. So you know I think discussions about uh the semantic conventions and the ECS and you know how to do things like data contracts with with open telemetry data I think is going to become more and more important. uh I see that in our own organization and it'd be good to come up with sort of common common themes there and common tooling and common ideas. Um so a little more convergence I think within the conver within the community there I think would go a long way. Um in terms of other feedback the tech stack I think uh in general is really really solid and you know the way things are managed I think is great. So I don't have a whole lot of feedback there. Um yeah so I think it's mainly looking forward and seeing how we can sort of use the technological base that we have and and come up with better ways of standardizing the data which is produced uh making it more useful for downstream consumers I think is the the main thing I would like to see discussed more right on thank you and I think we have a bit of time to catch up on some audience questions um so let's see one of them is from Doug Doug on YouTube and Doug wants to know with regards to collectors, their dream is that cloud providers will offer as a boring text similar to an SMTP or SFTP. Is Doug crazy to want that? Um we um the first question we ask any vendor now is do they support OTLP? Um, so you know, we treat that as our standard protocol and uh it's a negative uh a negative check mark against someone if they're not supporting it already. So we we're pushing all of our vendors and and various tech products and whatever to support that as much as possible. Um so I don't think that that's um I don't think that's a pipe dream. I think uh pushing people in that direction is is a good thing. Um as I said we are uh very heavily a Microsoft shop so we use Azure uh very much and the feedback we've consistently given them is uh we want you know Azure monitor and those types of things to produce open telemetry signals because we don't want to have to do that translation ourselves. Thank you. And Manas I see you have a question about OPM tutorials. we will respond directly with some resources for you um in the LinkedIn chat. Um in the meantime, there was another question. So, did you ever feel that there were classes of telemetry data that hotel didn't collect well and you needed to add another collection method such as eBPF? we've had to uh we've had to come up with some um some custom receivers and things like that that uh that weren't covered beforehand. One of the early things we did was there was no way to sort of uh provide a web hook into open telemetry at the beginning. There is now. Um so that's one of our one of our legacy receivers that we've had. Um and that was something that was not difficult to write but something that was sort of lacking. Um in terms of other classes of telemetry uh I would say that the one of the biggest gaps we've had internally as an organization is front-end telemetry. Um and that's something that we're actually closing now. We actually have a pro project in uh in currently going on basically to collect more of our front end telemetry uh with the open telemetry SDK the JavaScript one. Um so that was something that we internally was difficult to do for for many reasons and we're now sort of closing that gap. In terms of other types of cle telemetry that's been difficult. I don't think we've had any real issues. most of the things that we've had to collect if it's not OTLP or things like Splunk Hack or Fluent Bit and things like that where we've been able to get things into into our collectors fairly easily. Perfect. Thank you. And I think our time is just about up. Um were there any last well not last parting words? No just uh thanks thanks for inviting us I think is the is the the last thing from our side. Um and you know if anyone wants to reach out or whatever I am in the uh in the slack so feel free to reach out to me directly if you have questions or whatever happy to discuss what uh what we've done in detail if it's if it's helpful. Thank you so much for offering that and thank you so much Kyle and Jerome for being on and sharing your journey with us. Um we will link to our um hotel sig and user channel as well. Um so you can find all of us in there. Um you can also reach out to Cal directly via the CNCF Slack as well. Um, and so we do have a link manus for you um that we'll put up here in a second um that links to OPE documentation and oh well we might have time for one more quick question. This is kind of what do you think about auto instrumentation? Uh me personally I think it's a great thing uh you know and we use it where we can uh and in fact for the front end stuff we are planning to use some of that auto instrumentation there. Um the downside we've seen uh we have tried to use it and we do use it on the net side in the net SDK for uh HTTP requests and things like that. The the problem we've run into there is volume. So it it it tends to be difficult to sort of scale down to exactly which ones you want. So the auto instrumentation tends to be overly broad sometimes, but that's the only sort of downside we've seen with uh with it. Uh so in general, we you know, I think it's a great thing and if it works for you, then yeah, definitely use it. Awesome. Thank you. Well, uh, Adriana and I would like to thank you again so much for being on here. This has been really great and I'm sure I'll have um myself and other people will have more questions to learn more about open s implementation since you guys were early adopters. Um, we do have a couple things we want to share real quick before we head out. Um, hotel community day is coming up on June 25th, I believe, and we are currently accepting uh talk proposals. So, if you would like to submit a talk, we definitely encourage you to submit a topic for open telemetry day. It's going to be in Colorado as part of open source in Denver. It it's a collocca I think it's colllocated event of opensource summit North America in Denver, Colorado. Yes. So if you want to come out get um well I guess that's too late to ski but the beautiful mountains I guess. Yes. Um, also if you are going to or if you are already in Europe and you're planning to um head out to CubeCon EU in London in two weeks, less than two weeks. Yeah. Um, we would love to see you there. Adrienne and I will be speaking um at the event and we have a blog post. Yes. and we have a blog post um about the event that outlines all the open specific talks. Um the recordings if you're not able to make it will be available on the CNCF YouTube channel I think about one to two weeks after I think um depending. So yeah, they're they're usually pretty fast. Sometimes they get it within a couple of days. So just keep an eye out. Yeah. So if you can make it um in person, you are definitely welcome to check out the recordings on YouTube. And I believe that's it. Um, do you want to mention also if you if you are at CubeCon uh EU, um, we are going to be hanging out at the hotel observatory off and on. So if you're there, come say hi. Um, the observatory is always lots of fun. Um, because there's usually tons of hotel contributors, maintainers, etc. hanging out. So it's great to have like ad hoc conversations with folks. There's also going to be um if you're a CubeCon hotel project updates. Um so I strongly encourage folks to join that as well if they want to see like what's new and exciting in hotel. That's always a a fun session to attend. I think it's it's usually like half an hour um that it's slated for. So, and and also if you um if you made this recording but you have a friend who couldn't make it um you can replay on our LinkedIn um just share this the same link for the link the live um recording um with your friends or um this will also be available on our hotel YouTube channel um so tell your friends- official tell your friends. Tell your friends. All right. Thank you all so much and we will see you next time. Bye. [Music]

