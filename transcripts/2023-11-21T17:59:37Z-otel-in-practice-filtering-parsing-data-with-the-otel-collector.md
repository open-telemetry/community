# OTEL in Practice: Filtering &amp; Parsing Data with the OTel Collector

Published on 2023-11-21T17:59:37Z

## Description

Filtering and parsing data with the OpenTelemetry Collector: a beginner's guide. Not all data comes out perfect from ...

URL: https://www.youtube.com/watch?v=y3AAerwIFfg

## Summary

In the latest episode of "Filtering the Noise," the presenter, NA, discusses the importance of filtering telemetry data at the OpenTelemetry (OTel) collector level, particularly for traces. The session emphasizes the need for operational tools to manage data without requiring code changes, especially when dealing with sensitive or excessive data. NA illustrates how to filter out unwanted spans using configuration settings within the OTel collector and highlights the limitations and capabilities of various processors, such as the filter and transform processors. The discussion also covers best practices for testing configurations and the significance of maintaining clean observability data. The session concludes with a Q&A, addressing viewer inquiries and encouraging further exploration of OTel functionalities.

# Filtering the Noise in OpenTelemetry

[Music]

Welcome to "Filtering the Noise." I initially promoted this as better filtering for logging, but today, I will mainly demonstrate filtering for traces. The principles for both are essentially the same. 

Let's start by discussing some considerations regarding why we would want filtering at the OpenTelemetry (OPA) Telemetry collector level. 

## Level Setting and Considerations

OpenTelemetry instrumentation, like any code-level instrumentation, can generate a lot of data, but some of that data can be noisy, sensitive, or simply excessive. After we go through the effort of instrumentation—especially at the code level—we ideally want to address data collection issues without requiring changes to our codebase. 

For many operational teams, accessing the codebase to make changes isn't feasible. They might not know where the hooks are added or may not want to approach the product team with issues like, "Hey, we’re accidentally sending Social Security numbers to our observability backend. Can you fix this tomorrow?" This workflow doesn’t make sense for many people.

While we can discuss the importance of having access to the codebase and better connections between teams, let's focus on the practical aspects. For instance, if we find ourselves sending too much data—like recording every step of a long loop as a separate span—do we really want to bother the product team to edit the code? It would be ideal if we had operational tools to manage this.

## Example Scenario

Let's consider a scenario where a developer was asked to add custom code points and ended up sending more data than necessary—like user IDs and database keys. This is not a critical security issue, but it is excessive. We may not want to send the full database key across the internet. 

Editing this at the service level would require multiple changes across the codebase, which can be cumbersome. Instead, we often utilize a collector-to-collector architecture to handle this at the collector level. Alternatively, we could implement filtering at the data store level, which would involve editing how we handle spans and traces.

## Why Filtering Matters

Filtering is particularly important for sensitive data or when dealing with excessive data. Sometimes, when looking at a trace, we might notice something that isn't helpful. For example, if a request is asynchronous but is rolled into database time, it could mislead us into thinking the database is slow when that’s not the case. 

So, how do we filter? Inside the collector, we have receivers and exporters for transport, and processors in the middle that handle data scrubbing, normalization, and sampling.

## Filtering by Attributes

We can filter spans based on specific attributes. For example, if we want to avoid recording spans for certain internal test users, we can add a processor section that filters out those spans. 

Here’s how you can implement this in your OpenTelemetry collector configuration:

1. Define your filter with a descriptive name.
2. Add it to the pipeline, ensuring the correct order of operations (filters should precede transforms).

After saving the configuration and restarting the collector, any spans with unwanted attributes will be dropped, while valid spans will continue to be processed.

## Key Takeaways

1. **Filter Processor Behavior**: Once a span matches a filtering condition, it won’t execute later conditions, ensuring efficiency.
2. **Event Handling**: If you drop span events, the span itself remains; however, if all data points from a metric are dropped, the metric will not be reported.
3. **Filter Configurations**: Ensure to add your filters to the pipeline correctly to avoid issues in execution.

## Transforming Data

We can also transform data using regex patterns. When we want to drop specific attributes but keep useful information, we can utilize a transform processor. This allows us to replace unwanted values while preserving necessary information.

### Cautions with Transformations

- **Metric Transformations**: Be careful when transforming metrics, as improper conversions can create meaningless data and obscure important signals. 
- **Naming Conflicts**: Changing labels can lead to orphan values in your data store, making it difficult to retrieve useful insights.

## Conclusion

This presentation covered the importance of filtering and transforming data at the collector level, especially for managing excessive or sensitive data in OpenTelemetry. 

Do we have any questions before we wrap up? 

### Q&A Session

- **Testing Config Changes**: To test configuration changes, consider using a test version of your observability setup or a canary deployment to limit the impact of changes.
- **Collector Restart for Config Changes**: Generally, a collector restart is required for new configurations to take effect.

Thank you for joining this session, and I hope you found it informative. If you have further questions or need clarification on any topics, feel free to reach out! 

[Music]

## Raw YouTube Transcript

[Music] this is filtering the noise uh wish I I think I promoted as like better filtering for logging but I'm mainly going to demonstrate filtering for traces the principles are the same I promise we'll get there we'll get there folks so let's talk about some level setting for considerations with let me just turn off my ageback here sorry considerations with why we would want filtering at the OPA Telemetry collector level so just review these like first principles everybody seeing the slideshow okay it's got the little green line around that's why I assume you're seeing this great I'm gonna assume that's great give me a negative Emoji react if if if you're not seeing it but otherwise uh we're good um so open lemetry instrumentation like any code level instrumentation unless you send a lot of data rather easily some of that data is noisy some of it is sensitive and some of it is good but needs tweaking and once we've done the work to do instrumentation especially instrumentation at the code level when we're faced with a data collection problem we would ideally want to fix that without requiring changes to our code base so for most people who are working operationally the the thing is like no I can't cannot edit the code base right I don't I don't know where these these hooks are added I don't want to message the product team and say hey uh turns out we're sending you know people Social Security numbers to the to the our observability back end please go do this code commit tomorrow that's just not that's not the workflow that makes sense for a lot of people and we could talk all day and all night about how well you should have access to the code base you should have a better connection that's real DeVos blah blah blah great points but let's talk about like you know okay it's one thing when oh my God we're sending people's a ton of people's pii into observability when we shouldn't what about the situation where it's like hey we're sending a little too much hey the traces they like you know every step of a 500 step four Loop they're recording is a separate span and they shouldn't is that really something where you want to bother the so the the the the product people about it and ask them to like edit the code like you know for sure that's like' be really nice if we had operational tools to do that so there are a let's take a look at this code where uh we can sort of Imagine where we might have gone a little far here so we had our developer we said hey would you add some custom code points maybe they got excited about it we did our little brown bag they added a bunch of points they added hey let's send the user ID as a value for this um for this span let's send like how many uh rolles are we doing we'll use that uh or right I think we decorate the child spam with that yeah and then um let's also send their database key up as part of what we're sending and that probably was going a little far right we're like oh no we're shipping this data like across the internet and maybe we don't want to send their full database Key by the way key is a somewhat overloaded term here but let's just say like this is not critical security uh issue but it is like hey that's more than we really want to be sending around and showing to everybody in the the the back end so there are a ton of this actually a diagram I drew for something else but works F here there are a ton of points where we could say hey I don't want that database key uh um completely uh uh a as a complete string to be stored and so going in and editing these code points right would be editing it at the service level which might be in a whole lot of places and then we might be able to we very often have these collector to collector architectures so so what we're going to be talking about today is is hitting it in one of these collector levels and then the last and and relatively common is doing it at the data store level so this would be like hey maybe we're hitting we're sending way too many spans on a single trace uh maybe that example of like Hey we're sending 50 identical spans actually this example I don't think I I don't have the code here I'm not going to hop over to my IDE for this this thing but it is like it's saying hey roll if you roll seven dice it's sending a span for every single dice roll like be like I don't know I don't want all those those those sub spans right that might be an example where instead of trying to filter The Collector level a a pretty normal step would be like oh let's just go and edit the data store um and say hey maybe older than this amount we want to compress our traces in this way that's a perfectly reasonable way to do it depending on whether or not you have access to do that I know with uh chronosphere generally you do um certainly if you like have a a you know self-hosted data store for this um that that should be very doable if you're using like Loki or signos to do that but um we're going to talk about filtering at The Collector level any questions about like why we're doing all this why this this matters or is a tool that we would want to have yes why uh yeah so in general it's going to be sensitive data or it's be over over like overfit data um are sort of the two main reasons but there's also just going to be this General sort of vibe which is hey I'm looking at this Trace here and I don't like what I see for some reason so see change the share here I'm saying hey I'm looking at this trace and this isn't helpful for a reason I will explain right like like and and that really could be anything but you know when we work on the op operation side work on the SRE side like here's an example that where it's like hey you roll the dice three times I don't need these little sub spans and once the request gets complex it really starts to clog up the view and it's not so much about like having an elegant view that should be something that you can control on on the UI side but it's like yeah something's not looking right and it's leading to distracting data a perfect example is um hey maybe you have a large request that is in fact asynchronous but it's getting rolled into your database time and so it's like hey that's very deceptive and it's it's causing everybody to say hey the database is running really slowly but that's not what's going on and so that pretty good pretty good uh time when we would say hey we we don't want to go and actually edit application code or custom code calls we want to do something at the collector level so just the reminder of like what can happen inside the collector right is like we have receivers and exporters right which do our our uh transport standard um communication can sell stuff out to Stats d can take stuff in from a million places um and then we have our processors in the middle and we're going to talk about data scrubbing data normalization and a little bit about sampling batching you know it's batching that that is that is what it is but um that is the concepts that we care about okay so let's talk about filtering by attributes so we have some attributes on this um uh a span that we say hey if we get these attributes we really don't want to save this so let's go ahead and demo that live let's do a new share and here so here's my open Telemetry collector configuration and just stepping through what was necessary to uh add this um to add this hey if if a span has these attributes I do not want to send it um across so in our application right we take a request attribute um and we add it to that span right down here we say hey the user for this is this request attribute user ID that we received obviously with a real application we should have the user ID from a million other places but um we picked it up here and it turns out that a couple of our users are like our internal test users right and so in fact that's so consistent because it's like a automated testing Suite so we know they're always going to be named this we say hey don't don't record any spans for these people because maybe they're testing stuff maybe those spans get crazy long they introduce a bunch of fuzzy unnecessary data to our to our production environment so we can add a processor section which starts with filter and then has whatever name we want it to have um so this is slash we say hey we can call this Dev users because that's that's descriptive and then critically and I think I have a slide about this too we also add it to the pipeline down here which is where I usually mess up I usually go ahead and create craft a very careful filter in The Collector config and let me give like one stke of Zoom here um and then forget to add it to this thing and and this uh pipeline is red left to right so uh you know if you're doing things like trans transform filters transform processors which we'll talk about you know you want to have just this in the correct order so that your Transformations will have effect when you filter and you generally want batch at the end so if we save this config and go and restart our collector come on bud cool cool cool cool and then we send in a couple of requests so that was from David and if I check my config David's like an allowed user but if I send one from Bob so here's the service right it's this it's the same dice roller that you've seen in the open Telemetry examples right it returns an array of dice rolls uh you know Bob still gets his data because it's not like we're you know this is all observability side so the the application works exactly the same but we should see that this span is is dropped um we can hop over to our UI and I have gone so far as to so this guy is just coming in as normal and I believe has a user attached to it yeah this is David he's allowed and this is like the Juliet child chopped onions because no I did not wait for this to show up in my observability back in it would be there probably by by now but just so just so I didn't hit refresh and have it not show up uh but then if we did send it in with a um with this guy who we didn't want um it will go ahead and drop that span now in this case that span did have ch have child spans so it still shows up as hey there was there was time spent here but we did drop the span so we know it existed but we're we're we're we're dropping all data from it which is in this case is what we wanted what we wanted was we wanted to say hey um just this span is what doesn't matter this is like we're doing like span level metrics for execution time and so then we care about dropping that single span okay I think I have like screenshots of that back in the presentation we'll find out uh new share okay so um we want to go ahead and drop out the span yep there's our nice little thing and then and then our our regular one rolls on just as we'd expect okay so uh some stuff to remember about the filter processor first of all once it hits an attribute that'll filter it's not going to hit the later conditions so if there's an error on a later condition for some reason um it's not actually going to execute um and then uh you know any matching condition will work so so you know the the config that you just saw where it's like hey user equal to Bob user equal to David user equal to Alice filter right that is going to work it doesn't have to meet all those conditions just needs to meet one and then um if we drop out events this is a confusion for some people depending on the model that they're using for for monitoring uh the span is going to stay there so if we say hey the type is a span event and we want to drop every single span event we're still G to have that span and then um a metric is works not the same way if we drop data points from a metric until there are no data points and the metric itself will not be reported which makes sense you wouldn't generally report right like hey this data point but but but nil metric or sorry this metric but nil data points was not something you would normally report so yeah this is my this is my slide to reminder myself and everyone else that you want to actually add these to the pipeline okay so now let's talk about this problematic data here where this is the key that we're getting through which we really didn't want so in this case obviously we could write something that just said hey Buy name drop this attribute um but we don't want to drop the attribute because we we have this guest down in here that's useful to us so we want to know if it's guest admin whatever else whatever other role that's useful information to us we just want to drop the the the full key and so for that this this got me for a minute so um you can have a um filter that says hey it does it by match and we'll take do star say accept any um attribute but then it will not accept a full REX and then I read this portion of the documentation which is hey here's some alternative config which could soon be deprecated and that includes using Rex's as your match type and so I was like wait can I not am I not supposed to be able to use rexes here am I supposed to just use this like relatively simple like direct string matching so you can and uh my thing is write down this link because the something that I I've had on my to-dos for a few weeks is to get this um this doc a little better Linked In to the other documentation because a complete list of the open Telemetry transform language functions is like not super well linked into the like filter and transform processor um don't actually write this down I will share this uh I'll drop it into the chat you don't have to write down a whole link but uh yeah open Telemetry transform language those functions include uh a pattern-based replace which we'll demonstrate in a minute here so some notes on using this Rex in a collector config uh you want to double your escapes like this uh which you can set as a style thing on Rex 101 or wherever other Rex test tool that you're using and then you can use capture groups they do work but it will seem like they don't because you have to use this double dollar sign thing to do it so in this case it's like hey whatever the actual in this case this person wants to change Cube to k8s and U they want to change it to KES Dot and but they want the ID they want to they want to preserve the ID so they're saying hey match this key which is q but then some string of letters and numbers and then go ahead and replace it with K8 but that include what you matched in the in the uh uh parenthesis apologies that's r review for you all but uh Rex is my my my home and my joy so I always like to talk about it but yeah so uh you need to escape the dollar sign for this capture group with a double dollar sign and you have to use triple dollar sign to use a single dollar sign so sorry uh this is just one of those things you have to note because the beautiful thing about redx is that it always works except when it doesn't and so uh yeah these are just things you want to remember about your syntax so yeah so in this case what we'd want is we'd want a transform processor because we want to change our value right to just be underscore guest or just be some kind of escape and underscore guest and so um in this case right we say okay go and find it and I've been a little bit uh greedy here where I've said hey look for this thing it's a whole bunch of numbers uh more than one number for this like don't just take anything that is a user DB key and then go ahead and overwrite the ID with replacement ID and then and then your uh whatever group you first matched so then you'll get something like this replacement idore guest I have a version of this Trace in the signal dashboard but take my word on it this works so transfer processors really effective way you can um grab some other values if you need to you can chain them together but the big thing is just to to do uh like really smart collapse of cardinality and collapse of pii um really really nice tool to do that so what can't you do collector processors you know we've seen this beautifully impressive uh uh uh demo here of the stuff we can do um a lot there's a lot that you cannot do with collector processors um so here I would say is probably the big one big ones is you you cannot change the relationships that spans have uh with the processor you cannot just say hey you go be part of this Trace now nor can you create a parent child relationship like oh you actually were kicked off and we're inside this other one and I'm going to know that from like your the N your naming convention and then I'm going to stick you on to this other one so you can do filtering and transformation and you can touch Trace ID I haven't experimented with this extensively but but apparently it is not possible to like push something into a trace after the fact this makes some sense because of the way that like sampling and other kind of uh working with uh a traces happen there's like some computer science reasons why this is like not uh a trivial thing at all would not be a trivial thing at all to implement but you want to be aware of it and I I learned this for the first time it's sort of you know it's hard to notice an absence sometimes you feel like oh yeah I'm fully in control this I can do whatever I want but then uh Hazel's great talk for this same User Group clued me into like hey this is a limitation here and um I haven't like tried to hack it to every degree but it's definitely yeah there's no like there's no like built-in call say hey like ADD child relationship not at all you also can't drop a whole Trace so you notice that I had this example where it's like hey don't look that the you know the span is gone no worries but you would sort of like to say hey I can go in and say hey um this span looks bad let me go ahead and just lose this Trace um and that is quite a new kettle of fish called tail based sampl right where you're looking at all of the traces that you receive and um you're saying hey I only want these ones to come through based on some information about the the trace so this is the the the dichotomy between head and tail based sampling head based sampling is right before Trace even starts you decide if you're going to run it or not maybe based on like the tiniest amount of data like what route it's in or uh whatever like request information you have but otherwise basically it's just you know um uh uh I want to say the word I know it's the wrong word is skoric skoric has a completely different re meaning but you're doing probabilistic sampling in that case with tailbase sampling uh this thing that we all want whether or not we can have it is you're saying hey now that I'm looking at the trace I decide that you're intered you want to send you on tailbase sampling has a bunch of promise for the efficiency that it's going to add to your system and only sending interesting traces traces is relatively expensive to transmit store process so it's very promising but it has its own caveats to make work um so this is Mo we did most of the presentation I did not pause for questions a bunch of times but do do we have questions about that about head and tail Bas sampling and why that is not supported in the simple uh filter transform system love it I have to go give this same talk not the same talk but a version of this talk um to cgl next week I think and so thank you so much for joining me on this journey um because there's not a ton out there about these p i mean midly like the transform processor is in Alpha so I totally get it why they're wouldn't uh be a ton out there about it but uh I um yeah I'm glad we're getting to talk about it now so to do feel free to make comments or drop in the chat if you have other questions there are 29 things in the chat and I have not been looking at the chat at all sorry most of it is our but Paige is asking how do you recommend testing config changes before shipping them to prod or monitoring to know if you messed up a filtering rule yeah really good question so um I'm gonna hit back a million times sorry about that uh I I I do think that a really good way to do kind of experimental stuff where you're saying Hey I want to do a kind of a multile level change to what we're doing I want to do a complex transform uh the two things are one is to just have a test version have some piece of your observability inside your test harness um and so have your local version of a collector or somewhere else to to test it um the other is to this um collector to collector architecture is extremely extremely useful here so you can do like a canary deploy to say I'm just going to send it out to this one sub piece instead of just our our uh Master collector architecture and then the third is to do some further limitations because you can you can chain together an and here so you can and and then you can use um uh resource values so you can say hey only do this to this service name so that's super useful for for doing this I didn't I didn't show that piece I didn't I didn't do that here but you can say hey I want this to have X service name and also check for this span value and then drop the span so you can started out as being more limited before it spread out to everybody else really good question um is it collector restart always required to pick up new config great question from a long time ago I would get kicked off of twitch so fast because I'm not looking at chat H embarrassing um I used to know the answer to that page and now I do not I I I seem to recall that there was some uh there was some issue that I ran about about the the feasibility of some kind of hot reloading of config but essentially yes you have to do a collector restart certainly in your like little test test bed environment um yeah there's also not like um The Collector has relatively limited like remote abilities to pick up config so yeah um but yeah that's uh something maybe one of the honeycom people will know something about has as has fiddle with a bit and I can check with the rest of sign on Steam as well um Okay so we dropped this replacement ID we did that talk about what we can and can't do um and yeah there is uh a tail sampling processor out there um it has its limitations but you want to you want to explore that separately as its own processor and its own its own service for dropping entire um for dropping entire traces okay so words of caution with metric Transformations so uh conversion between data types isn't supported by the metric model but you can can do it with a Transformer but don't do it but you can do it but don't do it um it's exceedingly easy to CL create meaningless metrics right to like doing things like creating averages from Max values right which you shouldn't do um yeah I I would say very much like you're in the realm of you know creating statistical Transformations just with like Excel equations at that point so you are in danger certainly of messing up what you're doing and uh getting stuff that looks okay but it's in fact extremely bad signal um what have I seen I've seen like uh somebody processing in creating a new average with a uh single new value so they have an average the last 10 values and they get a single new value and they say okay cool I can just add that one to the average which was not quite right uh um and so yeah things like that are are are worth a concern um so yeah a bunch of the Transformations between um uh uh measurement types are one directional and so you should not use the transform to try to go back um and or to try to move between these Lanes uh because you will end up with bad data that is still data that still looks like data and will chart like data so so that's that's a concern very similar to going in and doing like uh just queries directly into your data Store to edit your data right like um you want to be exceedingly careful with that because of course there is always the possibility of destroying the information that you have uh slide two of cautions about metric Transformations so you can change the labels on metric and traces so uh you can cause yourself data store Problems by giving things the same uh names uh as as two metrics two time series you give the same name attributes and scope and um I tried to break signas the like click house data store doing this because I was very excited to see something break but uh I couldn't quite do it but but presumably you can get to the point where you're having your know dashboards not load or other queries not working because you have um double results when there should be unique values in a in a column so more common outcome of that will be orphan values will be spans that are not part of any trace and traces that are not connected to any service um oh yeah Ren I'll put it we we'll put it in the the uh cncf collector Channel I'll ask it in The cncf Collector Channel and the first thing I'm going to do is actually search The cncf Collector Channel because it's interesting the question was um do you have to do a collector restart always to get to pick up new collector config I think the answer is yes but um I yes yeah and I I thought there was an issue about it there was a talk about exploring some some uh support for that but uh I I'm certain that by default right now the answer is no that was the end of the slideshow folks which really just crept right up on me um uh do we want we we saw live demo stuff oh yeah let's go do let's go do one last live demo thing that's right that's what I wanted to do um new share new share share okay so um man there was totally some oh right uh we have the situation where we are getting way too many components this thing like um roll it it you know it sends you back an array of roll dice um and we had a situation where when we sended a request to say like hey roll um you know 41 dice for me we get back this nice array very very quickly but uh the problem is that when we go and look at the uh traces for these guys they just have a ton a ton of these individual single dice rolls which you know in this version right is not really a problem right you know you're you're Trac dashboard should be able to just hide those a whole bunch of n rules but for whatever reason in our example we're saying hey this is this is an issue we have hundreds of these we have thousands of these we know they're all identical they always take the same amount of time we we want to get these out of here this is an older Trace that didn't have 40 we just had six but you get the point and so what we want to do is we want to say hey I I want to drop these spans these roll one spans but some helpful and also uh sabotage focused developer has named each of spans uniquely so we can't just say hey go ahead and drop any span that has this name because it includes like this is the counter on that uh role so um that is something that we want to handle we don't need a reject for this we just need the match tool so if we go back into our IDE let me oh I started typing the command but I won't actually show this so if we go back into our IDE we go back into our config we can say hey span if you have a match on your name that is roll ones followed by anything let's go ahead and drop that span and if we save this and then we restart our collector um our app's been running this whole time so now asking for 41 dice rolls let's ask for 40 uh who likes using a prime number I certainly don't um while I politely wait for a second for these traces to hit my local collector get to the batch moment which is like every I think every five minutes on my thing and then get sent off to the back end do we have other questions about this stuff about um oh uh I didn't mention it but you can uh you have the same filter functions available on metrics and logs so uh we demoed this all with spans but it is totally doable with metrics and logs by the same principles but yeah other questions feel free to drop them in chat or come off mute I promise I am watching the chat now no worries okay so yeah now let me go ahead and start sharing this this is just our last five minutes of uh requests and so we look at we we just looking at a previous one where it was like hey we have way too many of these now if we go ahead and sort this and look at our most recent traces we should sure enough be able to see oh I I also gave it the name that's uh that's like hey do drop drop this name so we're dropping two things here but our actual R dice is happening it it drops the routing span because again it has this name it doesn't like and then it also drops the sub like individual Ro once spans um okay folks that's been my stuff man 38 minutes wow usually when I'm doing something for like the first or second time I like time it out talk it all through I'm like 45 minutes and then when I actually get in front of people 11 minutes uh just zipping zipping but this one I guess there was there's a lot to say about this um final stuff uh the um functions are all out Are all uh fully supported the transform processor is still in Alpha and you're going to need your own build of The Collector to make sure you include it uh which I mean you need to do for for other you know collector contrib stuff so that's that's that's pretty standard but just be aware of it um you wouldn't want to base like your whole DIY observability stuff on transform processors like um do do some work ahead of time to to like get your stuff labeled right and don't don't be like completely relying on it because it is and outfit is going to shift a little bit and there are some standards conversations happening but it's definitely worth doing for stuff like this for like hey you have this filtering or or transformation task and you don't want to bother your product team okay folks that's been my time I don't know Adrian if you want to give like final stuff oh um while you're filtering out these spans not to be shipped to signas can you route them to something like S3 oh that's an interesting question so not with this set of tooling you cannot use like a filter processor to say now go to a different exporter Lane but you certainly can do that with the with um you know uh uh with with your import tools right so so with your um components where you're taking in data you can obviously say on your receivers oops don't do that you can say on your receivers like hey I want to uh pick up this these values and send them to this other endpoint yeah and then go go into a separate pipeline um I don't yeah filter filter is like Drop filter is like drop or remove data it's not route this data to another endpoint great question pretty sure you can do that routing by span name but also also uh should you do that should do that's a good question um that's a really good I should write something about that uh butare that's that's really good I'm gonna I'm gonna like a look at a future blog post about that that's a really good question any other questions for na Ren says in chat yeah there are a lot of tools out there that help with managing your pipeline uh including like especially you know uh we got three I think observability no four observability team people here who can talk a lot about how like there are times when I I think all of us want to be like hey we're your One-Stop shop to look at your data but there's going to be reasons that you're that you're sending it to multiple places uh examples of course be like uh sending stuff to cloudwatch but also sending it to a New Relic dashboard sending stuff to S3 as like hey this is we're going to Cold Storage this but we want to have it uh obviously classically logs right sending them to multiple places to one place where the retention is 10 days and another place where the retention is 10 years so yeah that's that's going to make a ton of sense so yeah there's there's something written about that Ren maybe I'll maybe I'll hit you up for like a we'll do like a a partner post or something on it because it's an interesting question and and not one where there's a ton written about it sure yeah that sounds good um most of the pipeline stuff I've seen is from pipeline companies it'd be interesting to break it down from a perspective of folks who are more neutral towards which pipeline you choose yeah yeah yeah really good page is like is like um uh Paige says in the last webinar I did they asked us how many absorbability tools they had in a significant portion of the groups at five plus I think if we were being honest everyone's gonna say at least two right every well no at least three they say hey I have an observability tool which is the the thing called observability that says observability on its um SEO filing then I have some kind of logging to and then I have a debugging tool that I use right I have my click Ops tool that I use to go in and look at what the heck's going on and so that's at least three right um if not then okay well now what's what do you use as like a pinger or other synthetic metrics right that can can so yeah now we're at five right with just with just what I would say like oh like you you list all five you're like yeah that that's normal that's normal I need all this it's like me looking in my purse I'm like uh I need to get some stuff out of here they're like no I need all of this I need two batteries for my phone uh okay rid of these oh man great chat thank you so much uh and and again big shout out to R you missed it right at the start when I was like really thanking you for uh doing the last minute promo uh I will be giving a a similar but not identical version of this talk because we can all admit there were parts that the dragg um I'll be a similar version of this talk at cgl next week um and then also we'll be doing it at another conference who Name Escapes Me in uh in January uh will will to follow uh coach smash Cod smash which is in January or something I'm gonna be giving the same talk um and also be a cubec con and if you want to come say hi cubec con uh come say hi and also tric signas and also thank you thank you so much na for uh for joining us today um and thank you everyone who was able to make it this was actually a really good turnout for uh for otel and practice so thank you for taking the time um also for um if you know anyone who um wanted to attend but couldn't make it let them know that we will be posting a recording of this on the open Telemetry YouTube channel um the handle is otel Das official if you don't haven't subscribed yet um we've got a bunch of uh previous videos from otel and practice otel Q&A and even some of it are in user discussion groups so definitely check it out out um please check out the Hazel weekly one from six weeks ago now was really really key stuff so that's that's really worth so if you want a reason to go check it out that's that's a really good one yeah it's it's next level it's very very good content and we've got an otel Q&A coming up at on November the 16th with uh Jennifer Moore where she talks about uh her experiences with uh with otel implementation at one of her previous companies so that will be also a really great session to look forward to it'll be in this time slot same time slot um yeah um does anyone else have anything else they want to share um reys Ren nope just a huge thank you to NAA yeah this was awesome

