# OTel Me... An End User Conversation with Ariel Valentin

Published on 2024-12-20T07:03:14Z

## Description

Join the OpenTelemetry End User SIG for our last event of 2024! We'll be talking to Ariel Valentin of GitHub for OTel Me...An End ...

URL: https://www.youtube.com/watch?v=vB9_SiTV5CI

## Summary

In this episode of "Oh Tell Me," hosted by Reys, a Senior Developer Relations Engineer at New Relic, the format of the end-user Q&A has been revamped. Reys introduces Ariel Valentin, a Staff Software Engineer at GitHub, who specializes in observability. The discussion revolves around GitHub's adoption of OpenTelemetry, with Ariel sharing insights on his journey into observability, the challenges of transitioning from proprietary APM tools to OpenTelemetry, and the importance of distributed tracing. He highlights the growth in the number of services being instrumented for tracing since the adoption of OpenTelemetry and discusses technical aspects such as the architecture at GitHub and the use of probabilistic sampling. Audience engagement is encouraged throughout the conversation, with a focus on community contributions and feedback mechanisms in the OpenTelemetry ecosystem. Reys also provides information on how to get involved with the end-user Special Interest Group (SIG) and emphasizes the welcoming nature of the community.

# Oh Tell Me: End User Q&A Episode Transcript

[Music]

Hello everyone! Welcome to a brand new episode of *Oh Tell Me* and our End User Q&A. If you have attended one of these sessions in the past, you might notice that this looks a little different and has a different name. That's right! We have done some growing up since the last few times, and we're very excited to debut this new look. 

But don't worry, almost everything else is going to be the same. We have a great interview here for you today to learn from. Before I introduce myself and our guest, I would love to know where everyone is connecting from. I am online from Portland, Oregon today and would love to see where people are from. It looks like someone is here from Brooklyn, New York, so thank you so much for joining us out in New York!

My name is Reys, and I am a Senior Developer Relations Engineer at New Relic. I also co-lead the Ner SIG, which is our cute little logo that you see up in the right corner. At the end of your SIG, we focus on connecting directly with users and are dedicated to helping the rest of the SIGs get feedback from end users so they can help improve the project. This is one of those events that we host to do that.

Today, we are going to have Ariel on. Ariel, if you would like to join us and introduce yourself.

**Ariel:** Hey, how's it going, everybody? I'm Ariel Valentin, a Staff Software Engineer at GitHub working on observability. Thanks so much, Reys, for inviting me here to chat with you today. It’s an absolute honor to be the first in this new format, so a big shoutout to everyone who has worked to get this new platform up and running. 

**Reys:** Oh no, thank you for being here! I'm really excited. To get you and everyone up to speed on the format, it's similar to the ones if you've been on one of these before, but we'll start with some warm-up questions to get Ariel comfortable. Then, we'll hit him with some meaty questions. We'll also get into questions about the OpenTelemetry community, where he'll have an opportunity to share feedback about his experiences contributing to the project. He'll also get a chance to ask us questions in a section we call "Turn the Tables." At the very end, if there are any audience questions—oh, actually scratch that—if you have questions that come up during our conversation, feel free to put them in the chat. If you're watching from YouTube, just put them in the live chat, and I think LinkedIn will also have its own chat, so put your questions there as well. We will get to them as we can throughout the conversation. At the end, if you have any questions, we will also get to those.

Alright, I think we are good to get started. Ariel, you mentioned a little bit about your role at the company. How did you get started with observability and OpenTelemetry? 

**Ariel:** Oh, those are great questions! I should have mentioned that I'm here in Austin, Texas—Sunny Austin, Texas—so now you know where I am in the world. 

I think a lot of us started originally working with APM tools, which are generally proprietary tools that didn’t have distributed tracing in place at that time. As the community evolved and OpenTracing became a standard, that was my first experience with working with distributed tracing. I tried out different vendors and experiences with OpenTracing. By the time I got to GitHub, I was a champion for distributed tracing because I saw the power that was in there. Around that same time, folks were already moving towards developing and transitioning away from OpenTracing and OpenCensus to OpenTelemetry. I got very involved early on in trying to spread the word, learn more, and work with my observability team to start adopting tracing more and to make OpenTelemetry sort of our North Star for all of our telemetry signals.

I feel like I keep saying the word "telemetry" over and over again. I'm going to have to find a better way to say that! 

**Reys:** We’re both going to stumble over "OpenTelemetry" or "distributed tracing" at some point; it's alright. This is a safe space! 

So, you mentioned you became a champion for distributed tracing. I think something that’s interesting is that we are so entrenched in the world of observability that at least I tend to presume everyone understands what that is. It’s always surprising to me when someone asks, "What’s distributed tracing?" What do you think is the main challenge that most organizations face regarding observability? Is it simply understanding the value of distributed tracing and these new concepts? 

**Ariel:** Yes, because these are all sort of new concepts to folks, right? People have different levels of understanding of the tools available to them, and there's all this vocabulary that you hear that’s a little hard to parse through. Sometimes it’s like, "Oh, when you say ‘trace,’ do you mean a trace log, or does it mean samples taken from a profiler?" There’s a lot of language that even though we’re converging on a lot of it and have these dictionaries defined and published everywhere, there’s still this hump we have to get over. There’s a challenge we face with trying to get everybody speaking the same language within the same context. 

It’s very similar to domain-driven design; we have these bounded contexts where the same term means a different thing. That flows over into our domain language when it comes to observability and SRE practices. I’m sure many folks have faced those challenges as well, and it’s like, let’s all get on the same page about what we mean.

**Reys:** Absolutely! What are currently some of the most interesting problems that you are facing in your role? 

**Ariel:** One of those things is the transition. It’s really hard for an organization like us, which has been around for a long time—10 years or so—where the system has grown and evolved. There have been acquisitions that have been brought together, and there are disparate backends where we’re collecting this data. It’s trying to transition from one way of doing things to a new way of doing things. Learning something new is always going to be a big challenge. 

Folks are trying to do their job every day. They’re not trying to learn new SDKs or vocabulary; they’re trying to keep the system up and running and keep our customers happy. I think those are some of the challenges we all face. I know I face it, and I’m sure others do too, which is making these transitions with the fewest pain points possible. 

**Reys:** That’s a great segue into the next section. What was the process like for GitHub to adopt OpenTelemetry? 

**Ariel:** For us, it was the role of the advocate. I acted as an advocate and was a champion for OpenTelemetry at the company. I was specifically brought in to help advance the mission of tracing. There were other challenges we faced too, such as getting everybody to build a data dictionary and agree to the same language when it comes to our attributes. 

As you can imagine, as a system with many acquisitions, every team has their own log attributes or metric attributes. I said, "Look, here’s a North Star—here are semantic conventions from OpenTelemetry. Let’s anchor onto this and follow the rules around semantic conventions so that we can build our own internal dictionary." That comes with its challenges, like schema migrations and trying to keep up to date with the spec and what the instrumentations are doing. 

As an advocate, I was also a lead on the rollout. One of the things I wanted to do was sunset all our old SDKs and move over to our open-source SDKs. A few other members of my team and I were involved in OpenTelemetry Ruby, for example, because we’re a big Ruby shop. We got involved there to help the instrumentations get better and to test different releases of the SDK and get that rolled down into our GitHub monolith. We were very early adopters, ran into some challenges, and continue to give feedback to the community that way. 

I’m not only an end user; I’m also a maintainer, so I’m playing multiple roles there, trying to help the community along.

**Reys:** That’s a really interesting position to be in—as an end user and contributor. I definitely want to circle back on that when we get to the OpenTelemetry community questions. Can you tell us about the architecture landscape at GitHub and the telemetry that you’re capturing? 

**Ariel:** Sure! I've got this little slide that I put together in Markdown, so it’s not anything official. We can bring that up now to take a look at it. I imagine that for a lot of folks out there who are working with virtual machines where they’re running systemd units or Kubernetes, we have different deployment styles here. Some of our main workloads are running in Kubernetes.

The way we have everything set up in our Kubernetes cluster is that we run our own custom mesh. On every worker node, we have a deployment of the OpenTelemetry collector, which we build using OCB. We chose that route because we wanted to ensure that we had the most secure build possible with the minimum number of dependencies. We also wanted the ability to build our own custom processors so that we can address any issues specific to our needs. 

In each of these pods, we also have a mesh sidecar. Ingress traffic comes into the OpenTelemetry collector over HTTP. If you have another application running your service, it shoots over traces to the mesh and sends it to our OpenTelemetry collectors from which we generate span metrics, sample traces, and send those off to a SaaS provider where we aggregate all this data. 

On each individual worker, we’re running in a hybrid world right now. We’re only leveraging OpenTelemetry for traces and generating span metric data. We’re still living in a world where we’re using non-OpenTelemetry formats for collecting things like custom metrics, system metrics, and logs. On each of our worker nodes, we have a metrics agent that can speak various protocols, mostly using OpenMetrics or StatsD to collect data, aggregate it, and send it off to the SaaS provider. 

In addition to that, on all of our worker nodes, we have Fluent Bit running, and that’s what we’re using to collect our logs. All of our logs end up getting streamed out through Azure Event Hubs, processed by a bunch of consumers, and sent off to our log store and search system. 

In addition to these Kubernetes workloads, we have our own virtual machines where we run systemd units, such as Git file system services. Those are running on systemd, and we have the metrics agent and Fluent running there. We don’t yet have the OpenTelemetry collector deployed there, but that’s where we want to get to. We want our entire platform to run on open-source software—the OpenTelemetry collector—and we’re converging on OpenTelemetry for all these formats. 

Some facts about our trace data: we’ve rolled out OpenTelemetry SDKs for different programming languages, including Ruby, Go, JavaScript, Node.js, and .NET. We had some experimental Rust usage but had to scale that back, and we had some Java experiments that didn’t pan out. 

A lot of what we do started before any of the automatic instrumentation was available for some of these languages, so we’re still deploying them through wrapper libraries. We maintain a set of wrapper libraries that give you the minimal defaults we require. We’ll do periodic updates of those through Dependabot, so as we roll out new versions, we keep everything updated. 

I feel like I’ve said a lot so far. Reys, do you have any follow-up questions for me? 

**Reys:** I actually have so many! You mentioned you’re leveraging OpenTelemetry for traces right now, and you talked a little bit about the plans and some of the things you tried. I was curious to find out more about that. Is that mainly because you’re primarily a Ruby shop, and maybe the signals for metrics and logs aren’t quite as mature yet? Is that the reason? 

**Ariel:** That’s one of the reasons. One of the other challenges I didn’t discuss regarding adoption is that even though OpenTelemetry says, “Hey, this is what the signals look like; this is what the semantic attributes are like,” there’s a lag between the time that something is published or declared in the OpenTelemetry spec and the time that vendors or open-source platforms can leverage those things. 

There’s a feedback loop as we go through OpenTelemetry and say, "Hey, we want to try this new thing out." We do a couple of things in experimental languages, but it was a big enough challenge for us to start rolling tracing out and adopting semantic conventions for logs that we didn’t want to take on yet another thing. 

As you said, some of the SDKs are ahead of others. With Ruby, we recently got help from our amazing maintainers—Schwan and AAA—who have been working diligently to get the metrics SDK up to speed for Ruby. So, that’s something that wasn’t available to us to use. 

One of the biggest advantages I see in tracing is the ability to generate span metrics from traces. The fewer things we impose on our users for them to try to figure out, the better for us, right? 

**Reys:** Are you using the span metrics connector? 

**Ariel:** We’re using a custom connector right now. I’d like to get us to the point where we’re using a span metrics connector, but we don’t have egress in OpenTelemetry right now. We’re still using vendor proprietary formats for export. What I want to get to is that one of the biggest strengths of OpenTelemetry is that it provides a standard format that we can make things portable for us.

**Reys:** So it sounds like the plan is to migrate your other signals over once those reach GA in languages? 

**Ariel:** Absolutely! Once I have more time on the calendar, right? We have so many projects to do as engineers and companies, and it’s like, "Hey, where do we fit these in?" As you can imagine, GitHub is constantly growing every day. We just hit 150 million users, which is amazing growth for the company. So we’re here to support that volume growth and support our end users.

**Reys:** I hope that answers your question! 

**Ariel:** Absolutely! I also want to mention real quick for those who joined us a little bit later: if you have questions that come up, feel free to pop them into the live chat of whatever platform you’re watching from, whether it’s YouTube or LinkedIn. Go ahead and pop the question in, and we will try to get to them throughout the show.

I noticed on your second slide you mentioned probabilistic sampling, so it sounds like you’re not doing any kind of tail sampling right now. 

**Ariel:** No, not at the moment. One of the hard things about tail sampling is that the traces all have to go to the same collector to make that decision if you’re using the collector for tail sampling. Right now, we wanted to reduce that burden in complexity immediately, and when we started with probabilistic sampling, some of the things we want to do in the future is more advanced remote sampling. 

We want to have more fine-grained control over what we’re looking at and be able to leverage tail sampling rules, but as you know, that’s very difficult to implement and scale in our case. We have about 2,000 collectors supporting everything right now across our fleet of about 14,000 hosts. 

Right now, we’re doing about 26 million spans per second, and just yesterday during our peak times, we hit our all-time high of 32 million spans per second as we continue to grow our volume. This is one of the challenges we’ve had, and tail sampling is definitely on my list of things I want to do.

**Reys:** At what point do you think you might get to the point where you could implement tail sampling? 

**Ariel:** That’s a tough question for me to answer because it’s on the pile of the list of things I want to do towards the bottom. So, it’s like, "When can we do it?" When I get to all my other wish list items! 

I’m also curious about the experiments you mentioned in Java and Rust that didn’t quite pan out. What was it that didn’t meet your expectations? 

**Ariel:** We’ve reduced the number of Java workloads we’re running, so those have been migrated over to different programming languages. That’s one reason we didn’t continue to roll out JVM workloads; we just didn’t have the return on investment we wanted. 

We also have a very limited set of applications that run Rust, and those ran into challenges with how it impacted their latency when they introduced the use of the SDK. I’m going to be honest with you—I’m not a Rust expert and couldn’t get in there to help address some of those problems and report them upstream. That was something we had to put on pause because, again, it was one program versus all these other services that are running in Go, Ruby, and JavaScript that we need to pay attention to.

**Reys:** What was GitHub's observability tool before migrating to OpenTelemetry? 

**Ariel:** We were using proprietary SDKs for OpenTracing, which was how we collected traces before. Generally speaking, GitHub is a huge StatsD metrics user still to this day. Logs are a big part of what we utilize here, so folks are looking at exception stack traces a lot of the time to try to understand where errors are coming from. 

They’re looking at access log streams and trying to piece together where the request is going and where it’s slowing down. Then I showed up with my "magic tool" for distributed tracing and said, "Hey, look, here it is on the flame graph, or an ice cycle graph, or here's a waterfall view of this thing." People started looking into that as an additional tool in their toolbox to help debug things during an incident.

**Reys:** How have things changed since GitHub switched to OpenTelemetry? 

**Ariel:** This year alone, we’ve seen a dramatic increase in the number of people using tracing and the number of services that have been instrumented. Part of that comes from the fact that I said, "Hey everybody, we’re all moving to this new SDK, so everyone install it," and let Dependabot do these installations on your apps. 

People started seeing the value of this investment, so they started to use it more. When we started this adventure, only about 80 services were instrumented, and now we’re closer to about 300 of those services, which effectively are Kubernetes deployment types or systemd types. 

The monolith itself has broken down into eight services, like web UI, API, GraphQL, and background workers. We saw quite an increase in the number of services that came in. We went from doing something like 5 million spans per second to now 32 million spans per second, which is just a huge increase in volume and usage.

**Reys:** Along with that increase in usage and data volume, how has that impacted how your services are running and how quickly your team can debug issues as they arise? 

**Ariel:** Part of it is education. So I mentioned that I’m on the observability team, but really we’re two groups. One of the groups is called the Experience Team, which works directly with teams. They operate in a role sort of like developer relations and advocacy but also work on cost control and improving your experience. 

They help identify new workflows and introduce them into your incident command experience. Those teams set up education sessions to bring folks on board, but we try to do them not during an incident because that puts some stress on people. 

When an incident is happening, we say, "Hey, here are some insights we’re gaining from the traces that you might not be able to see somewhere else." That has helped in many cases where we couldn’t pinpoint what was going on. There are also spots where not every system has been instrumented. We often rely on client metrics or client trace data and say, "Hey, look, this client is experiencing this problem. Can we take a deeper look at this other service that hasn’t yet been instrumented?"

It’s been sort of a mixed bag. Some teams have identified issues even before they go out to production, while other teams have leveraged it when it’s like, "Oh, we’re having an incident right now. Here’s the reason why this is failing." We’ve been able to identify bottlenecks and little coding mistakes, like forgetting to ack the message before pulling it out of Kafka, or the client timeout not matching the server timeout. 

**Reys:** That’s fascinating. One more question from the main section: What would prevent you from implementing better telemetry?

**Ariel:** I think it’s because there’s so much that’s in early stages. We were early adopters of a lot of things, but there’s a lot more that’s risky for us to roll out. For example, one of the things I’m really excited about since I came back from KubeCon is the continuous profiler. 

If I could roll that out today, that’s what I would want to do. But because we’re still in the early stages of the profiler and the specification, and there’s still some churn with the data model, we don’t want to take that risk by going too early with those tools. 

Also, there’s not a lot of support from our current vendors that will leverage that. It’s kind of like we would be experimenting with that to go nowhere. Once vendors start to support the OpenTelemetry profiling format data model, I’d love to jump in there and roll that out more widely. 

Additionally, we have a bit of a bumpy road ahead in migrating semantic conventions from pre-1.0. We were early adopters of semantic conventions, so we’re at this pre-1.0 stage. Getting ourselves to migrate toward a 1.x version of semantic conventions is another big challenge because we’ve sent all this data out to our backends. We have to figure out a way to upgrade it or say, "Oh, this is version X." 

It feels like a lot of progress was stalled there for the moment. 

**Reys:** I think that answers your question! 

**Ariel:** Absolutely! I think this is another interesting topic—migrating some semantic conventions. I might jump back if we have time later, but I do want to get to some of the community questions!

Yes ma’am! One of the big things I think you’ve probably seen is that people want to contribute to the project but aren’t really sure where to get started. We have resources such as getting started in the documentation, various SIG channels, and the CNCF Slack. I think we all have the problem of how to reach these people because there are so many.

What was the contribution process for you and your team like for OpenTelemetry? 

**Ariel:** For us, it was a short process with a lot of observation. The things we looked for—myself and my teammates when we approached the community—were the code of conduct and the expectations of the code of conduct. We actually looked at previous PRs and the feedback that was in those PRs to see if the behaviors expected in the code of conduct were reflected in the PRs and issues. 

There’s nothing worse than wanting to be part of this community and finding that those two things don’t match. When we saw that, we participated in the SIGs, which were very nice for us because the Ruby SIG was in US hours. We joined those meetings during the day and started to provide feedback as end users. 

If we identified something that was a challenge for us, we could contribute a PR. The maintainers were very generous with their time, did their reviews, and we were able to get a couple of custom propagators merged and some instrumentations merged. 

We identified other gaps for the instrumentations we use because one of the biggest challenges as a maintainer is that there’s so much to do. You’ve got the SDK, the API, documentation, and language-specific instrumentations. That’s daunting, especially with all the popular libraries out there. 

We focused our energy and contributions back to the libraries we use heavily at GitHub, which are sort of "blessed" by their popularity. Anytime someone comes to me and says, "Hey, I have this challenge with this thing; it’s missing this attribute or I’d really like it to work like this," I say, "Let’s open up an issue and work on this together. I’d love to review your PR." 

Creating an environment that lowers the barrier to entry for folks to collaborate and submit contributions is important. I want them to feel like this is ours, not mine, and I’m gatekeeping. We still have some standards for quality, obviously, and some expectations from you as a maintainer, but we try to make this a painless experience for you in the Ruby community. 

That’s what the contribution process was like for me and getting involved in the community. 

**Reys:** That’s awesome! So you started by joining the SIG meetings as an end user and sharing feedback, then found information about how to open an issue, and from there built custom components and helped the community build out more of their components. 

Do you have any feedback for the SIG on ways to improve how things work based on your experiences with Ruby and with other SIGs? 

**Ariel:** Yes, I want to thank everybody for their amazing work contributing to the collector, in particular. We release our OCB build every time a new version of the collector package rolls out. Every week or two, Dependabot tells us, "Hey, it’s time to upgrade; a new release has rolled out." 

We ran into some challenges where we identified performance issues in the collector. We worked with the team, provided profiles, and gave feedback. They were very responsive to our concerns, and they were happy to see we could contribute back just by providing feedback and actual data from production workloads. 

That’s very difficult to do as a maintainer; it’s hard for me to know what’s going on in a customer’s deployment or user deployment. So that spirit of collaboration was great, and that’s what I’d like to see happen in all the other SIGs. 

I have limited experience with other SIGs. The other part was contributing to Semantic Conventions. I contributed to Semantic Conventions to introduce some new attributes, but there’s so much churn in that repository that it was hard for me to keep up with changes and deprecations. 

That was a little bit on me to keep up and find out that the stuff I submitted was deprecated in favor of something else. It’s those kinds of challenges that I’d like to find a better way for us all to keep up with each other. 

One suggestion I might have is: when a change happens, here’s an OpenTelemetry change. Open issues for every maintainer repo and say, "Here’s a new thing that has changed; this is a new feature we expect the SDK to support." This way, we can track it because right now, the way it works is that we have someone who’s a representative at the SIG spec. They collect some data, come back to us, and say, "Hey, we need to implement this other thing." 

Sometimes we’re not diligent about project management; we’re individual contributors doing this in our spare time. We need to improve overall at being able to keep each other informed and track work. 

**Reys:** Got it! So, making sure everyone’s on the same page at the same time is a bit daunting. 

What would you say are the things you interact with most right now? 

**Ariel:** Right now, I almost exclusively participate in the Ruby SIG. I don’t have a lot of interaction with other SIGs at the moment, other than through an occasional issue or discussion that pops up. I’ll jump into a channel and say, "Hey, I’ve run into this problem," or "I have this question for you; can you help me?" 

I have limited interactions these days with other SIGs. Just recently, I met with the End User SIG at KubeCon, so I’ll probably get more involved now that I have this special invitation from you! 

**Reys:** I’m so excited! This brings us to our "Turn the Tables" section where you get to ask me or anyone on the call questions, and we’ll see if I can answer them or if anyone else can answer them. 

**Ariel:** For me, Reys, how do newcomers get involved, particularly in the End User SIG, to provide feedback? How do they get involved in other SIGs that you’ve been involved with? 

**Reys:** There are many different ways to contribute! I think a lot of people, when they think of contributing, think, "Oh, I have to open PRs; I have to write code." That’s not necessarily the case. There are so many different ways to provide feedback. 

One way you mentioned is that you just started going to a SIG meeting and sharing things you’re seeing. That’s an absolutely great way to contribute to the project, and that’s what the End User SIG is all about. One of the things we’re all about is gathering feedback to give back to the appropriate SIG so we can help improve things. 

We do things like surveys and open Q&A interview sessions where we dive deeper into the adoption and implementation process and find out your pain points and specific feedback. You can contribute blogs—maybe you love writing documentation! 

If you’re not already part of the CNCF Slack instance, I would join. I just realized I did not include that link, but we will have it in the show notes. Once you’re a member of the CNCF Slack instance, you can scan the QR code to join the OpenTelemetry SIG End User channel, and we will be happy to direct you if you have questions about your implementation or if you just want to know, "Hey, where do I get started?" 

We also have a survey going, which I also just realized I did not share the link to! I’ll see if I can get that right now. 

So yes, join the SIG meetings! Where can they access the calendar? 

**Ariel:** It’s through the community repository that has links to the calendar. 

**Reys:** Perfect! If there’s a specific language you’re already working in, check out the calendar to find out when the SIG meets, and just hop on. 

When I first started, I was like, "I’m just going to check out a few different SIGs." I went to the Python SIG, and everyone there was so welcoming and nice. I was so intimidated at first, but then I realized, "Wow, these people are so lovely!" That’s been my experience in almost every SIG meeting I’ve jumped into—everyone is so open to answering questions and wants your help. 

It’s important that we make it clear that contributing doesn’t have to mean opening PRs. If that’s what you want to do and are able to, we would love that, but there are so many other ways to contribute, like providing feedback, donations, blogs, doing interviews, and surveys. 

Every little bit counts, right? Every little contribution can help us. If your contribution helps you, it’ll help the community. So, come if you’re having trouble with a specific problem. I guarantee you at least a dozen other people are having the same problem. If you don’t understand something, I guarantee at least a hundred other people don’t understand it either!

**Ariel:** I really appreciate you answering my questions here. 

**Reys:** You are so welcome! I think we’re right about on time. 

If anyone has any last-minute questions, we’ll hang out here for another minute and give people an opportunity to share if they would like. Otherwise, I’ll just ask about any fun Christmas or New Year plans?

**Ariel:** We’re going to be spending it with family, so that will be really nice. We’ll be traveling, so that might be a little hectic. We are visiting my father, and we are traveling on Christmas Eve, so that should be fun! 

I’m going to show up there and we’ll be singing Puerto Rican Christmas carols at the top of our lungs, waking up the neighbors as soon as we arrive! 

**Reys:** Oh my gosh! How about you? What are your plans for the holiday season? 

**Ariel:** I’m going to be at home working on house projects that have been on my to-do list for three years! 

**Reys:** Nothing like it, right? You’re going to get to the tail sampling project, right? 

**Ariel:** Oh my gosh, yes! I’m really excited about it because I moved into this house about three years ago, and it’s like 85% done. 

**Reys:** Just don’t look in the cabinets; that’s a project I need to organize! 

**Ariel:** Exactly! And that’s the thing I tell people all the time: "Hey, don’t look inside the repositories! You may not like how things are arranged, but I know where things are!" 

Thank you so much, Ariel! You were a fantastic guest. I’m really excited to learn more. As I said, I do have some follow-up questions, so I’ll be reaching out to learn more about your adoption processes, as well as some of the other components you mentioned you used, such as OCB. 

If anyone wants to reach out to either of us, you can find us in the CNCF Slack OpenTelemetry SIG End User channel. Again, the link is up here for you to scan, and I believe we will have the information in the show notes as well, which will be posted right after this. 

Thank you so much for joining us, and we look forward to seeing you all again next time! 

**Ariel:** Thank you for having me, Reys! This was really awesome. Thanks, everybody, for watching wherever you are and whenever you are. I hope to see you all in a SIG room sometime!

**Reys:** Yes, and happy everything to everyone! Adios!

[Music]

## Raw YouTube Transcript

[Music] hello everyone welcome to a brand new episode of oh tell me and end user Q&A if you have been to one of these sessions in the past you might notice that this looks a little bit different and it has a bit of a different name that's right we have done some growing up since the last few times and we're very excited to debut this new look um but don't worry almost everything else is going to be the same we have a great interview here for you today to learn from um but before I introduce myself and our guest I would love to know where everyone is connecting from I am online from Portland Oregon today and would love to see where um people are from it looks like someone is here from Brooklyn New York so thank you so much for joining out in New York and so my name is reys I am a senior developer relations engineer at New Relic I also co-lead the ner Sig um which is our cute little logo that you see um up in the right corner and yeah um at the end of your Sig we really focus on connecting directly with users and we are dedicated to helping um the rest of the sigs um get feedback from end users so they can help improve the project um and to that end this is one of those uh events that we host to do that so we are going to have um Ariel on Ariel if you would like to join us and introduce yourself hey how's it going everybody um Ariel Valentin I'm a Staff software engineer at GitHub working on observability thanks so much ree for inviting me here to chat with you today as absolute honor to be the first in the new format also so um you know big shout out to you know everybody who's uh done work to try to get this um this uh new platform up and running so thank you oh no thank you for being here I'm really excited um so just to get you and everyone up to speed on the format um it's similar to the ones if you've been on one of these before but we're going to start with some warmup questions where we'll kind of massage Ariel into being comfortable and then we'll hit him with some meaty questions and then we will actually um also get into questions more around the open c community and where he'll have an opportunity to share um like feedback about um his experiences with contributing um using the project stuff like that and then he'll get the chance to ask us questions as well um in a little section we call turn the tables and the very end if there are any audience questions um oh actually scratch that if you have questions that come up during our conversation feel free to put them into the chat if you're watching from YouTube then just put them in the live chat and then I think LinkedIn will also have its own chat so put in your question there and we will get to them as we can throughout the conversation um and then at the end if you have questions at the end as well then we will get to those but yeah if you have any questions that pop up as Ariel as telling us his story we definitely want to get to those as they come in all right so I think we are good to get started so adiel ad told us a little bit about your role of the company um how did you get started with observability how did you get started with open Telemetry oh those are great questions I mean I I should have mentioned that I'm here in Austin Texas Sunny Austin Texas uh so you know where I am in the world um I think a lot of us started originally um uh and I don't think my experience is unique is working with sort of like APM tools uh which are vendor proprietary tools uh generally speaking that uh didn't have distributed tracing in place um at that time and as the community evolved and open tracing became a standard uh that was my first experience with working with distributed tracing was uh the open tracing uh um specification and uh working and trying out different vendors different experiences with uh open tracing and then um when by the time that I had gotten to GitHub um I was a champion for distributed tracing because I saw the power that was in there and around that same time folks were already moving towards developing and transitioning away from open tracing and open sensus to open Elementary so I got really uh involved very early on in trying to spread the word and learning more um and working with uh my obser my team which was the observability team to uh start to adopt um tracing more to embrace it more and to make otel sort of our North Star um for all of our Telemetry signals I feel like I keep saying the word Telemetry over and over again I'm going to have to find yeah me a lot of that we're both going to stumble over over open Telemetry at some point or distribute tracing at some point it's all right this is a safe [Laughter] space what um okay so you mentioned you became a champion for distribute tracing um and you know I think something that's interesting is you know we're so ins scon in the world of observability that we at least I you know tend to presume everyone understands what that is um and it's always surprising to me when someone's like what's distributed tracing and they working um what so kind of along with that what um do you think that's kind of a main Challenge and you think most organizations organizations face when it comes to observability is not just understanding the value of distribute tracing and yeah because these are all sort of new Concepts to folks right uh you know folks have their different levels of understanding of the tools that are available to them and so there's all this vocabul that you hear that's a little bit hard to to parse through so sometimes it's like oh when you say Trace do you mean like a trace log log level is at the trace level uh when you say Trace do you mean the samples taken from a profiler when you say so there's a lot of this sort of uh language that um even though we're we're uh converging on a lot of this language and we uh have these dictionaries that are defined and published everywhere everywhere there's still sort of like this uh there's a hump that we have to get over or like a challenge that we have with trying to get everybody speaking um um the same language within the same context right very similar to in domain driven design we have these bounded context where the same term means a different thing you know that that flows over into sort of our domain language when it comes to observability and Sr practices um and and I'm sure many folks have faced that the those challenges as well it's kind of like let's all get on the same page about what we mean right oh absolutely um what are currently some of the most interesting problems that you are facing in your role oh well I mean one of those things is uh is that transition right um it's really hard for an organization like us who's been around for a long time um and I say a long time but you know whatever it is 10 years um where the system has grown and involved there have been Acquisitions that have been brought together uh there's disparate kind of uh uh backends where we're collecting this data um it's it's uh trying to transition from one way of doing things to a new way of doing things right so it's learning something new that's always going to be a big Challenge and you know folks ever are are trying to do their job every day they're not trying to learn new sdks or trying to learn new vocabulary or trying to learn how to be um proficient in their back ends what they're trying to do is keep the system up and running and keep our customers happy right so I think you know those are some of the the the challenges that that we all you know I know I face it I'm sure others do which is um is is making these transitions with the fewest pain points as possible with like trying to avoid these paying points um and there's I mean there's so many more we can go on and on Reese but I I think right now that's kind of like one of the biggest challenges for adoption I think overall that's actually um that's a great segue into the mey section so what was the process like for GitHub to adopt open Telemetry so uh for us it was um the the role of the advocate right so I acted as an advocate and was a champion for otel at the company and I specifically was brought in to help Advance the mission of tracing and uh it was something that I had pitched and and also sort there there were uh other challenges that we faced too which was hey let's get everybody um uh building a data dictionary let's get everybody uh agreeing to the same language when it comes to what our attributes were going to be right because as you can imagine as a system of walls or Acquisitions or other teams are rolled in everybody has their own uh you know log attributes or their own metric attributes or whatever it is and and so we started to I said look here's a Northstar right here here's semantic conventions from otel let's anchor onto this and let's follow the rules around semantic convention so that we can all uh build up our own internal dictionary right and that that comes with its own challenges right scheme of migrations and trying to keep up to date with the spec and you know what the uh instrumentations are doing right and um and as an advocate you know was also uh a lead on the roll out so one of the things I wanted to do was sunset all of our old sdks and move over to our open source sdks um and you know myself and a few other members for my team we all were involved in um otel Ruby for example because you know U we're a big Ruby shop uh but uh we got involved in there to help the instrumentations get better and to help test different um releases of the SDK and get that rolled down into our GitHub monolith so um we you know were very like I said very early adopters ran into some challenges and um and and continue to give feedback um to the community that way as a and I am not only an end user but like I said I'm also a maintainer so um I'm playing multiple roles there you know trying to help the community along yeah that's a really interesting position to be in is as end user and contributor and I definitely want to Circle back on that when we get to the OA community questions um tell us about the architecture landscape at GitHub and the Telemetry that you're capturing sure so um I've got this little slide that I put together in mer marked down so it's not anything official we can bring that up now so we can take a look at it um and I imagine that for a lot of folks out there who are do working um with uh um virtual machines where they're running system D units or if they're running kubernetes we have this this uh uh this uh we have different deployment Styles here but some of our main workloads are uh running in kubernetes and the way uh that we've got everything set up you could see here is as part of our kubernetes cluster you know we run our own custom uh meshing grass um and on every worker node um or um um on the worker nodes we're running a deployment of the open telemetry collector which we uh build using OCB so we have our own version of The otel Collector that is specific to us uh we chose that route because we wanted to ensure that uh we had the most secure uh build possible with the minimum number of dependencies and we also wanted the ability to build our own custom processors so that um we can uh address any issues that we might have that are specific to our needs um in inside of each of these pods you know we also have a a mesh side car and so Ingress traffic is going to come into the hotel collector otel over H ltlp over HTP um and if you have another application that's running your service it's shooting over um OTP traces over to the over the the mesh and sending it to our oel collectors which we then from there generate span metrics and um sample traces and send those off to a SAS provider that has a um where we aggregate all this data um and then on each individual worker you know we're running in this hybrid world right now um we're only um leveraging oel for traces and for generating um a span metric data uh we're still living in a world where we're using nonel non-lp formats for collecting things like custom metrics system metrics and for collecting logs um we have uh on each one of our worker nodes a metrics agent um and that can speak various different um um protocols but um mostly it's either using open metrics or it's using statsd uh to collect uh data and aggregate it and send it off to the SAS provider um in addition to that on all of our worker nodes we have a fluent bit running and that's what we're using to collect our logs um and all of our logs are end up getting streamed out through Azure event hubs which are uh processed by a bunch of consumers and then sent off to our log Store and search system um in addition to that you know we in addition to these uh kubernetes workloads we have our own virtual machines where we run systemd units you have to think about you know like the git file system Services um those are all running on systemd and we have the metrics agent and fluent running there we don't yet have the otel collector deployed there but um that's where we want to get to we want to get to a world where essentially our entire platform is running um an open source software the open Telemetry collector and um we're converging on OTP for all these formats uh so those services are also using statsd open metrics uh flu bits pulling things out of Journal D because we you know um um that's where we're streaming all of our our logs of the system Journal so um and all that stuff again goes right through to the same channels and it all gets aggregated in these places where we do our work um a little bit about some facts about sort of our our uh Trace data is that um we've rolled out the otel sdks for different programming languages we have them for Ruby goang uh JavaScript nodejs net we had some experimental uh uh rust usage but um uh uh we've had to scale that back um and we had some Java experiments that didn't pan out we didn't roll those out uh to production um and a lot of the stuff that we do when we we started before any of the automatic instrumentation was available for some of these languages so we're still deploying them through rapper libraries so we maintain a set of rapper libraries you install those it gives you the sort of the what we consider to be the minimal defaults um that we require for for our needs and uh we'll do periodic updates of those through dependabot right so it's like as we roll out new vers versions of those things um I feel like I've said a lot so far and I don't know ree if you had any follow-up questions for me or I actually have so many but oh okay great um so you mentioned and um so you mentioned you're leveraging open for traces right now um and then you talked a little bit about you know um the plans and some of the stuff that you tried um and I was curious to find out more about um is that mainly because if you're primarily a ruby shop and you know the SK maybe the signals for metrics and logs aren't quite as mature yet is that the reason or that's one of the reasons so you know one of the other challenges that I didn't discuss when you asked about adoption challenges is even though oel says hey this is what the signals look like this is what the semantic attributes are like this is um there's a lag between the time that something is published or declared in the oel spec to the time that um vendors or open-source platforms are able to leverage those things and there's sort of like this feedback loop as we go through otps and say hey we want to try this new thing out we do a couple things in experimental languages we'll do this stuff and um you know it's it was a a big enough challenge for us to start to roll tracing out that we didn't you know and adopting semom for logs that we didn't want to take yet another thing on which was to say okay now we're going to switch over to Native OTP metrics as well um and like you said some of the sdks are ahead than others so with Ruby we're just recently you know with the help of our amazing maintainers um Schwan and um and AAA they've been working diligently to get the metric SDK uh up to speed for Ruby so that so that's something that wasn't available to us to use um and I think that one of the the biggest advantages that I that that I see in tracing is the ability to generate span metrics from traces it's like hey the the the less things that we have to impose on our users for them to try to figure out how to do the better for us right and are you using the spam metrics connector is that what you we're using a custom connector right now I'd like to be able to get us to the point where we're using a span metrics connector but we don't have egress in OTP right now we're we're still doing sort of like vendor proprietary formats for uh for export where I want to get to is you know that's the strength one of the biggest strengths of the of otel is OTP is that standard format that we can that makes things portable for us absolutely so that's definitely sort of high on my list of things that I want gotta okay so it sounds like the plan is to migrate your uh other signals over once those reach GA in languages oh certainly and you know once I have more time on the calendar too right we have so many projects that we have to do as engineers and companies and it's like hey where do we fit these one these in right um uh as you imagine you know GitHub is constantly growing every day um we just hit 150 million users uh you know so it's like just an amazing um amazing growth of the company so shout out to all the people at GitHub who've been working so diligently to make this happen you know um and so we're here to support we we're here to support that volume growth and support our end users so um um so I hope that answers your question absolutely um and also want to mention again real quick for those um who joined us a little bit later feel free if you have questions that come up um you want to learn more about something that Ariel has gone over feel free to pop it into the live chat of whatever platform you're watching from so whether it's YouTube LinkedIn um go ahead and pop the question in and we will try to get to them um throughout the show as you can um so I noticed on your second slide you mentioned probabilistic sampling so it sounds like you're not doing any kind of tail sampling no not at the moment so would you mind bringing that up again uh um L the second slide yeah that's awesome yeah so we started with probabilistic sampling one of the hard things about tail sampling um as you can imagine is that um the traces all have to go to the same collector in order to make that decision if you're using the collector for uh for for tail sampling so um right now we wanted to reduce that burden in complexity immediately um and we um when we started with probabilistic sampling some of the things that we want to do in the future is um more advanced remote sampling uh so that we can have um uh more Frank grain control of what we're looking at we want to be able to do leverage uh tail sampling rules but as you know that's very difficult to implement a little bit difficult to scale in our case because you know we have about 2,000 collectors which are supporting everything right now um across all of our our Fleet right of like 14,000 hosts or something like that and I'm making that number up off the top of my head I think that's the last number we had um and you know right now we're we're doing about 26 million spans per second and uh just yesterday during our peak times you know we hit our alltime high of 32 million spans per second as we continue to um uh to grow our volume so this is one of the the challenges that we've had and so on my list of all the things that I want to do tail sampling is definitely on there you know what I mean so okay at what point do you think you'll you might get to the point where you could Implement heill sampling would be more like side when the you know processor has been uh more develops or devs it's you know it's it's uh I don't know how to answer that question because it's on the pile of the list of things that I want to do towards the bottom so it's like when can we do it when I get to all my other wish list items got it got it um and although I am curious about the other but that's okay that's all right questions that I want to get to as well sure um also just going back uh you mentioned trying some experiments in like Java and Russ I didn't kind of pan out and I was just curious um what was it that didn't quite work out or that you know didn't meet your expectations well uh We've reduced the number I'm sorry um all the jvm people out there we've reduced the number of java workloads that we've uh that we're running um so those have been migrated over to different programming languages um so that's one of the reasons why we didn't go through and say oh let me continue to roll out uh jvm work like the we just didn't have the return on investment that that we wanted to you know uh try and um also like we don't have a lot of um uh the same thing was for rust is that we have like a very limited set of applications that run rust and um those are perform for performance reasons they ran into some challenges with um how it impacted their latency when they introduce the use of the SDK and I'm going to be honest with you me not being a rust expert and being able to get in there and get my hands dirty to try to help out with addressing some of those problems and Reporting them Upstream that was something that we had to put on pause cuz again it's like one program versus all of these other services that are running and go and Ruby and um and JavaScript that we need to pay attention to right and um so that that I think that's really where it was where sort of like a critical mass of application services that use these programming languages and was like hey we have to focus our attention ption on those that are going to get us a higher return on investment for this absolutely okay so what was oh yeah what was github's observability tool before migrating to I think you mentioned um you were using proprietary sdks uh yeah I mean we were use proprietary SDK for open tracing so you know open tracing was our uh how we collected traces before um but um generally speaking the I I I I think I covered this in the first slide GitHub is a huge uh sort of statsd metrics user still to this day um and um logs are a big part of uh of what we utilize here so it's like folks are looking at acception stack traces a lot of the times to try to understand where errors are coming from they're looking at um uh access log streams um and they're trying to and they were trying to piece together Hey where's the request going and where is it slowing down and this and that and I showed up with my you know magic Tool Distributor tracing I'm like hey look here's a here it is on the flame graph or an icycle graph or here's a waterfall view of this thing and it's like oh that's pretty cool and um people started looking into that as uh as a an additional tool in their tool box for them to help try to debug things during an incident um so I hope that answers your question there yes and kind of along those lines how have things changed since GitHub switch to open cemetry so uh We've what I'll tell you is that uh just this year alone we've seen a dramatic I wish I had these statistics off hand but we've seen a dramatic increase in the number of people using tracing and a number of services that have been instrumented part of that comes from the fact that I was like hey everybody we're all moving to this new SDK so everybody install it and let the pendot go ahead and do these installations on your on your apps and people started seeing the value of this uh of this investment as well so they started to use it more um but we um I want to say that like when we started this adventure uh only about uh 80 and I'm going to use the word services in air quotes only about 80 Services were uh instrumented and now uh we're closer to about 300 of those Services which are effectively like kubernetes deployment types or like system D types uh because as you imagine our monolith has th you know uh maybe a thousand services within it so there're like subservices in there um but uh the but the monolith itself has broken down into like eight Services as like you know web UI and API and graphql and background workers and stream processors and whatnot so um that's why I put them in air quotes as these are Services um but we saw that that that was quite an increase in the number of services that that came in we were doing something like uh uh like 5 million spans per second to now we're up to 32 million spans per second it's just a huge uh uh increase in volume in usage and along with that volume increase in usage and data volume um how would you say that's impacted how your services are running how um you know quickly your team is able to debug issues as they arise part of it is an education think so a lot of things that uh so I mentioned that I'm on the observability team but really we're two groups uh one of the groups is called The Experience team that works directly with teams and they're they operate in a role sort of like developer relations and advocacy but also working on um you know cost control and improving your experience uh helping you with identify new workflows and introduce them into uh your incident command experience um those those those teams set up sessions education sessions to bring folks on board uh but we try to do them not during an incident um because that that that puts some stress and so what we try to do is like you know we'll go into the if an incident is happening it's like hey here's some insights that we're gaining and we'll share with you that we're seeing uh from the traces that you might not be able to see you somewhere else and that has helped in a lot of cases uh uh where we couldn't exactly pinpoint what was going on uh but then um there's also these spots where not every system has been instrumented so we have to rely a lot on sort of like client metrics or client Trace data and say hey look this client is experiencing this problem can we take a look deeper at this other service that hasn't yet been instrumented um and so it's been sort of I'm going to say like this mixed results some teams have like identified issues even before they go out out to production and other teams have been able to leverage it when it's like oh this we're having an incident right now oh here's uh uh and and here's the here's the reason why this is failing um and and we've able to do things like identify bottlenecks and like mistakes that you know little little coding mistakes oh I forgot to act the message before I pulled it out of kavka oh I'm just retrying that same message millions of times or um or oh the client timeout doesn't match the server timeout so the client out and the server is continuing to turn along to try to do this request so we're identifying things like that that we couldn't identify before easily easily you know um and so those are some anecdotes oh no that's awesome I think I will probably be asking you more about that because I think this is really interesting um one more question from our mey section what would prevent you from implementing better Telemetry what would prevent me from doing that I think it's because there's so much of the stuff that's in early stages and you know we were early adopters on a lot of things but then there's a lot of stuff that's a lot more risky for us to try to roll out so for example one of the things I'm really excited about when I came back from cubec con is the continuous profiler I mean that is one one of the when you talk about one of the things on my wish list that we'd be able to use today if we could uh that's the thing that I would want to do but because we're still in the early stages of the of the profiler and the specification and there's still uh some turn with the data model um I think that there are when it comes to the resource intensive or sort of like introspective tools like that it's I we don't want to take that risk going a little bit too early to adopt those tools uh because also there's not a lot of support from our current U vends and vendors that will be able to leverage that it's kind of like we would be experimenting with that to go to nowhere and so that's kind of you know once vendors start to support the OTP profiling format um data model I should say and once the profiler is a little bit more stable I'd love to jump in there and and be able to roll that out a little more widely um and um I think that uh a little bit of a a a bumpy road for us to still is migrating um semom from pre 1.0 because we were early adopters of semom so we're at this pre 1.0 stage and sort and getting ourselves to migrate towards a a 1x version of of semom is uh another big challenge because we've already sent all of this data out it's our backends there we have to figure out a way to upgrade it or to say you know oh this is version X um and and I and I kind of it feels like a lot of the progress was stalled there for the moment um so that there I don't know that there's any real solutions in um available that would say okay we can start migrating these schemas over and keep our user experience very high quality um if that answers your question oh absolutely I think that is another interesting topic migrating some semantic conventions so I might I might jump back if we have time later but I do want to get to some of these Community questions yes ma'am um yeah so one of the big things I think um you know working worked in the community that uh probably you've seen you know I've seen sure um is people want to contribute to the project but they're not really sure where to get started and yeah we have like resources such as you know I think in the documentation we added you know getting started um and you have you know various uh Sig channels and cncm slack um and I think we all have the problem of how do we how are how do we reach these people because you know there's still some many I don't know so what was the contribution process for you and your team like to open cemetry for us it was um uh it was a a short process with a lot of observation so the things that we looked for uh myself and my teammates at the time when we approached the community we were looking at code of conduct what were the expectations of the code of conduct we went through and actually looked at issues previous PRS the feedback that was in those PRS to see if the behaviors expected in the code of conduct were reflected in the PRS and in the issues uh cuz there's nothing worse than you wanting to be part of this community and finding that those two things don't match and to see that that happened then we um participated in the sigs which was very nice for us to be able and convenient for us in the US because the Ruby Sig was in the US hours we joined those meetings during the day and started to provide some feedback as end users and say hey look we ran into this challenge if we identified something that was a challenge for us we would could contribute a PR um and the maintainers were very generous with their time um did their reviews and um and we were able to get a couple of Uh custom propagators uh merged we got some instrumentations merged we identified other uh gaps for the instrumentations that we use cuz one of the biggest challenges I think as a maintainer is there's so much that you have to do right you've got the SDK you've got the API you've got documentation to do and you have to have language specific instrumentations and uh that's daunting especially with all of the popular libraries that are out so for us it sort of we contributed back to the libraries that we use heavily right we have like a subset of libraries that are very popular at GitHub and um sort of blessed right so by by their popularity so that's where we focused our energy and our contributions um and then we reached out to people you know uh anytime somebody comes to me and says hey I have this challenge with this thing it's missing this attribute or I'd really like it to work like this or I've identified this bug the first thing I do I say is let's open up an instr and let's work on this together and I'd love to review your PR and creating in um an environment that makes it lowers a barrier to entry to have folks uh uh collaborate and submit contributions I want them to feel like this is yours this is ours this is not mine and I'm gatekeeping right we still have some standards for Quality obviously and some expectations from you as a maintainer but we try to make this an a painless experience for you um at least in the Ruby uh um um community so that's what the the contribution process was like for me and getting involved uh in the community that's awesome so you started basically joining the Sig meetings as an end user and sharing feedback and then finding information about like how to you can open uh you know an issue and then like from there building custom stuff and uh helping the community or sorry the Ruby s to build out more of their components okay yeah certainly and like you know with the home for open source right so uh for us um you know we open sourced uh for example new database drivers in Ruby for um MySQL right um and so we were the we had that instrumentation in house before all of that became public and as soon as we released that new driver I went right to the Sig and said Tada I have a donation for you and it's like oh yeah we able we were able to pull that in there so um it's this thing where um there's just this Spirit of collaboration right and this Spirit of supporting open source that um that's important to our mission right okay so it sounds like you had a pretty kind of positive experience it sounds like from for contributing to Ruby um I like to know do you have any feedback to the SS on ways to improve how things work based on your experiences with Ruby and then other sigs I know you mentioned also some other components that you've used but we'll come back to that yeah for sure and I again I I want to thank everybody for their amazing work that they've done um contributing to the collector in particular uh because you know we are uh we release our OCB build or sorry our our uh our custom build every time a new version of the cont package rolls out so um every week depend on bot is telling us or every two weeks it's telling us hey it's time to upgrade a new releases rolled out and uh we ran into uh some challenges where we identified some performance issues um in the in the collector and we uh worked with the team provided profiles provided feedback uh showed them our configurations and they were so responsive to um our concerns and uh they were happy to see that we were able to contribute back just in providing feedback just giving them actual data of production workloads which is very difficult to do right as a maintainer it's really hard for me to know hey what's going on in your um in this customer's deployment or this users uh a deployment we can't replicate it in our in our own environment so um uh that Spirit of collaboration was um was really great and that's the thing that I'd like to see uh happen in all the other SE I have limited experience uh with other sigs the other uh part was contributing to semom um and I contributed to simcom trying to uh introduce some new attributes uh but there's just so much turn in that repository that it was hard for me to keep up with uh changes and deprecations right um and so that was a little bit on me to be able to to keep up and find out that the stuff I had submitted was deprecated for example in favor of something else uh but it's it's those kinds of challenges that I like I I I have to find a better way for us to all be able to to to keep up with each other so one suggestion I might say is hey look a change happened here's an otop A Change happened in the spec that should automatically open issues for every maintainer repo and say Here's a new thing that has changed uh um this is a new uh feature we expect that the SDK should support and that way we're able to track it because right now the way it works is hey we have somebody who's a representative at the Sig spec they collect some data they come back to us and they tell us hey we need to implement this other thing and sometimes you know we're not diligent about project management right we're IC who are doing this in our spare time right um so it you know we don't um we need to improve I think as overall um at being able to keep each other informed and tracking work got so making sure everyone's on the same page at the same time which I can see obviously is a bit of a it's daunting yeah um so I know you mentioned some of the sigs that you have worked with in the past what would you say are the things you interact with most right now right now it's the I almost exclusively participate in the Ruby Sig um I don't have a lot of interaction with other sigs at the moment other than through an occasional issue or a discussion that'll pop up or in slack I'll jump into a channel and say hey I've run into this problem um or I have this question for y'all can you help me um and so um I've limited interactions now these days with with other s and uh just recently I'm meeting the end user Sig at uh cucon um so I'm going to probably get more involved in the end user Sig now that I have this special invitation from you so yes oh I'm so excited um and actually this brings us to our turn the table section where you get to ask me or anyone on the call questions and we'll see if I can answer them or if anyone else can answer them okay so uh for me re it's like that's how I got involved but maybe you know a special sort of formula or the best way to have someone who's new get involved in particular in the end user Sig to provide feedback or or how do they get involved in other s that that you've been involved in oh for sure so there's a lot of different ways to contribute right I think a lot of people for them when they think contribute they think it means oh I got to open PRS I got to be writing code um and that's not necessarily the case there's so many different ways it can provide feedback and you know like one of the ways you mentioned was um you just started out by going to a Sig meeting and sharing some um stuff that you're seeing that is an absolutely great way to contribute to the project and that's what the N Sig is all about well one of the things we're all about is gathering feedback to give back to the appropriate Sig so we can help improve things um so to that end we do things like surveys um open solary Q&A interview sessions um where we kind of dive deeper into the adoption implementation process and find out your pain points and specific feedback um you can contribute blogs maybe love writing documentation is a great way um and probably if you are not already a part of cncf slack instance I would join and I just realized that I did not include that link but um we will have it in the show notes and you can once you are a member of the CNC of slack instance um you can scan the secure code and join the otel Sig end user Channel and we will be happy to direct you um if you have questions about you know your implementation or you just kind of want to know hey where do I get started um I'm trying to do you know we are happy to help um direct you and we currently have a survey going which I also just realized I did not share the link I'm so sorry I'm gonna see if I can get that right now Henrik um and let's see so yes join the Sig meetings um where can they access the calendar it's through the the community Repository has links to the calendar perfect yes so if there's like a specific language that you are already working in um check out the calendar to find out where uh sorry when the Sig meets and just hop on and you know when I first started I was like I'm just GNA check out you know a few different sigs um I went to the python Sig and everyone there was so welcoming and so nice I just like I was so intimidated at first and then I was like oh wow these people are so lovely and that's been my experience with like almost yeah I mean not almost but every Sig meeting that I've jumped into is everyone is so open to answering questions and they want your help you know they they want to help you help them um so I think one of the things is just you know letting people know that it doesn't have to be contributions of course if that's what you want to do and are able to we would love that um but there's so many other different ways um feedback doation blogs doing interviews um surveys and yeah so you know similar related to that is like you know you're saying that hey you can contribute in all these other ways what kind of expectations of commitment do you have from folks who want to participate like um do they have to show up to every single meeting do they have to constantly be reviewing PRS do they have to review you know issues um if they miss a meeting are they like kicked out of the group never to be invited again you know got it I think so for the end s specifically we are a pretty small group right now um so we are always happy to you know if anyone wants to hop in and um if you're an end user and you're like yeah I I have some stories to share perfect please reach out to us in the Sig Channel um we would love to work with you to get something scheduled and talk to you a little bit further um and as for the expectation uh commitments that you asked about I I mean I would hope that you could you know join at least once one Sig meeting a month just so we can you know connect virtually um but even if you have a hard time making this thing meeting because of like other commitments or time zone slack is a great place to get in touch with us we are totally happy to um Converse async over slack um so don't worry about you know if you can't make the meetings um or every meeting and as far as what's needed I think it just depends on the priorities of each individual Sig um for us right now it's really just we're working out we're working on building out um our YouTube channel for instance um we are still working on building out some of the end user resources we have available which there is a link to here um you can scan to see to to learn more about some of the events that we do um but yeah time commitment wise you know we have some contributors that hop on as they can so we'll maybe see them like you know a few times a quarter and honestly that's fantastic you know we don't need someone to be of course it would be great but um if you have obviously your day job uh you know it's taking up most of your time like we completely understand uh you know we would just love to see you coming as you can so don't worry about having to yeah get a number certain number of PRS um each month or hit a certain number of meetings um yeah it's it's like every little bit counts right every little bit helps so if you find yourself interested in trying to contribute back or you don't know where to get started or you feel just like intimidated just know that all of us um we do our best to make the most welcoming communities possible in our sigs and I love that the end user Sig um creates these opportunities for folks to come and just provide feedback a space for them to say like hey I'm having trouble with this or I like this or this was a great experience for me um that's this this part has been very enjoyable for me to be able to share our my personal experience and our my team's experience with you so um uh I really hope that folks who are watching at home hey come on in the the water's fine don't worry and we have if you didn't host him we have booies we have floaties we got all kinds of gear yeah to to help you get started right so it's just a it's the little things every little contribution can help us um if it help if your contribution is there to help you it'll help out the community so um come if you're having trouble a specific problem I guarantee you at least the dozen other people are having the same problem if you don't understand something guarantee you probably at least a hundred other people don't understand it either so for sure for sure so I really appreciate you um answering my questions here oh you are so welcome um and I guess I kind of put this at right about time um in case anyone has any last minute questions we'll hang out here for another minute and just kind of chitchat um and give people an opportunity to share if they would like um otherwise I will just ask about any fun Christmas New Year plans yeah we're going to be just spending it with family so that's going to be really nice um and uh we're going to be traveling so that might be a little bit hectic uh we have to travel uh we're going to go visit my father but uh we are traveling on Christmas Eve so that should be fun right I'm going to show up there and we'll be seeing U poran Christmas carols um at the top of our lungs and waking up the neighbors as soon as we arrive so uh totally fine for me you know oh my gosh how about yourself whoa whoa whoa whoa whoa what are these Puerto Rican Christmas songs and where can I listen to them oh okay you can look at them on on YouTube on Tik Tok they're all over the place but um they're uh um in Puerto Rico you know folks and in other Caribbean island folks will join and make uh something called the trya and have a paranda which is a party basically and it's like a Katamari ball so you go from house to house knocking on doors and then you start singing these songs and the songs are like hey open up the door I'm here to say hello oh you turn the lights on I can see you in there let me in and then you know you have to feed the guests and give them um Beverages and stuff and then um what you do is you take those people whose home you invaded that we call it a we call an asto which is like we show up and um we uh I don't want to the direct translation is more like you've been Stu we're sticking you up we're holding you up but uh we we break into your home and then we bring you and add you to the Kari ball and we move to the next house and we come and you know we'll sing and stuff like that so uh it's really great um it's a lot of fun and so we're looking forward to doing that with our family that sounds so fun what is that um what did you say that was called uh the there's the it's a couple of words the trya the the aalos those are the songs that we will sing and the asalto like the we're we're showing up uninvited that's so that's lovely honestly that sounds fantastic it's a lot of fun it's a lot of fun I'm so excited you have to you have to share pictures I will thank you um and uh maybe videos too who knows what kind of trouble we can get ourselves into yes oh would love to have a video of you singing one of these songs uh there are videos of me on the internet singing songs but not not now uh ree what are you gonna be doing this uh this holiday season I am going to be at home busting out hopefully a bunch of house projects that have been on my to-do list for three years nothing like it right you're going to get to the tail sampling project right that want oh my gosh I have yeah but I'm I'm really excited about it because yeah I moved into this house uh about three years ago and it's like you know 85% there there's still are just don't look in the cabinets that's a project is I need to organize all the inside stuff yeah and that's the thing I tell people all the time it's like hey don't look on the inside of the of the repositories you know you may not know you may not like how the way things are arranged but you know I know where things are so that's totally fine well all right thank you so much ARA you were a fantastic guest um I'm really excited to learn more um as I said I did have some fla questions so I'm sure I'll be slacking you to learn more um even more about um some of your adoption processes as well as um some of the other components you mentioned you had used such as the OCB um but yeah if anyone wants to reach out to either of us you can find us on um in cncf slacks otel Sig and user Channel um again this link is up here um for you to scan and I believe we will have the information in the show notes as well which will be posted I think right after this so yeah thank you so much for joining us and we look forward to seeing you all again next time yeah thank you for having again Reese this was really awesome thanks everybody for watching uh wherever you are and whenever you are um and I hope to see yall in a Sig room sometime oh yes and happy everything to everyone adios [Music]

