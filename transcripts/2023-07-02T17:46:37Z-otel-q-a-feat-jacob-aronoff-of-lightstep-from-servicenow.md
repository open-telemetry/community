# OTel Q&amp;A Feat. Jacob Aronoff of Lightstep (from ServiceNow)

Published on 2023-07-02T17:46:37Z

## Description

Jacob Aronoff, one of the OTel Operator maintainers, joins the #OTel End-User Working Group's Q&A series to talk about how he ...

URL: https://www.youtube.com/watch?v=dpXhgZL9tzU

## Summary

In this YouTube video, a Q&A session features Jacob Aronoff from Lightstep, who discusses his experience with migrating to OpenTelemetry (otel) from OpenTracing and OpenCensus within their organization. Jacob, a staff engineer on the Telemetry pipeline team, shares insights about the challenges and benefits of this migration, including performance improvements and cost savings from eliminating sidecars for metrics. He explains the approach taken, which prioritized safe and gradual migration strategies, and discusses the implications of using a monorepo versus microservices. The conversation also touches upon various deployment strategies for collectors, performance issues encountered, and the importance of keeping up with the latest versions of OpenTelemetry. The session concludes with Jacob expressing interest in sharing more insights on this topic in future talks.

# Hotel Q&A with Jacob Arnoff

[Music]

**Thank you!** Welcome everyone to Hotel Q&A. Thanks for joining us today. We are super lucky to have Jacob Arnoff from Lightstep at ServiceNow with us. It's a great treat because I contacted Jacob after he mentioned he had submitted a CFP to one of the conferences, I believe it was KubeCon, right Jacob?

[Music]

**Jacob:** Yeah, it was a CFP on migrating to OpenTelemetry (otel) from our organization.

**Host:** That's a pretty cool story to tell. I thought it would be great to have you share that experience during this Q&A. It makes for a compelling narrative when an observability vendor talks about using OpenTelemetry themselves in their own products. So here we are! Welcome, Jacob. Would you like to give a quick introduction?

**Jacob:** Sure! Hi, my name is Jacob Arnoff, and I'm a staff engineer at Lightstep on the Telemetry Pipeline team. I've been working on this for almost two years, and the first year of my journey was solely focused on OpenTelemetry migrations—doing them internally and making them easier for customers. 

**Host:** That's interesting! How do you want to proceed? Should we keep it Q&A style, or should I just dive right into the story?

**Jacob:** Let’s see how it evolves organically.

**Host:** Sounds good! Why don’t you start by setting the scene for us?

**Jacob:** When I joined Lightstep, we were still using OpenTracing for tracing, along with a mix of OpenCensus and some hand-rolled StatsD stuff for metrics. This setup meant we had to run a proxy on every single pod in Kubernetes. Since the proxy acts as a sidecar, it required running an additional application alongside each one of our applications, which reads from StatsD and forwards those metrics.

For us, as we were building out a metric solution, that destination was Lightstep. I joined at a time when we knew that OpenTelemetry for metrics was going to be a huge effort for us in the next year or two, and we wanted to reach stability. The internal OpenTelemetry team at Lightstep had been working hard on improvements and really wanted feedback on how to enhance it, so I took on that migration.

We also theorized that this migration would save us a good chunk of money, as we would no longer need to run those relatively expensive StatsD sidecars. I initially planned the migration to be as safe as possible. There are different approaches to migrations like this; you can do an all-in-one go, which is possible for us since we are in a monorepo. However, it’s much riskier because pushing a bug could take everything down. 

For us, it was important not to disrupt application data, as this data is critical for alerting and understanding how our workloads function across all environments. So, we wanted a safe and easy migration. Our initial approach used feature flags to disable the sidecar and enable some code that would swap to OpenTelemetry for metrics, forwarding them to the appropriate destination.

Midway through the migration journey, after testing everything in staging, I noticed some significant performance issues. I reached out to the OpenTelemetry team, and we worked together to alleviate some of those concerns. One major blocker we found was that we heavily used attributes on metrics, and it became tedious to figure out which metrics were using these attributes and how to remove them.

I theorized that a particular code path was the issue. We were converting our internal tagging implementation through OpenTelemetry tags, which had a lot of additional logic that was expensive to execute on every call. I decided there was no better time to start another migration from OpenTracing to OpenTelemetry while we waited for the team to push out more performant code for metrics. 

So, I paused my metrics work and began the tracing migration. For this migration, I chose the all-or-nothing approach, as the OpenTracing to OpenTelemetry path was more known. There were small docs and examples available, and they were backwards compatible, so emitting OpenTelemetry alongside OpenTracing wasn’t a big deal.

I started by ensuring that our propagators were set up correctly. The first step was to make sure all of our plugins worked, which at that time weren’t open-sourced. I executed the migration in one swoop and had to revert a few times in our staging environment, but nothing major. I did encounter a bug regarding in-app sampling, which required implementing a custom sampler. This was actually ten times easier with OpenTelemetry than with OpenTracing, allowing me to eliminate around 1,000 lines of code and some risky hacks.

**Host:** That’s a fantastic story! I have so many questions already. Do you mind taking a quick pause?

**Jacob:** Sure!

**Host:** A couple of comments: this is an interesting story because we see two types of organizations—those with zero code instrumented and those that have dabbled in OpenTracing. It’s great to have a real-life migration story to provide advice on what to do if someone finds themselves in this situation.

You mentioned you’re working in a monorepo. Did you find it more challenging to do the migration in a monorepo than if you were dealing with microservices?

**Jacob:** We do use microservices, but we have a repo of microservices. Since we were going for a big-bang approach, I started with small services owned by my team that had low traffic but were constant enough to provide meaningful data. 

If a service is too low traffic, you might not have enough data to compare against, which is crucial. I wrote a script early on to query different build tags on all metrics. This script would query metrics for service X grouped by release tag. If the standard deviation for the newer build tag is greater than one, there’s likely an issue with your instrumentation library.

Another thing I checked was to ensure that all attributes were present before and after migration. It was essential to confirm that all attributes stayed the same during the trace migration since I was particularly focused on that aspect.

**Host:** That makes sense. Starting from an existing setup gives you a frame of reference, which is a double-edged sword. You know what has been instrumented, but if something is missing, it raises questions.

**Jacob:** Absolutely! One of the tricky parts was dealing with gaps where attributes were missing. Sometimes it was due to a forgotten addition, but other times it stemmed from an upstream library not emitting the necessary data. 

For instance, I did a migration on a gRPC utility package and encountered an issue where there was supposed to be a propagator but it was marked as a "to-do." I reached out to the responsible person to resolve that issue, and it turned out to be a relatively simple fix.

**Host:** That’s a great example. As logs have matured, do you have plans for conversions related to them? The log specification is evolving to replace span events to some extent. 

**Jacob:** That’s outside my recent focus, so I’m not completely sure. I think we would change how we collect logs, but currently, we use Google's logging agent in our GKE cluster, which runs Fluent Bit on every node to send logs to GCP.

**Host:** Speaking of GKE, have you heard about the new telemetry collection feature in Kubernetes?

**Jacob:** Yes, Kubernetes now has the ability to emit OpenTelemetry traces natively, but I’m not sure if we’re collecting those yet. I want to look into whether we can use those to generate better Kubernetes metrics.

**Host:** It sounds like you’re focused on infrastructure metrics, which can be quite painful in their current form.

**Jacob:** Exactly! I prefer using Prometheus APIs for those metrics since they are more ubiquitous in the observability community. However, Prometheus script failures can be a common issue, and dealing with metrics cardinality can be frustrating.

I once found a bug in GKE related to certificate signing requests that led to Prometheus crashing due to high cardinality metrics. It's an ongoing challenge.

**Host:** That does sound challenging! Could you elaborate on the target allocator?

**Jacob:** The target allocator is a component of the Kubernetes operator in OpenTelemetry that dynamically shards targets among a pool of scrapers. This approach helps distribute the load effectively and is especially useful when you have many nodes in your cluster.

**Host:** That’s a great overview. So, the target allocator helps with efficient data collection, especially in large clusters. 

**Jacob:** Yes! It allows us to scrape metrics from various nodes efficiently without overloading a single collector.

**Host:** Awesome! Now, I know there are different deployment modes for the OpenTelemetry operator. Which mode do you find works best for your needs?

**Jacob:** We use all deployment modes depending on the requirements. Sidecars are the least popular since they can be expensive. For most cases, we prefer using deployments for stateless collectors, while daemon sets are great for node-level metrics scraping.

**Host:** It sounds like you have a solid strategy in place for your collector setup. 

**Jacob:** We run various collectors to handle different telemetry types. We’re constantly experimenting and improving our setup based on internal needs.

**Host:** That’s great to hear. It sounds like keeping telemetry up to date is crucial.

**Jacob:** Absolutely! Staying current with library updates and performance improvements is vital for effective migrations and overall system health.

**Host:** As we wrap up, do you have any parting thoughts?

**Jacob:** I think it’s important to keep learning and sharing knowledge within the community. If anyone has questions after this session, feel free to reach out. Staying updated with the latest changes and improvements in OpenTelemetry is beneficial for everyone.

**Host:** Thank you so much for your insights today, Jacob! This has been a fantastic discussion, and I’m sure everyone here has learned a lot.

**Jacob:** Thank you for having me! I’m glad to share my experiences.

**Host:** Thanks again, everyone! We appreciate you joining us today.

[Music]

## Raw YouTube Transcript

[Music] thank you welcome everyone to Hotel q a thanks for joining um today we are super lucky to have Jacob aaronoff from lightstep from service now join us um it's it's a cool treat because um like I tapped Jacob because he was telling me that he had submitted a cfp to one of the I think it was cubecon right Jacob [Music] yeah it was a cfp on like um migrating to Hotel um from like our or like our organization migrating at hotel right if I if I understand correctly which I thought that's pretty freaking cool story to tell so um so I asked them to to join and and have this q a and um and hear the story because I think it's it's I think it makes for a compelling narrative like when an observability vendor talks about using otel themselves on their own products so here we are um so welcome Jacob um do you want to do like a quick little intro sure um yeah hi my name is Jacob arnoff I'm a staff engineer at lightstep um on the Telemetry pipeline team um I've been to lead stuff for almost two years uh and the first like year of that Journey was solely focused on like Hotel migrations um making doing them internally and making them easier for customers um sort of that whole process um so yeah I could get into this anyway that is interesting how do you think do we want to do this very q a style or should I just go right into the story talk about um we can we'll we'll let it evil organically about um cool yeah I like that why don't we start like um I mean I think you're you're rearing to go so maybe why don't you describe like set the scene for us so um when I joined lightstep uh we were still on open tracing for uh tracing um and then a mix of open census and some hand rolled stats key stuff or metrics so this meant that we had to run a proxy on every single pod that we ran in kubernetes and a proxy since it's a sidecar on every pod which means that every single time you run it one of your applications you have to run another little application that's going to read from statsd and then forward those metrics off um and you know for us as we're building out a metric solution that destination was light step so I came in and this was sort of at the beginning of like metrics Alpha I think um and I was like hey it would be great for us to we know that um open uh we know that like hotels hotel for metrics is going to be a huge effort for us in the next like year or two we wanted to reach stability um the hotel team had been we have like an Hotel team internal light step they've been working on it a lot and really wanted some um immediate feedback on how to improve it so I took on that migration for us we also had the theory that doing so would save us you know good chunk of money because we would no longer need to run these relatively expensive stats key um uh stats the sidecars so I planned it initially um to be sort of as safe as possible I'd done some migrations like this in the past um and there are a few different ways that you can do migrations like this you can do the all-in-one go uh which for us would have been possible we're in a mono rebuild um but it's much more dangerous because you you worry about you know am I going to push a bug that's going to take everything down right um obviously this is application data that it's data about your applications which we use for alerting we used to understand you know how our workloads are functioning um in all of our environments and so it's important that we don't take that down because that would be disastrous for us but obviously for you know an end user is going to be the same story they don't they want the comfort that if they migrate to Hotel they're not going to lose all of their alerting capabilities immediately they're not like you want a safe and easy migration so that's the only one with our initial approach for doing a sort of like feature flag based um just like part of that can part of the configuration that you run in kubernetes um it would disable this sidecar enable some code that would then swap to otel for metrics and then forward it off you know where it's supposed to go um so that was sort of the path there um Midway through this journey of doing these migrations I had tested it all out and staging looks pretty good I tested the container in our meta environment so we use to monitor our public environment um I noticed some pretty large performance issues um in those 20 2021 and I had reached out to the hotel team and we had worked together to sort of alleviate some of those concerns one of the ones that we found that was a big blocker was we heavily use attributes on metrics right now and it was incredibly tedious to go in and figure out why you know figure out which metrics are using all these attributes and getting rid of them so um well I had a theory that like really this one code path was the problem where we're doing the conversion from our internal tagging implementation through Hotel tags which had a lot of other logic with it that was pretty expensive to do on pretty much every call so I was like you know what this there's no better time than now to begin another migration for her open tracing to otel basically while we wait on the hotel folks on the metric side to push out um more performant code more performant implementations for us we can also test out this theory that if we migrate the hotel entirely we're actually going to see even more performance benefits so at that point I said okay I'm going to put a pause in the metrics work while we wait for hotel and I'm going to begin on this tracing migration nutrition migration however um I decided to try a different approach which is the All or Nothing approach basically um the open tracing to open Telemetry path was a bit more known there were a few really like small docs and examples and they are backwards compatible like you're able to use them in conjunction with each other so one thing admitting Hotel one thing emitting um open tracing is not the end of the world so you can mix those as long as you have the propagators set up correctly so first step so propagators Second Step um make sure that all of our plugins worked which at the time they weren't open sourced uh now they are open source so people can just use them um and I just did it all in one swoop maybe I had to revert like three times from our staging environment nothing really major um and then there was one bug that I missed where um we were previously doing a lot of in-app sampling because we had a really noisy function call um so I had to implement the custom sampler which is actually like 10 times easier with otel than it was with open tracing I was able to get rid of a good like 1000 lines of code or so and some really dangerous hacks so that was a really good thing um and yeah so then that went out very happy um also stopping at any time if this if we want to get back to uh like more q a stuff but I I have so many questions from this already but do you mind do you mind taking like a quick pause because I I'm like sure um okay so a couple comments so one thing that that came to my mind as you're saying this I'm like this is actually really freaking cool story because like I think for there we we tend to see like two different types of organizations right we see the ones where there's like zero code instrumented like they are this is like their first foray into instrumenting their code and then we see the the organizations that I think have either that have like dabbled in open tracing so I think it's a really cool story because this is like a real life migration story where you can actually provide advice on this is what you can do if you find yourself in this situation which is really cool um so I wanna I wanna call that out because I I think um I think that's a really important thing um especially if you're like starting to get into open Telemetry um the other thing that I wanted to ask you about because uh you said that it's um that it's a monorepo um so in that case did you find it and especially since you did the All or Nothing approach um did you find having a mono repo more challenging than if you'd been dealing with microservices instead um yeah well so we we do use microservices it's just that we have like um repo of microservices sorry my my bad about you guys um in that case yeah then um how do you um how do you know like because I mean yes you're going for like a big bang approach but you got to start somewhere so then like where where do you uh where do you start so I started with um and it was the same with how I started the metrics migration um I started with really small services that um my team owned that were really low traffic but enough for it to be constant um so the reason that you have to that you want to pick a service like that is if it's too low traffic if you're only getting like one request every minute one request every like 10 minutes um you have to worry about sample rates um you might not have a lot of data to compare against really that's like the big um thing that you need to have is some data to compare against I wrote a script early on for the metrics migration that um just queried uh different build tags um that are on all of our metrics so you would say you know query all of the metrics for service X um grouped by release tag and if you see that the standard deviation for the newer build tag is like you know greater than one right so if it's one or more standard deviations away from the previous release then there's probably something going wrong in your instrumentation Library right if you assume that your metrics are relatively stable then if they're not it's important to know um the other thing I had to check for was that all of the attributes were still present before and after migration which is another thing that matters sometimes they weren't because something might be something that stats T just adds automatically that we don't really care about and so those were acceptable I just like hand waved instead those are fine we don't care um for tracing is sort of the same deal where I picked a service that had um both internal only traces traces that stayed within a single service and then traces that um span multiple services with different types of instrumentation so coming from like Envoy to hotel to open tracing and what you want to see is that the trace before has the same structure as the trace Factor so I made another script that checked that those structures were relatively similar and that all of them had the same attributes as well right right um because tracing attributes again I was doing an attribute migration that was really the point of doing the tracing one so what matters that all the attributes um stayed the same right yeah it's interesting too because like because you're you're starting from the point where you were migrating from an existing thing like you have that frame of reference which I guess is kind of a double-edged sword right because on the one hand it's like you know you pretty much know that you've instrumented the things hopefully maybe maybe you'll discover as you go along that there's like more stuff to instrument but at least like you have a baseline to start from but then I guess on the other hand if something's missing you're like oh damn why is that missing right yeah and those you know why is this missing stories or the really complicated ones because of course sometimes it's easy to just like you know oh I forgot to add this thing in this place and that's usually pretty simple but sometimes it's like oh there's an upstream library that doesn't admit the thing or hotel and now I need to like again this is like early stuff most of these have all been upgraded and are fine now um but there is an example with like our grpc um actually this is like an interesting one um I had done a grpc I migrated on a grpc util package which I think is now in like token trip um the like Hotel goken trip um and there was an issue with propagation I was trying to understand you know what's what's going wrong here and when I looked at the code and it just tells you how early in this story I was doing this migration um where there was supposed to be a propagator there was just a to do oh no um so and the city was from someone on it was from Alex book which I'll call him out um and so I I sent it to Alex and I was like hey this is a funny to do because I just you know took down um an entire Services traces and staging um so I spent some time to like fix that to do so it wasn't that difficult it was just that they were waiting on another thing I mean that's how it goes you're waiting on someone else which you know another person it's just endless cycles of that type of thing yeah um but then I got it working so that was like one of the main uh blockers for us nice nice and was able to Upstream it as well it wasn't just a fixed for ourselves it was a fix for the community and so that that was yeah there were a few things like that um a lot of the metrics work um actually resulted in big performance boosts for um Hotel metrics like Hotel go metrics and um it also has given the specs folks like some ideas about how descriptive the API should be or various features so things like um uh views and the use of views is something that we did heavily in that early migration because we were worried about can you just yeah definitely just tell folks what you mean by that yeah so um of metrics View is something that's run inside of the your metrics provider in otel your media provider in hotel so what it's doing is it's saying you can configure it to do kind of a lot it could just be um drop this attribute whenever you see it if some if you're a centralized SRE and you don't want anybody to instrument code with any user ID attributes because that's a super high card analogy thing it's going to explode your metrics cost right so you could just make a view that gets added to your instrumentation that says just don't let this attribute from being recorded just deny it um so that's like it's probably most common use case um there are other ones though more advanced use cases for dynamically changing things like the temporality or the aggregation of your Matrix so temporality being cumulative or Delta for like a counter um I you know am I recording um zero one three I'm trying to two crazy zero one three or am I recording one and two right right um and then your uh aggregation is going to be about how do you um I these things are so I always struggle to explain all of them and I'm trying to like come back to uh what I had done in that moment um talk about temporality oh aggregation is like um oh man being on the spot is so much harder because it's like I want to look it up but feel free to look it up that's totally cool well aggregation is like how you um like send off these metrics basically we had an aggregation that instead of doing um well for histograms this is like most useful when you record a histogram there are a few different types of histograms um datadog's histograms stats of these histogram is not a true histogram because what they're recording is um uh like aggregation samples so they give you a min max sum count average um and so I actually don't even think they give you some I looked at this last night they don't give you some they give you like min max count to average and like P95 or something um and so the problem with that is in distributed computing if you had multiple applications that um are reporting of P95 the there's no way that you could get a true P95 from that observation with that um aggregation um the reason for that is that in order to get P95 like you you can't if you have five p95s oh five P95 observations there's not an aggregation to say give me the overall P95 from that right you need to have something about the original data to actually recalculate it you could get the average of the p95s but that's not a great um metric that's not really like it doesn't really tell you much it's not really accurate um and if you're going to alert on something if you're going to page someone at night you should be paging on accurate measurements yeah um so initially though we did have a few people who relied on this min max some counter instrument so we used um Hotel views in the metrics SDK to configure custom aggregation for our histograms to do emit what some would call a distribution um what Oto calls an exponential histogram um or the min max and the Min Maxim count so we were dual emitting this works because they're different metric names that we were emitting so there was no overlap between them so what we did was we migrated after we did the metrics migration we were able to then go back and say any dashboard any alert anything that had was using a min max some count metric um just change it to be a distribution instead and because we had enough data in the past like you know a few weeks months of running Hotel metrics in our public environment that was possible to do um so that that was like one of the key features that because we had it it was 10 times easier um and we were able to do it from the application uh we didn't have to introduce any other components which is pretty neat right right cool um um another question that I had for you um so like when you were doing this migration it was traces and metrics were in existence logs I believe would not have been like the specification would not have been ready possibly not even in the works no it was still really early for that so um but I guess in lieu of logs there's span events that you could use so it's not something like that was leveraged as well definitely um we've heard a long time have you used span events analogs um or a lot of things um internally um I'm a big fan of them I am not a huge fan of vlogging I find it to be really cumbersome and really expensive um and iops for like tracing and Trace logs whenever possible I find it like easier for myself to reason about there are other people who are like logging first and that's great but um that's just not who I am um I like I like logging for local development and tracing for distributed elements that makes sense um but we use this heavily that was one of the first things that I checked worked um It's actually an interesting bug where we had some custom code or open tracing that allowed us to serialize like Json blobs in the span events um and that stopped working because we didn't emit them in the same way uh it's like a little hazy but um I had to like rewrite a processor to make that work and then update some Downstream code like in lightstep as platform to Facebook cool so now how about um keeping that in mind like now that logs are more mature is there are there any plans to do any conversions like and and please correct me if I'm wrong but my understanding too is that like with the log specification like maturing more and more that um like span events are going to be replaced by logs in some form like it's going to be the log specification for span events have you heard anything around that like no this is a bit outside of um where my recent Focus has been so I'm not positive um I think right now the way that we do I think the thing that we would change is how we collect those logs potentially um right now we use um uh how do we do this right now it changed recently I don't want to say something incorrects but um we previously did it by just using like Google's logging agent where they basically are running like fluent bit on every um node in a in the gke cluster yeah and then they send it off to like gcp and they just like tail it there um I think this changed though and I'm not sure what we do now okay cool cool um speaking of kcp of many questions on gke specifically like so um do you um because I I believe there's like a feature now in like newer versions of of kubernetes where there's like some um I think there's there's like some Telemetry collection do you know if that's been enabled in any of the uh in any of the Clusters yeah so I think that kubernetes now has the ability to emit like the hotel traces natively yeah yeah yeah um I'm not sure if we're collecting those yet um I don't know what version that's more of a like a question for this sres I I don't um kubernetes I came out like I think even last year starting whatever like last fall kind of thing yeah that's a really good question um that I want to look into because I want to see if uh really what I would like to do is see if we can collect the if the traces that we get from those are enough to use the spend metrics processor to generate like better kubernetes metrics from those traces [Music] um I'm very focused on like infrastructure metrics like kubernetes infrastructure metrics um and I find them to be very painful in their current form um and it would be really cool right now um I prefer to use the Prometheus apis for them currently um it's just a bit more ubiquitous in the like observability Community to use Prometheus to do that [Music] um just because like that's that's what kubernetes like natively emits right right go ahead oh uh no go ahead I'll let you complete the thought maybe it answers my questions um and so that's what we do right now and I use the target allocator which is you know nutshell component that I work on um to distribute those targets which is you know pretty efficient way of getting all that data um we also use like demon sets as well that we run in our clusters to get that data um in addition to that so that works pretty effectively the thing that's frustrating is just Prometheus um Prometheus script failures can be um a super common problem and it gets really annoying when you have to like worry about metrics cardinality um as well because it can explode yeah I actually found a bug in gke maybe six to eight months ago six seven months ago but they've since fixed where they weren't deleting uh they weren't reconciling certificate signing requests in their kubernetes cluster which meant that for Kube State metrics which reports on cluster State um it was oming because there were so many um certificate signing requests left from these abandoned nodes and so something on the magnitude of like six hundred thousand for like a single metric which is huge and so then Prometheus the Prometheus that I was running fell over because of that um and that's like a thing that happens constantly in this Prometheus realm which is just like someone admits a high cardinality metric Prometheus goes to scrape it and then it just like crashes oh wow um um which isn't Fun yeah that does not sound fun um I want to just take a step back because you you mentioned the target allocator I was wondering if you could expand a little bit on that because I know like we we actually had one of our previous q a folks also mentioned the target allocator um that was the first time I had heard of it so I think it'd be like super helpful to just get a little overview sure yeah so apparent allocator is component um part of the kubernetes operator um in hotel that does something that Prometheus can't do um which is uh dynamically Shard targets amongst a pool of scrapers so previous has some experimental functionality for sharding but you still have a problem for um querying because Prometheus is a um database not just a scraper yeah um if you Shard your targets you don't necessarily you have to do some amount of coordination within those Prometheus instances which gets expensive it's like a very experimental feature um or you could scale Prometheus with something like Thanos or cortex which is um grafana's Prometheus scaling solution I think right yeah which works but you just then have to run like six more components that you then need to Monitor and then if those go down how do you met a monitor it's all these other problems right right um an hotel we just basically like uh tack on this Prometheus receiver to get all this data but um because we want to be more efficient than Prometheus because we don't need to store the data we tell we have this component the target allocator which goes to do the service discovery from Prometheus so it says give me all of the targets that I need to scrape and then the target allocator says um with those targets distribute them evenly amongst the set of collectors that's running oh okay um so that that's the main thing um that it is doing it does some more stuff around um job Discovery now or if you're using Prometheus service monitors which is part of the Prometheus operator which is a very popular way of like running Prometheus in your cluster um it's what a lot of like vendors use as well so if you're on GAE or openshift I think both of those natively used service monitors and pod monitors oh so the target allocator can also pull those service monitors and pod monitors and uh update the collectors scrape configs to do that oh cool it's awesome um and so one related to the Prometheus thread um are you running like Prometheus itself or are you just scraping the Prometheus metrics and pumping them through to the collector then exactly right just um no Prometheus instances just um The Collector running Prometheus receiver and then sending them off to light step oh living the dream that that was always like that was always my dream that's awesome that's nice um do you use like because I I remember like lights up has like a Prometheus um like a Prometheus operator that uh helps facilitate that so we used to have this thing yeah we used to have this thing called the Prometheus sidecar which you might run yeah you would run it as part of your Prometheus installation which would then sit on the um on the same pod as your Prometheus instance and read the uh write ahead log that Prometheus has for like um persistence and uh batching and all these other things yeah and so we would read the right ahead log and then forward those metrics but if your Prometheus is very noisy as many customers have very noisy Prometheus statistics yeah um it's not really efficient it can get really noisy and it not that uh what's the word it's not the best thing to run the collector is like the fact the best way to run okay so and and it sounds like this thing still requires um to have Prometheus installed and yeah you would still need to be running a whole computer system oh okay I I thought it was I was under the impression it was like a replacement for Prometheus and that it was um maybe I'm thinking of something else his there was a thing that I knew it was like a replacement for for like needing Prometheus um and it was like vendor neutral so it wasn't like oh you have to use lightstep to use this thing um I think I might just be a hotel operator collector Target allocator Trio oh oh okay okay but maybe there's another thing out there oh unless unless maybe that got integrated into like the target allocator as part of anyway it is a mystery yeah okay cool cool oh that's awesome um so then okay since we're we're talking collectors now um I for me like the two million dollar question the one that I'm always curious about is collector setup so what what is the collector setup that you have that y'all have chosen has like what works for for now yeah it's hard to say because we run a lot of different types of collectors yeah headlight stuff we run like metrics things tracing things internal ones external ones there are a lot of different collectors that are running at all times you have like a separate one that just collects metrics and one that just collects traces and um right now we don't um it's all varying flux okay right now we're changing this a lot um to run like experiments and stuff um basically like the the best way for us to be able to make features for customers and end users is by running them ourselves and then using them internally making sure that they work and then sending them for the open source realm and so that's what we're trying to do even more of like we're kind of reaching a point where uh we dog food everything which gets really um confusing because you have to like yeah I can imagine yeah we're running like in a single path there could be like I think two different two collectors in two environments that could be running two different images in two different versions like it's it gets really meta and very confusing to talk about yeah yeah I can imagine and then you know if you're sending from collector eight across an environment to collector B um collector B also emits Telemetry about itself which is then collected by collector C yeah and so it like it's just chains like you basically ensure that that you have to like make sure that the collectors actually working yeah you have to be sure that everything along this path yeah you just have to know which thing has the data right well you shouldn't have to we do that for you but like right um yeah and we make like dashboards to help with that but um that's like the problem when it's like we're debugging this stuff is when there's a problem you have to think about like where's the problem actually is it in how we collect the data is it and how we emit the data is it in um you know the source of the how the data was generated it's it's you know one of like a bunch of things yeah yeah um now like you need to work on the hotel operator so and and I I've been reading up on the operator recently and there's like I think four different deployment modes right there's sidecar deployment demon set and um  what's the other one um yeah nice yeah um yeah um so my question is um which mode um or is it like all of the above depending on the thing that you need to do yeah it's all the above depending on what you need to do um and you're neat like your general needs and uh like how you like to run applications for like reliability and stuff yeah so um sidecar is the one that we use the least and is probably used the least if I were to make just like a bet um sidecars are really useful like across the industry you would think yeah across the industry I'd be willing to bet that those are the least popular that's the least popular method sidecares are just expensive and if you're not using them if you don't really need them then you shouldn't use them and you'll really only need them like something that's run as a sidecar it's like istio which yeah yeah makes a lot of sense to run as a sidecar because it's doing like traffic proxy like hooks into your um you know container Network to change how that all does its thing yeah and you get a performance hit uh if you sidecar your collectors right for all your services you just get like a cost it would just cost you a lot more um and you also wouldn't be able to do as much with like um if you're making like kubernetes API calls for attribute enrichment that's like the thing that would get exponentially more expensive if you're running it as a sidecar right but as like a staple set of like you know five pods that's not that expensive but if you have a sidecar on like 10 000 pods then that's 10 000 API calls made to the kubernetes API right yeah yeah that's exclusive what um what would be the advantage of running um your collector as a full set versus a deployment like where I guess what's what's the state that you would want to persist yes this is um like I don't know the right word is an unknown stateful sets aren't only used for their ability to mount volumes um there are a few other things that are inherent to how staple sets run that are really valuable in distributed computing this is an important thing to know for not just like how The Collector runs as an application but how like your applications can run right um stateful sets have um consistent IDs so if you have a staple set with 10 replicas they're all going to be um the staple set name Dash um um counter number so it goes from like zero to n so okay that's a really valuable thing when you want consistent IDs right as opposed to like with with deployments like when you like your your pods are all like random random crap right yeah so the Pod IDs are done where you it's um deployment name Dash replica set ID Dash pod ID right and so with staple sets because we have this consistent IDs we can actually do some extra work with uh for the Target allocator which is why we require that and so the other thing that stateful sets guarantee is what's called a um In-Place deployment which is what demon sets do as well where you have you take the replica you take the Pod down before you create a new one um right so the reason that this is important is that in the deployment you normally do a one up one down right um or what's called a rolling deployment a rolling update and so if we were to do this for um uh if we were to do this for the uh with the target allocator um we would probably get much more unreliable scrapes because you would and someone actually just asked this question in the operator Channel I mean I'm going to give them this exact response um when a new replica comes up um you have to redistribute all the targets because your hash ring that you place these on is changed so if you're doing a rolling deployment if you're doing one up one down that's a really expensive operation um because then you have to recalculate all these hashes that you assign um so if you were to do a one down one up you would still have to redo this whole thing because um you would lose a pod which means it's taken out of the ring redistribute you would gain a new ID and then you'd have to redistribute again right um whereas stateful sets because it's a consistent ID range you don't have to do that at all and so this means that when we do a one down one up it keeps the same targets each time right right so it's like it's almost like a placeholder for it like you don't have to recalculate the ring basically because yeah it's just sort of like a little um uh what's it called uh yeah yeah yeah yeah I can't think of the word cool that's really neat I didn't know that yeah it was funny because I was reading about like pause being deployed a stateful sets I'm like straight or sorry not uh collectors I'm like I straight up do not understand what the use case would be but this makes a lot of sense so that's uh that's really cool and so it's not as useful um or this is really only useful for like a tracing use case I would say or sorry metrics use case where you're like doing complete test scores and we would probably run it as a deployment for anything else um because a deployment gives you everything that you need pretty much um because the collectors are stateless they don't need to hold on to anything yeah um deployments are much more lean as a result yeah yeah um they can just run and roll out and everybody's happy and that's how we run most of our collectors deployment and then at what point would a demon set be useful yeah so demon sets are really good for um things like her node scraping um which we do a lot of so um this allows you to scrape like the kublet that's run on every node um it allows you to scrape the uh node exporter that's also run on every node which is another Prometheus demon set that most people run right right yes the demon sets guarantee that you've got odds running on every node right exactly every node that matches its selector right right right yeah um and so that's really useful for like scaling out so if you have like a cluster of like 800 plus nodes um it's more reliable to run like um a bunch of little collectors that get those tiny metrics rather than a few bigger staple set pods right because your blast radius is much lower so if like one pod goes down you lose like just a tiny bit of data but remember like with all this cardinality stuff that's a lot of memory so um if you're doing like a staple set scraping all these nodes that's a lot of targets that's a lot of memory it can go down much more easily and you lose more data luckily the collector isn't like Prometheus where we don't care about that state so if a collector goes down it comes back up super fast so usually the blip is low but it does mean that the blip is more flappy right where like it could go up and down pretty quickly if you if you're past the point of saturation that's why it's good to have like a HPA horizontal Auto scaler right right on that stuff but still demon said is a bit more reliable right and it sounds like it would be useful again like from a metric standpoint yeah yeah tracing um you could do it for tracing um and just send it on like a node port but tracing workloads again because it's all push based they are much easier to scale on um and you can distribute targets you can load balance like there are all these other benefits that we get from push-based workloads pull based is like the reason that Prometheus is so ubiquitous in my opinion is just because it makes local development really easy um where you just can scrape your local endpoint and that's what most back-end development is anyway so you could like hit end point a and then hit your metric set point and then hit end point a again metric standpoint you can just like check that it's like a very easy like developer Loop no it also means that you don't have to reach out side of the network so if you're a really strict like proxy requirements to send data local Dev is much easier for that right just why like hotel now has like a really good Prometheus exporter so you could do both right right right if if you have that hankering for for running Prometheus yeah um and and then um I'm assuming there's a centralized Gateway somewhere or um or on flights um this is part of the like collector chain that I was talking about um again we're running a lot of experiments cool um I can like half talk about it okay um I can be vague um a big effort within hotel right now is um around Arrow which you might have been hearing about some um the there's been some work done by lightstep and F5 to improve the um processing speed and egress and Ingress costs of otel data um by using Apache Arrow which is a project for columnar based data representations um and so we're just like doing some group of Concepts or like proof of implementation work to to um see what the actual performance of this stuff looks like right um and also like you know check that everything works as expected yeah yeah as well which it is but that's you always have to check yes absolutely well I'd say the main takeaway from from this whole story on collectors is like it sounds like it's always going to be an evolving game which is not a terrible thing to do no it's important that you keep your telemetry up to date yeah um I think that like Library authors and maintainers are like constantly working on new performance features and new ease of use like quality of life stuff as well yeah yeah um especially with an Hotel like we talk about quality of life a lot yeah um yeah and so that is definitely a focus and that's why it's important to keep up to date it makes migrations easier as well trying to migrate from like an ancient version of something to a latest version probably missing a lot of breaking changes potentially yeah and you have to be careful of that and and on that vein then how do you ensure that everyone's keeping up to date with the latest versions of hotel across the org um I think like stuff like depend about is pretty good um we use it internally or not internally we use it um in a hotel we're keeping up to date with Hotel stuff so I find it to be really helpful it's very frustrating sometimes because it's like hotel packages all update and like lockstep pretty much that means you have to update update fair amount of packages at once but it does do it for you which is pretty nice that's nice that's nice um but you should be doing this not just for hotel but like any dependency yeah right like CVS happen in the industry constantly and if you're not staying up to date with um vulnerability fixes then you're opening yourself up to security attacks which you don't want so um yeah yeah do something about it is is my recommendation that's fair that's fair um I know we've got like four minutes left um do you want to give us any like parting thoughts as we as we wrap up um I'm interested to hear if there are any questions from the group that we've we've missed in our discussion here any takers with burning questions Now's the Time if not I'm going to call on uh Rhys but I this was great um I feel like uh I was like rapidly trying to take notes um I probably will have more as I try to like go back and tidy up some of my notes um but yeah honestly I was just trying to like keep up with people with so much information I feel like because I've been reading up on the on on the operator like the last week and so like more questions than answers and I feel like I have some answers now um I I've definitely learned a lot um I hope folks on this call have learned a lot as well I think this is like really great information I think it gives it gives folks like an idea of like what's involved in a migration things to consider like when you're setting up your your collectors um things to avoid doing keeping up with those latest versions of Hotel y'all always a good thing yeah that's the big one is that like new fixes um really often yeah yeah like new versions every two weeks for The Collector I'd say it's like a pretty frequent Cadence there are some Prometheus libraries that like don't update like ever like Thanos hasn't released since March right which is absurd to me because there's like a bug in compatibility between Prometheus and Thanos right now oh really how cheap yeah so you can use them together if you're like coding so not fun yeah damn all right last chance for for burning questions y'all Erica said thanks for the info on your time Jacob thank you Erica for hopping on um yeah this was really awesome thank you so much um because like for real this is a dope dope topic so yeah we'll you know reach out to your uh friendly neighborhood uh cfp approvers no thank you so much I feel like there was so much more we could have chatted about as well so we might um yeah yeah maybe I can um if I get accepted for the talk then I can give a sneak peek at least I can get some feedback on it actually you know what like if you're if you're interested because we have hotel in practice which is kind of like giving like a little talk so if you're interested in doing that even like before you find out whether or not you get accepted like we are happy to have you yeah it sounds great cool I'm in cool cool we'll uh we'll figure out the behind the scenes on like when to schedule you and um sounds good yay cool all right and Daniel said thanks Jacob as well and um yeah we're at the top of the hour thanks for joining us thank you all thank you yeah thank you everyone bye [Music]

