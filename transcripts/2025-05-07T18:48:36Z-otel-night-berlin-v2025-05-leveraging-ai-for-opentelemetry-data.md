# OTel Night Berlin – v2025.05 – Leveraging AI for OpenTelemetry Data

Published on 2025-05-07T18:48:36Z

## Description

Lariel Fernandes, Principal AI Engineer at Dash0, will share their expertise on leveraging the power of Artificial Intelligence to ...

URL: https://www.youtube.com/watch?v=8JlFuGTDCXQ

## Summary

In this video, Lariel, a principal AI engineer at Dash Zero, shares insights into applying AI to open telemetry data, focusing on a case study of their log AI. Lariel discusses the challenges of making sense of unstructured logs, which often appear messy and unhelpful compared to the ideal structured formats presented in demo datasets. She outlines the development of their log AI system, which aims to automate log parsing and abstraction without needing manual regular expressions. The solution combines various techniques, including resource clustering and prompt engineering with language models, to effectively identify and parse log patterns. Lariel emphasizes the importance of evaluation, achieving a 98% success rate in log parsing accuracy, and highlights the potential applications of AI in observability, including query generation and trace analysis. The session concludes with key takeaways about the cost of AI in production, the use of traditional machine learning in conjunction with language models, and the significance of context in AI prompts.

# AI Application in Open Telemetry Data

## Introduction

Hello everyone, I'm Lariel. Today, I will share my experience applying AI to open telemetry data. 

### About Me

I’m from Brazil and moved to Berlin a couple of years ago. Currently, I work as a Principal AI Engineer at Dash Zero. Before that, I played various roles in data and AI, from data engineering to data science and ML Ops. My CV is quite diverse.

For those who might not have heard of Dash Zero, we are an open telemetry native SaaS solution for observability. We work with metrics, traces, and logs, providing dashboarding and alerting services. Our platform is built on open standards like Prometheus and Prometheus Remote Storage, and we're developing innovative AI capabilities to help users derive more value from their data.

## Main Subject: Case Study on Log AI

The main focus of today’s talk is a case study on our Log AI and how to make sense of unstructured logs.

### The Challenge with Logs

Many of you might have experienced disappointment when using open telemetry demo datasets. The logs initially seem perfect—shipped via OTLP and formatted as JSON with ample useful attributes. However, in reality, most logs look far less organized. For instance, when working with Kubernetes, logs collected from different nodes might appear as raw strings with basic attributes, making them challenging to interpret. 

There is often hidden information in logs, such as module names, durations, timestamps, and severity levels. The motivation behind our Log AI is to parse and abstract logs at scale for any application without needing human intervention or custom regular expressions.

### Our Approach to Log Parsing

We aim to separate prefixes from messages in logs. For example, consider a log entry with a prefix containing a timestamp, host, and severity level. The messages may follow different patterns, such as “ad lookup failed” or “displayed this ad to this user.” Our goal is to identify structured fields for querying without manually writing regular expressions for every log format.

Initially, we explored available options for log parsing, including the Drain model, which is a well-known NLP model. However, it has significant limitations, such as requiring pre-processed logs and custom rules for each log source. As a result, it didn’t meet our needs.

### Alternative Solutions

Another option is to use large language models (LLMs) to parse logs. While LLMs can accurately parse logs on the first try, they are costly and inefficient when processing billions of logs daily from thousands of applications.

We also considered deep learning models trained on labeled data to predict log patterns. However, the lack of a comprehensive labeled dataset across our diverse customer base made this approach impractical. Additionally, benchmarks for these models often indicate they only perform well on familiar log patterns, limiting their generalizability.

### Our Solution

To address these challenges, we combined different approaches to leverage their strengths while mitigating limitations. 

1. **Defining Instance Granularity**: We defined the right level of granularity for predicting log formats. Instead of treating each Kubernetes pod as a separate resource, which would create inefficiencies, we implemented resource clustering rules to group similar resources, allowing us to cache and reuse predicted log formats.

2. **Processing Pipeline**: Our pipeline begins by clustering logs by resource attributes. We then employ heuristics and prompt engineering to identify log prefixes and messages, extracting important fields like log severity. After parsing, we use traditional word frequency models to cluster log messages by structure before leveraging LLMs to refine variable names and types.

3. **Evaluation**: Before deploying our Log AI in production, we created an evaluation dataset combining community and proprietary logs to test our model’s performance across various use cases. We aimed for graceful degradation, meaning that while the model doesn’t need to work perfectly all the time, it should provide accurate information when it does function.

### Results

Our evaluation revealed a 98% success rate in log parsing, with 100% accuracy in log severity among successfully parsed cases. The patterns we extracted covered an average of 85% of each application’s logs, successfully abstracting most of the available information.

### User Interface and Visualization

When integrated into our UI, users can visualize log severities over time, with clear distinctions between known and unknown severities. We also provide dedicated visualizations for log patterns, allowing users to see the structure of log messages and relevant variables. For example, a pattern might show a product ID lookup failure, which can trigger alerts based on frequency.

## Other Experiments

We’re also experimenting with AI agents for writing and debugging Prometheus query language expressions and diagnosing failures in alerts. By providing agents with semantic conventions and resource attributes, we improve the accuracy of generated queries. Additionally, our Trace AI groups duplicate spans by attribute similarity and generates meaningful trace names and descriptions.

## Key Takeaways

1. **Cost of AI in Production**: Applying AI at scale can be costly. However, leveraging attributes in the open telemetry schema can help cluster similar resources, enabling caching and reuse of AI predictions.

2. **Combining Techniques**: While LLMs are powerful, they can be expensive. It’s essential to balance traditional machine learning techniques with LLMs for optimal results.

3. **Importance of Evaluation**: Evaluation is crucial, even for simpler LLM applications. By modeling and measuring expected behaviors, we can quantify limitations before production deployment.

4. **Contextualization for LLMs**: Providing context using resource attributes and semantic conventions enhances the performance of LLMs, leading to better results.

## Conclusion

Thank you for your attention. If you’d like to connect with us, please follow us on LinkedIn, visit our blog, or check out our Code Red podcast on Spotify and Apple. We also have open positions for an AI engineer and a product manager with a background in observability.

### Q&A Session

**Question:** How do you handle logs with no severity level?  
**Answer:** We only measure accuracy where there is a ground truth. If a log lacks a clear severity, we do not assign one, avoiding false positives.

**Question:** Can customers customize semantic conventions?  
**Answer:** While we use a standardized workflow for all customers, we can improve contextualization by incorporating customer-specific semantic conventions.

Feel free to ask more questions! Thank you!

## Raw YouTube Transcript

All right. So, hello everyone. I'm Lariel. Uh, today I'm going to share a little bit of my experience in applying AI to open telemetry data. So, short intro about myself. I'm from Brazil, moved to Berlin a couple years ago. Nowadays, I work as principal AI engineer at Dash Zero with these guys here. Uh, before that, I had been around uh playing different roles in data and AI. So ranging from data engineering to data scientist, ML ops. So my CV kind of looks like a mess nowadays. Um also for those of you who have not heard of D-Zero, just quick introduction, we are an open telemetry native SAS solution for observability. So we work with matrix, traces, logs, we do dashboarding, alerting, we're built on open standards like Prometheus and Persis. And obviously we are building lots of cool AI capabilities to help our users get more value from the data that they send to us. And that brings me to the subject of today's talk. Actually the main subject which is a case study on our log AI. So how to make sense of unstructured logs. Okay. Uh many of you probably had a similar disappointment at some point. Okay, you plug in the open telemetry demo data set and then the logs look beautiful. The logs are shipped via OTLP. They have plenty of useful attributes. They are uh formatted as JSON. So they're very easy to work with, right? But in reality, most people's logs look like this. So you have some something running on on Kubernetes and you have those uh file listeners that get uh the logs from every node and in the end uh the result is something like this. So your log has some uh raw string body and some attributes saying what file it was collected from and that not very useful but you know that there is information there like if you look into the log there is something that looks like a module name something that looks like a duration a time stamp a severity at debug level and how can we harness that information so that's the motivation behind behind uh the log AI that's the thing we we developed. So we want to do log parsing and log abstraction at scale for any logs of any application without a human in the loop. So without having to write custom regular expressions. For example, if I have an application writing logs like this, I want to parse them by separating this prefix from the message. So I have in the prefix time stamp host a severity text and we can see that the messages follow different patterns right I have some ad lookup failed displayed this ad to this user. So there are basically two different patterns of messages log messages that my application is writing and there are some structured fields that I would like to be able to query right there is some add ID in the middle there is some user ID um and we want to abstract that in the form of a pattern without writing a regular expression manually and we want to do that for every log of every application that's the challenge and the motivation behind our log AI. Uh so first we started looking into the available options to do that. Now the most famous one is the drain model that some of you might have uh seen already. So drain comes from this family of natural language processing uh models that work with uh word frequency. So they basically uh compare the parts of the text that change frequently with the parts that are often uh the same. So they can identify where the variables are and what the patterns should be. Uh but those algorithms have plenty of limitations. First one, they assume that the log is already prepared. So you have already separated that prefix from the the message that can change from one pattern to another. And for that you need custom regular expressions written by hand for each log source. That is already a deal breakaker. And secondly, those algorithms are very dependent on pre-processing. So there is this paper called pre-processing is all you need where they describe this manual iterative process of coming up with tokenization rules, um variable identification rules, uh text cleaning rules for each of your applications. And unless you do that, you cannot expect these models to produce any decent results for your custom logs. Believe me or not, the screenshot here on the left side is real code from the repository where they have the benchmarks for this models. So you can see that for each log data set that they are benchmarking on they have hardcoded rules otherwise the model doesn't work. And yeah that's a real deal breaker for us. We cannot just apply this drain model to every log and expect it to to work right. Um, okay. So, we started looking into other alternatives. What's very popular nowadays is just, yeah, throwing things at a large language model. So, if you ask GPT to parse a log, it's going to get everything right on the first try. You get the fields really nice. The problem is with hundreds of customers uh instrumenting thousands of applications and sending billions of logs every day, the using an LLM for everything would be very uh cost ineffective and slow. So what do we do? Yet another option is like this paper suggests using a deep learning model which is trained to predict the pattern on every log. But in order to train a model in the first place, you need to have lots of labeled data. So logs where you already know what the correct pattern should be. So you can teach the model how to infer that. And we cannot possibly have such a data set for all of our customers. Yet another problem with these uh models is that although they have a nice benchmarks, those can be interpreted as a form of data contamination because uh the the logs used to benchmark the model. They were produced from the same applications with the same templates as the logs that were used to train the model in the first place. which indicates that the model can actually only identify patterns that it is already familiar with or parse logs of applications that it is already familiar with. It doesn't generalize for any logs of any applications. Uh so yeah, this was the status quo. Plenty of challenges, different options, each one with their limitations. So how we solved this in the end was no magic at all. We actually just combined different approaches trying to balance out the limitations of some with the advantages of others. So first thing we did was figure out the right uh level of granularity for predicting log formats and patterns. So um I don't know if anyone here has already worked with predictive AI uh in in production but you usually have the concept of an instance. For example, if you are doing customer churn prediction, then every customer is an instance. If you're doing product sales forecast, then every product is an instance. It's quite straightforward. When it comes to open telemetry data, it's quite challenging because uh there are different levels of granularity. The schema is very nested. Um so what is an instance? We started with the concept of the open telemetry resource. So we tried to predict the log format and patterns for every resource but uh it wasn't efficient at all because if you think for example of a Kubernetes deployment with autoscale it is creating new pods and killing old pods all the time and every new pod has different resource attributes. So it's treated as a new resource. If we would predict log formats and patterns for every resource, our model would be working all the time, which would not be efficient at all. So instead of doing this, we came up with resource clustering rules that we apply to resource attributes of different resource types. This allows us to scope and cache and reuse the predicted log formats and log patterns between different resources that belong in the same group. And then to make this work in production, we also came up with a recommended configuration for the open telemetry collector that guarantees that the logs that we get from our customers always contain all the resource attributes that are relevant for our resource clustering rules. So all right, we have now a concept of an instance that we want to make predictions for. Next step okay how do we predict the the patterns. So uh we came up with this pipeline where we start with all the logs and then we first clusterize by resource attributes like as I said before. Then we focus on parsing. So we use a combination of eristics and prompt engineering to identify uh what is the the prefix, what is the message, what parts of the prefix are worth extracting like the log severity and we come up with log part log formats for each uh resource cluster. And after the logs are parsed, we apply those traditional uh word frequency models like drain in order to clusterize the log messages by structure. So every log message that has kind of the same structure goes into the same cluster. And then looking at each log cluster, we take the output of the drain model and inject it into the prompt uh for a language model in order to identify the final variables and also give names and types to the variables. At this stage, we observe that including semantic convention information and resource attributes in the prompt helps a lot and getting variables names and types that make sense for each application. Then in the end we just apply some eristics to optimize the patterns that we get so they match as best as possible the respective log cluster. Um then at ingestion time it's actually really straightforward in our uh open telemetry collector the processor um is going to match the logs against patterns from the respective cluster and attach the attributes with the variables that that it finds. So yeah, this uh overview of our solution. But before I show you what the results look like in our beautiful UI, I'm just going to have a quick word about evaluation. So this is really important before we applied this to every log of every customer all the time in in production. We needed to build some confidence to know that it would produce the expected results. So we came up with an evaluation data set that combines um logs from community data sets with logs from our own proprietary data. Uh the focus was not to have too many logs or quantity but rather to have diversity of logs so we could uh really stress the the model and confirm that it was working as expected for many different edge cases. And our main goal with this evaluation was to confirm that our approach had this uh kind of builtin graceful degradation. What does that mean? Means that the model doesn't need to work all the times. It doesn't need to work for all the every of log from the billions of logs that we are ingesting. But whenever it works, it needs to produce correct information. when it doesn't work then it cannot produce any unwanted side effects. So graceful degradation and that's basically what we observed in the evaluation. So we had a 98 uh success like 98% success rate at the log parsing step with 100% uh log severity accuracy between among the cases that are successfully parsed. So the model almost always understands the log format and when it does the log severity that it extracts is always correct. When it does not then it doesn't extract anything and the the log stays as it was before. Besides that the patterns that we extracted for each uh applications logs they cover an average of 85% of that application's logs. That means we successfully abstract almost all the information that is available to be abstracted. So yeah. Oh, sorry. Ask a question right at the end. Uh let's save the questions for for the end if you don't mind. Don't forget it. Uh okay. I'm just quickly going to show what the results look like in our UI. So this is a histogram of uh log severities over time. The gray uh bars are logs with unknown severity. And then uh as soon as we plug in the new uh log processor that applies the log formats and patterns, then we start having colorful logs in the histogram indicating the information and warning and error logs. Also, we observe a couple examples here of uh errors and warnings that would have passed by unnoticed if it wasn't for the the correct log parsing. What else? Um we come up with this dedicated uh visualization for the patterns. So this is data from the open telemetry demo and then uh we have uh different patterns where the variable is in the middle. things that say for example um targeted ad request received for add category and then add category is is a variable or a method name called with user ID and then the method name and the user ID are variables. Um and if we open one log record like this one with product ID name then uh we know which pattern it matches and we have the product ID and product name as dedicated variables. This is structured data that can be referenced in queries in filters in group by expressions also in Prometheus query language. So uh example here let's say if I want to create an alert based on log frequency using the patterns. So I have some logs that say product ID lookup failed and then it has some ID in the end. This one matches a pattern and the product ID is a variable. So I can create a log frequency alert with the filter saying that the pattern should be that one and grouping by the product ID field that comes from the the semistructure text. Right? So this is the Prometheus query language expression to uh alert on that log frequency and then the result is when uh I'm having too many lookup errors for that product I get a nice u failed check with the the custom message up here that says lookup failures increasing for product with the ID of of the product that so basically alerting based on semistructure information that was abstracted from from logs. This happens without any metrics, without needing an extra metric, without c needing uh to write any regular expression, without depending on on traces and anything else, just based on on the locks. U yeah, this is really cool. So yeah, this was the case study on the log AI that we have been uh developing. And I'm just going to quickly comment on a couple other experiments that we have been running. So uh we have been experimenting with AI agents to write and debug Prometheus query language expressions and also to diagnose and find the root cause of failed checks and alerts. uh the agent that we work with, it has access to a model context protocol, MCP server that allows it to browse through every metric and log and trace kind of like the same way that a human would do in our UI. Um in this context we also observe that if we feed the agent with information about semantic conventions and um resource attributes then it gets way better at writing correct queries in the first try. So it doesn't take so many iterations to figure out the correct metric names, the relevant attributes and so on because it builds on top of the semantic conventions. And yet another use case is our trace AI where we group uh duplicated spans in our trace explorer uh by their attribute similarity and we also give all we generate uh trace names and trace descriptions to clusters of similar traces. In this context, we also observe that giving uh semantic convention information to the the prompt increases the quality of the the names and the descriptions that we get. And also when clusterizing the traces by uh their semantica embeddings, if we include the resource attributes in the embedding, we get way more meaningful embeddings. Therefore, a better clustering of similar traces. So yeah, those were just a couple other examples of use cases that we have been working on. So before we go on to the questions, I just wanted to leave you with four key takeaways. So first yeah, applying AI at production at scale can be expensive. It can get expensive very quickly. But remember that in the open telemetry schema we have plenty of attributes that you can use to clusterize similar resources. This way you can scope cache and reuse AI predictions between different resources. Second LLMs are cool but they are expensive. Um, so we always try to see what we can do with traditional machine learning techniques or how we can combine them with LLMs to get the best of both worlds. Third, evaluation is very important. Even if you're building a simple um LLM application and you're not really training or fine-tuning any model, you can always try to model the expected behavior of your application and measure how often and how close it gets to the expected results. So you kind of quantify the limitations and the risks before shipping it to production. And lastly, uh, context is everything when it comes to LLMs. So, always benefit from resource attributes from the rich open telemetry schema and the semantic conventions so you contextualize your prompts and get better results from language models. Yeah, that was it. Uh, if you want to connect with us, uh, please follow- on linkading. We also have a a blog on our website. We're always posting some cool stuff. We have the Code Red uh podcast on Spotify and Apple. And we have a couple open positions in our careers website too, including one for another AI engineer and for a product manager with a background in observability. That was it. Uh thanks for your attention. I'm sorry. Do you remember your question? Yes. I think it was about uh the lock levels. Um you had 98% with lock levels, but there are some locks that do not have a lock level. How is that working out? We only measure the accuracy where there is a ground truth. So for a a log that uh really doesn't make sense to assign a level to it doesn't have the level even in an unstructured way or in the form of a status code or anything then um we do not assign it a log level. Actually in that case if um we would assign a log level let's say error warning or anything to a log that doesn't contain any textual information that references that it would count as a false positive and we have a separate metric for measuring how often that occurs. It's the specificity metric. So it's how often we avoid uh false positive and that one is maximized as well. uh but it wasn't on the slide. Okay. I think there's even a unspecified level for open country right there is uh yes but it is the default or at least when we ingest if that field is not set we set it to unspecified and then after the AI runs if we don't identify a an explicit level it remains at unidentified. Any other questions you there? I didn't get this specifically about the semantic conventions and resource. Did you use them to insert resource attributes and semantic conventions into logs that don't have it or what was the No. Uh the the thing is whenever we get uh any signal that has u attributes or like a span name, a metric name that matches the latest semantic conventions, then we have access to the documentation of that metric or attribute and we include that uh documentation in the prompt. So the model uh has context of what that metric means in which uh system it belongs. So it is able to for example give uh an appropriate name to to a variable or give a a better description to a trace. So that's how we work with semantic conventions for prompt contextualization. That's very interesting. I was thinking it would be also nice to um have like these kind of resource attributes and convention are sometimes not like the values are sometimes not correct especially if you have your own semantic conventions on top of hotel like business relevant semantic conventions and um if you could use AI to rectify basically um the discrepancies in what the users are putting in because they usually they put in a lot of stuff and a lot of different stuff and even if you have like written it down please use these values they make up their own values and their own writing systems And then instead of going there and telling them every time no please lower case not uppercase or something along the lines if we could use like kind of AI to understand the not sure if AI is really necessary. Sometimes regular expressions do the trick but sometimes they don't. So yeah that that can actually be a challenge. Every case where I I mentioned that we employed so many conventions it was the official one ones like from the the open source repositories theoretically and theoretically yes yeah sorry I have a follow-up question on this so a question I asked myself uh I don't know how dero works so as an as an obsability platform but my assumption is that the log AI feature is the same for every customer because you used something like yeah now someone is turning around if you're not so uh you found an approach how to leverage different kind of AI technologies to have lock AI enabled and it's the same for every customer so do I train this follow-up question on your so with my own semantics as my my own and everyone has something like a slightly different uh implemented AI approach so that's a that's a good question so uh the approach right now is the same for everyone But we work on a multi-tenant way. So uh even if two customers have let's say Oracle databases, we don't mix like the the log formats and patterns of one customer with the other. So all of your data will be processed with formats and uh patterns that we inferred from your data only. Uh it is but it's always pre-trained. Yes. uh we are not uh customizing by uh customer although not right now maybe in the future. Yeah. Yeah. Thank you very much. There are plans on open sourcing any of that. I suppose that I mean I love the log AI part but I suppose the the Prometheus part the Prometheus would also to have that kind of feature and I think I heard other companies doing the same right? Uh I think I heard some other companies doing something very similar. I see a lot of potential collaboration like opportunities there. Yeah, there could be uh yeah uh regarding any kind of open source you know need to talk to our CTO uh but uh what we are planning to do is to release an MCP uh so anyone can connect their agents to -0 data if they want to over there. So here to me so you apply a model to every single log. Do you use an LM? No. Okay. So how do we apply? Okay. uh we do it in a batch where the logs are already clustered by by resource and then they are clustered by structure and then within each log cluster we identify the pattern with the LLM and then we store that pattern then during ingestion time we don't invoke the LLM at all we just apply the patterns that we have already cached so by pattern You mean regular expression? Yes. No. Um, you can try that. Uh, maybe the newer ones will succeed for some cases, but LLMs are usually bad at writing regular expressions on their first try. We actually uh use them to infer parts of the the log, extract variables, uh get some insights on the log structure and then we apply aristics to these results to compose the regular expression ourselves. Yeah, I hope that rest on the data at rest. Yes, it was also part of the processor which was done on the fly. On the fly during ingestion, we apply log formats and patterns that have already been cached. So there is like this uh pattern identification phase and there is the production time which is exploiting the patterns that have already been cached. Question. So um at what frequency do you refresh the cache? That's uh in our documentation uh it should be every two hours. Why should it's in beta phase? I'm working on it but yeah um it's meant to be every every two hours. There is obviously a quot per uh like rate limits per per customer. So if you're sending logs uh of different formats and different pattern all the time like generating random formats just to make us spend money it's not not going to work. So refreshes for when patterns change but actually it's only needed when it changes. Sorry you said like the cash refresh the cache every two hours. Yes. And so that cache is for you're caching the lock patterns or Yes. So when the lock pattern changes the cache needs to be refreshed too. Yes. So then yeah and it would be good if you could um do this like on demand. It is on demand every two hours. Okay. Seriously that's that wasn't a joke. I mean we if they change we realize that they changed at latest to ours after and then update them. Uh I mean one very important aspect of dash series is that we do not sell AI capabilities as separate modules. We give them kind of for free. You only pay for the data that you send and every capability all the cool stuff in the UI you get out of box. So whenever we implement something like this, we have to establish rate limits uh and uh yeah just optimize it as best as we can to make it uh cost effective. But if I as a customer know that my won't change let's say or I know when it will change is there a possibility to change that to tell zero that now there's a new pattern coming uh right now uh Right now the way to do that would be you you message our customer success people and then they they would ping me but we wouldn't need to do anything if they change and at latest two hours later they would I mean the pipeline would realize exactly. So what it means is um within the first two hours I might not recognize all of the new fuse or all of the new patterns that that are flowing through the pipeline. Yeah. After two hours it's going to be recognized and the bar. So that graph they've shown with the gray bars that could we could see a difference there for the first two hours and then it goes back to the same level it exactly. That's very cool. Wow. Go on. So one question is uh so you mentioned that you have some kind of prompt engineering in the pipeline detecting the patterns. Uh do customers somehow can can they involve are they involved in this process? Can they change something like for example adding their own smart? Uh well we use the same um workflow for every customer but oh something is falling over there. [Laughter] monitoring. Yeah, we use the same workflow for every customer, but uh the more information we have for each customer, the better we could uh contextualize the prompt and maybe having custom semantic conventions for each customer's domain would be something to to help improve to the prompt. Yeah, that's something we could we could build. And a second question, how do you monitor like end to end for example how many how much money you spent on how much tokens and so on well we use -0 to monitor- zero right uh so um we also use the community uh libraries for instrumenting LLM applications the ones from the uh open Python open telemetry contrib uh so from those we get LLM traces and a cost uh for every LLM request that we make the costs are also u like contextualized with the price per token of each uh vendor and model variant and we have a nice um LLM trace view in our UI where we can uh debug LM LLM interactions like with what tools were called and so on. Folks, if anyone has other questions, we can chat uh later. Uh yeah, here around in the kitchen. And uh yeah, I'm going to give into the the next and thanks for having me.

