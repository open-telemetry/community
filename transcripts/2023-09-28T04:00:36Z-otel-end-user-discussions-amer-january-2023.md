# OTel End User Discussions (AMER) January 2023

Published on 2023-09-28T04:00:36Z

## Description

Want to know what we talk about in our monthy OTel End User Discussions? Check out this session from January 2023 to get a ...

URL: https://www.youtube.com/watch?v=qn4x0DgG5SI

## Summary

In this YouTube video, Reese, who works at New Relic and is involved with OpenTelemetry, hosts a discussion with participants including Derek, Dan, and others, focusing on the role of "enablers" in technology teams, particularly in adopting OpenTelemetry across various programming languages. They explore challenges faced when dealing with languages outside one's expertise, the importance of creating champions within teams, and strategies for fostering understanding and adoption of observability practices. The conversation shifts to technical issues such as clock drift and pipeline configuration for data processing, discussing best practices for scaling collectors and handling telemetry data efficiently. Participants share insights and experiences on creating effective observability cultures within their organizations, fostering knowledge sharing, and the importance of community support. The session concludes with an informal moment featuring a kitten named Taco.

# YouTube Transcript Cleanup

[Music]

**Reese**: Of course, I was looking forward to the sessions. My name is Reese. My day job is with New Relic, and I also do a lot of work with the OpenTelemetry user working group, including hosting these sessions. 

Alright, let's see... Okay, let's start with this one. 

**Question**: Does your role in Compass involve being an enabler? How do you deal with helping in languages you're not an expert in? 

Usually, I run these sessions based on whoever put the topic or question in. If you can unmute and maybe give us a little bit more detail... It looks like Derek has a question on how to define "enabler."

**Derek**: Yeah, this one's mine. Happy to expand on this. Basically, the role I have in my company is kind of twofold. The first is to maintain OpenTelemetry collector deployments and be responsible for the data pipeline—receiving data and sending that to various backends. But it's also what I call the enabler, which involves being a champion for OpenTelemetry, driving adoption within the company, troubleshooting, and helping teams troubleshoot issues they have in their respective services.

In my company, for instance, we're probably about 70% .NET, 10% Java, 10% Go, and 5% Python. We're primarily focused in one area, so we build examples in .NET. I personally am more of a .NET developer, and my team is as well. 

A great aspect of OpenTelemetry, in my opinion, is that the concepts are extremely similar across languages. However, I don't really know how to write Java. It's not too far away from .NET, but sometimes it's troubling for me to provide specific feedback or look at a PR and really understand everything happening. 

I'm curious if teams have this problem at the companies they work in. Do you try to create champions in those languages you're not familiar with, or communities of practice around that? If anyone's experiencing this, how are you solving for it?

**Response**: Sure! As a former enabler, I can share that my strategy was to lead the way by showing how to enable observability within my domain of expertise, which at the time was Go. As I was demonstrating the benefits of using OpenTelemetry—or at that time, OpenTracing—people would come in and ask me questions. Many of these individuals worked in different teams and languages. They were interested in starting out but weren't familiar with the concepts.

My role was to pair with those people who were experts in whatever languages they were trying to become enablers in. I worked alongside them until they felt comfortable with the concepts of the observability platform I was using. This strategy worked out pretty well, and we managed to deploy things to three or four different teams within the organization.

**Derek**: Okay, cool! That's kind of what I mean by champions—basically leveling up individuals or groups within those smaller subsets and then letting them run with it. Do you find that when you did this, those people sought information on their own? Did they jump into the OpenTelemetry Slack channels and ask questions, or were they routing things through you?

**Response**: It was a mixed bag. Some people felt more comfortable talking to the person they knew, so to a certain extent, I became a router for questions. However, I did see a couple of people who were really into talking to the community directly. They would just go into the Slack channels or reach out to people directly on GitHub. Over time, as I directed people to specific issues in GitHub or specific spec changes, they tended to get more comfortable with the idea that they could go there themselves.

**Derek**: That's great! One last follow-up, if you don't mind. How long would you say it took for those individuals you worked with to get up to speed? Was it a matter of weeks, or was it a longer period? How easy was it for them to sort of understand the domain when it wasn't their primary focus?

**Response**: Sorry to give you a mixed answer, but it really depended on the individuals. Some people who were keen on learning new ways of doing things got up to speed a lot faster. Others who were more accustomed to doing things a certain way took a bit longer. To be honest, some people I tried to get distributed tracing adopted just never really understood the benefits. It really is a mixed bag, depending on where people are in your organization and how resistant they are to change.

**Derek**: Thank you! I appreciate that because I feel like we struggle with that sometimes. They don't really understand the purpose or how it may have value outside of their individual team, especially when we start connecting services or something. Just trying to gain insights into improving the culture, so I appreciate the response. 

**Response**: It's really hard to take someone from their day-to-day job, where most people are already strained with their tasks, and ask them to dive into what appears to be an unnecessary aspect of their jobs—like implementing OpenTelemetry. It’s not until they have those "aha" moments of understanding how much better their lives will be in the future that they can really wrap their heads around the benefits.

One thing I've tried to help people understand is how to get started as quickly as possible. One of the main areas of OpenTelemetry that I'm really excited about is all the auto-instrumentation work. I think this allows people to gain some benefit without a tremendous amount of upfront investment. 

Even if it only gets them 60 or 70% of the way, it tends to give people enough visibility into what their systems are doing through auto-instrumentation that they can then get excited about investing more time in this work.

**Derek**: Makes sense. 

**Response**: I sympathize with you. As someone who codes less than what you described in your role, it feels weird to lead the way as an enabler for these things in your work without being the person who can just say, "Alright, let me show you some examples."

In my previous organization, when we were trying to bring OpenTelemetry into the development teams, we faced interesting challenges. The teams recognized the importance of OpenTelemetry but were constantly fighting fires. They didn't have time to instrument their code, which was ironic because instrumenting their code would probably help them fight those fires more effectively. 

While my team was the enabler, we defined best practices and taught teams how to instrument their code, they kept trying to use my team as, "Oh, you know this stuff, so why don't you instrument our code for us?" We had to push back really hard on that because we didn't know their code. They had to instrument their code because they knew it best.

**Derek**: I feel like we're all in the same boat. It's super interesting because I've experienced what you guys have said. What Gerald mentioned is something I'm also experiencing. I sometimes try to help a few open-source projects add instrumentation to their code base to have good references to point other people to. 

The problem is that some projects want good instrumentation, but they aren’t ready for it. I’m experiencing that within my company and other companies as well. We can try to get a network to instrument things for them, but if they aren't ready for it, it's just not going to work.

**Response**: I agree. First, we need to sell the idea that if teams recognize the benefit earlier, it helps in all these avenues. It helps them understand the purpose and, therefore, maybe want to contribute more to solving the problem. Maybe we should hone in on showing examples where doing this solves some problems, especially if they're experiencing similar issues.

**Derek**: Is my microphone better now? Am I closer? 

**Response**: Yes, that's much better!

**Derek**: Good! I had a conversation with someone who mentioned something similar. They have an observability group that serves as the reference group for observability. They can monitor what the whole company is doing in terms of observability. There was one team using a specific programming language that found a specific metric helped them recover very quickly from an outage. 

So, the observability group shared that information with the entire company, saying, "Teams using this programming language should include this metric, or else..." I see the enabler's role as spreading information—getting information from one team and sharing it with the whole company.

**Response**: Would it be beneficial to have documentation around real-world cases like the one Gerald mentioned? Do you think that might help if you could share similar stories with your team?

**Derek**: I think we try to do that already. I think it's a combination of everything mentioned here. I don't want to say what we're doing isn't working, but progress is slower than I would prefer. Having a collection of good news stories could help. 

For example, "Hey, I work at company X, and we were able to solve this problem and reduce mean time to resolution." If people are looking to get into this space, I think those examples are great. As someone in the observability group, I read blogs, but I don't think everyone at my company does that.

**Response**: Maybe we should consider having periodic forums where organizations looking to adopt OpenTelemetry practices have a chance to interact with folks in the community for Q&A. Just to ease their concerns. Would that feel like too much overlap with what this group already does?

**Derek**: My first thought is that it does have some overlap with this group. I think I would need to think about that a bit more. 

One thing Reese mentioned is that she's recording this meeting, which is good. People can go back and gain insights from past discussions. Some topics brought up here aren't necessarily easy to find answers for, as they are more gray. 

**Response**: It’s a valid concern, and I’d like to think about this more as well. 

I just realized I haven’t been time-boxing this, so I apologize. It sounds like we might be good on this topic. Dan, was there anything else you wanted to add before we move to the next one? 

**Dan**: No, I'm good.

**Response**: Can I hop in real quick just to share one last thought? 

We're trying to drive change across various departments through the idea of a service catalog and scorecard for services. Various subject matter experts can define a set of standards in a particular domain that should apply to services and provide a rubric for those standards. 

This way, service owners can consult that rubric to grade their own services according to the criteria. If a service is graded a C, they can refer to the documentation provided by subject matter experts to improve their service to an A. 

This is something new at our organization, and we’re trying this to drive change across different areas, like SRE practices, Kubernetes, code quality, test coverage, observability, etc. 

**Derek**: I see. 

**Response**: Since the next two topics have equal votes, we’ll go with the one about dealing with clock/time drift.

**Derek**: These are actually all mine for today. I noticed our backend doesn't accept data that's in the future—specifically, data points that are more than 10 minutes ahead. 

I'm curious if people are experiencing this problem. The naive side of me wants to say they should just fix their servers, but if you are experiencing this, do you notify services or implement something in the collector to understand that it's happening?

**Response**: I can talk a bit about how Jaeger deals with that. Clock skew is going to happen, and you just have to account for it. It's important to realize that there's no way to synchronize clocks, especially in a microservices architecture. 

One way to deal with that is for Jaeger to assume that you don't have asynchronous processing, which is very synchronous. The parent span or the first span is adjusted to be as long as the longest span that it has. However, this created confusion for users who had asynchronous processing, so we ended up doing a flag to disable this behavior.

The owner of the system generating the telemetry data is in a better position to understand clock drift than any general-purpose tool like the collector. 

**Derek**: Thank you! I think there are two use cases here—like a low mild clock skew and the really bad ones that are 10 minutes off. I was thinking of creating a processor that detects incoming data points that are over a certain threshold—like five minutes. 

Creating a data point that captures the service name could help identify that this data is out of sync. I don't know if this would be useful, but right now, my backend is rejecting data, so more visibility into the issue would be a first step. Does that sound like a weird use case?

**Response**: It's an interesting use case. You could have a processor that detects things that are out in the future. The hard part is that you wouldn't catch all of your clock drifts; you would only catch the very obvious ones. 

If your backend is rejecting data, it would be good to know that. If you had a 10-minute threshold and data was only three minutes delayed, you wouldn't detect that, but it could still cause issues with data interpretation. It might be better to shore up data loss and have service owners interpret the drift since they are the experts.

**Derek**: Are we talking specifically about metrics or any telemetry data type?

**Response**: I would need to double-check, but I think it was for spans. In my opinion, it would be useful for any type.

**Derek**: Okay, cool! 

**Response**: For metrics, some metric stores will block new metrics from the past and future, so you would have a warning in your logs. For traces, it might not be too hard to detect clock drift by looking at the spans of a trace. Logs are tricky because you generate so many logs on a single server, and they generally have the same timestamps.

**Derek**: I see. 

**Response**: The simplest thing that would work is to flag log timestamps that are still in the future. However, this would only catch clock drift in the future, not in the past.

**Derek**: Makes sense. Thanks for the input, everyone! 

**Response**: Alright, moving on to best practices for bifurcating data in a pipeline. For example, having a filter processor with positive and negative condition clarity around fan-in.

This is also mine, and I don't know how to phrase it simply. I have data flowing into two pipelines. For one pipeline, I want histograms to go to backend A, and for the second pipeline, I want non-histograms to go to backend B. Is configuring two pipelines like that the right thing to do? 

**Response**: There's a new type of component for the collector coming up called connectors. The collector currently has four types of components—extensions and pipeline-specific components like receivers, processors, and exporters. The connectors can act as both receivers and exporters. 

You can have one pipeline that receives OTLP, does some processing, and ends with an exporter. The connector then connects with another pipeline as a receiver. In your case, you could have one receiver and two connectors—one for histogram-specific and one for non-histogram-specific data. Each connector can filter what should be passed through.

**Derek**: That sounds like a good feature! I'll look into it. 

**Response**: It is! We have a routing processor that routes data points based on their characteristics to specific exporters. However, it makes another network connection, which can be inefficient. 

**Derek**: I'm currently using a filter processor that works. I just felt like having two receivers receiving the same data was doubling up on memory. 

**Response**: Profiling might be helpful. Check out the connectors, as they could help reduce memory usage.

**Derek**: Sounds good! 

**Response**: Does anyone else have anything to add or ask? 

**Derek**: I do! I asked this a couple of sessions ago. I’m trying to understand how to scale my collector deployment correctly. I opened a GitHub issue that was large in scope, so I haven’t had responses. 

I’m curious about criteria for when to horizontally scale my pods versus modifying the configuration of an individual collector. Does that make sense?

**Response**: If you only have stateless components in your collector, you can just increase the number of replicas. Your workload will dictate how you scale—not just for demand but also for high availability. Is it better to have one per namespace? Should you have one per tenant?

If you have one collector that all traffic goes through, it can be a critical failure point. 

**Derek**: I understand. 

**Response**: There’s no easy answer, but you could chart your pipeline based on the type of processing. You might want to split by metrics, logs, and traces because they have different workloads. 

**Derek**: Okay, cool! 

**Response**: Regarding the "num consumers" setting, it relates to the signing queue. The number of consumers is the number of workers picking from the queue. It’s complicated because the optimal value depends on your workload and the backend you're sending data to. 

**Derek**: So there's no one-size-fits-all answer? 

**Response**: Exactly. You’ll need to do specific testing around your expected data volume to determine the optimal settings. 

**Derek**: Awesome! Thank you for the insight. 

**Response**: Thank you all! I want to be respectful of everyone’s time. Dan, thank you for your questions. Alex, Gerald, it was great to have you on. If anyone has questions about these sessions or anything else, feel free to reach out to the end-user working group on Slack. 

Oh, and before I go, I just want to show you my kitten, Taco. 

**Derek**: Hi Taco! 

**Response**: Thank you all so much!

[Music]

## Raw YouTube Transcript

[Music] of course I was looking for near to the sessions um my name is Reese I my day job is uh with New Relic and I also do a lot of work with the open Telemetry and user working group including hosting these sessions all righty let's see okay let's start with this one does your role in compass being an enabler how do you deal with helping in languages you're not an expert in um usually I run these is whoever put the topic or question in if you can unmute and um maybe give us a little bit more detail it looks like Derek has a question on how do you define enabler yeah this one's mine uh happy to expand on this um so like basically the role I have in my company is kind of twofold the first is like you know maintain uh open Telemetry collector deployments and and kind of be responsible for the data pipeline uh receiving data and sending that to the various backends but it's also like what I call the enabler which is like uh being a champion for open Telemetry driving adoption within the company uh you know troubleshooting helping teams troubleshoot issues that they have in their respective Services uh you know things like that um so like in my company for instance we're probably like 70. Net 10 Java 10 go five percent python you know like we're primarily focused in one area so like we build examples in.net we I personally am more of like a.net developer my team is as well uh you know a great thing about open Telemetry my opinion is that like the concepts are extremely similar across languages but like you know I don't really know how to write Java you know it's not too far away from.net but for instance um so sometimes it's like troubling for me to provide specific feedback or like look at a PR and like really understand the scope of everything that's happening and I'm curious like if teams have this problem at the companies that they're working in do you try to like you know I don't know create champions in those languages you're not familiar in or you know communities of practice around that um just you know if anyone's experiencing this and maybe how you're you're you're solving for it I hope that makes sense there totally yeah as a as a former enabler um I I can tell you that my my strategy was um really to kind of first lead the way by showing how to enable observability or you know within my within my domain of expertise which at the time was uh go and you know as as I was demonstrating the the benefits of using open Telemetry or I guess at the time it was open tracing that if that gives you an idea of how long ago this was um but you know the one of the benefits as I was demoing the benefits people would come in ask me questions and a lot of the times these people work in different teams and different languages um and you know they would be interested in in starting out but they wouldn't be familiar with the concepts of open tracing open Telemetry and so you know my my role as I thought was to kind of pair with those people who were experts in whatever languages that they were trying to be um becoming enabler in and you know really work alongside with them until they felt comfortable with the concepts of the observability platform I was using so I think that's that's kind of my strategy it worked out pretty well I think we managed to deploy things to you know like three or four different teams within the organization and it worked pretty well yeah okay cool that's kind of I guess what I mean by like Champions so like basically leveling up you know maybe individuals or groups within those like smaller subsets and then letting them kind of run with it um do you find when you did that that those people you know as like I wasn't familiar with open tracing but like you know as the spec evolves and as new features get added do they do they come back to you do they seek information under their own do they jump in the hotel slack channels and ask questions or uh you know were they routing things through you and your experience um it's kind of a mixed bag I think I think some people feel more comfortable with just talking to the person they know and so you know you to a certain extent I became a bit of a bit of a router for questions um but you know I did see a couple of people who were really into um you know talking to the community directly and so they they just went into the slide channels or reached out to people directly in GitHub or whatever so it it's a bit of a mixed bag um you know I I think at some point as you direct people to specific issues in GitHub or specific spec changes people tend to get more comfortable with the idea that they can go there themselves but um yeah cool and then maybe one more follow-up if you don't mind like how long would you say like it took for those individuals you worked with uh to like I don't know get up to speed whatever that means in your uh opinion you know is that like a matter of weeks was that like a longer period you know how how would you engage like how easy it was for them to sort of like understand the domain when it is maybe not their like primary focus yeah um again sorry to give you maybe uh uh a mixed answer but again it was it depended on individuals I think some people who are really keen on trying to learn new ways of doing things we're really excited about the prospect of of trying out um you know something new and then you know those people kind of got up to speed a lot faster the people that were you know um you know maybe more uh used to doing things a certain way took a little bit longer um and then you know if I'm honest I think some people you know I was trying to get distributed tracing adopted some people just never really understood uh what the benefits were and I think that you know it really it really is a mixed bag so it really depends on where people are in your organizations and how resistant to change they are I think yeah cool no thank you I appreciate that because I I feel like we struggle with sometimes exactly that like they don't really understand the purpose or they don't understand how it may be has value outside of their individual team you know like when we start connecting services or something um so just trying to you know come up with any insight I can into like improving like the culture and that sort of thing so appreciate the response yeah I think you know you know I think the it's really hard to get to take someone from their day-to-day job which you know most people are already kind of strained on doing whatever it is that they're doing for the business um and asking them to do more by you know heading off into this completely what a kind of appears unnecessary aspect of their jobs right to implement open Telemetry um and it's not really until people have those aha moments of you know understanding how much better their lives will be in the future um to that they can really wrap their head around the benefits I think one of the things that I've really helped try to help people understand is you know how do you get started as quickly as possible and I think that's one of the one of the main areas of open Telemetry that I'm really really excited about is all of the auto instrumentation uh work um because I think that allows people to get some amount of benefit without a tremendous amount of investment up front and a lot of the time I find even you know even if it only gets you 60 or 70 of the way they're um it tends to give people enough of visibility into what their systems are doing through Auto instrumentation that they can then like get excited about the prospect of investing more time in doing this work yeah makes sense awesome Dan I don't really have an answer all I can say is I sympathize with you uh as someone who like codes even probably a lot less than you know what you described in your role it feels weird to kind of want to lead the way as an enabler for these things in your work without necessarily being the person who's able to kind of just all right let me let me do a couple examples for you let me kind of show you from start to finish how this looks so I don't know I'm trying to figure it out too probably further behind you actually so in my uh previous organization um like when we were trying to bring open Telemetry into uh the development teams we Face some interesting challenges where the teams recognize the importance of open Telemetry but they were constantly fighting fires so they didn't have time to instrument their code which was kind of ironic because instrumenting their code would probably help them fight those fires more effectively and so like while my my team was uh their job was to be that enabler so we you know Define the best practices and taught teams how to instrument their code they kept trying to use my team as like oh well you guys know this stuff so why don't you instrument our code for us which we had to push back really hard on because like we don't know your code so you have to instrument your code because you know it best right you know what the things are that you need to look for yeah I feel like everything like we're all talking about like we all have like somehow are in the same boat and I've just it's like super interesting because I feel like I've experienced all all of what you guys have said yeah what what a general mentioned is something that I'm experiencing as well um and even if it was wider than just one company it goes even throughout the community so sometimes I um I try to help a few open source projects to instrument uh to add these orientation to their code base because I wanted to use to have good references of what other people can can do you know so I could then point other people to that code base and say you know this is a very well instrumented application you can use it as reference for instrumenting your own application um the problem with that was some projects were they wanted to have a good instrumentation but they didn't they didn't actually um they weren't ready for it right and this is something that I'm experiencing within the company uh within you know other companies as well is that we we can try to get a network and instrument things for them but if they are not ready to have their code instrumented it's just not gonna work I don't know yep um I I could by the way you sound a little distant I I was able to hear you um but just that way um I I think that's true like I feel like first somehow we need to like sell the like if teams maybe recognize the benefit earlier like it helps in all these avenues like it helps with them understanding the purpose and therefore like maybe wanting to contribute more how to solve the problem so maybe I don't know I'm just thinking out loud here and maybe like pushing to show more like here are some examples where doing this solves some problems like if you're experiencing similar problems and perhaps like this would be a really good idea for you um maybe like you know I don't know honing in on that kind of an attack might might be beneficial yeah so is my microphone better now am I closer yeah that's much better yeah okay that's good uh yeah so um I I just came with a performing conversation with another person that um that person was mentioning something exactly like that you know um and one thing that that they're doing and I found very interesting is um they they have like an observability group and that group is uh the reference group in terms of observability and they they can watch what other people what the whole company is doing in terms of observability and what what he mentioned was um there was one team using this specific programming language and they found that one specific metric helped them recover very fast from an outage so what the observability group did then was to spread that information to the whole company and saying so teams were using that programming language you should you should you now have to include this metric here um otherwise you know because that metric helped that team to recruit were very fast from an outage so um I see the role of the neighbor here like the observation neighbor as also to spread information to get information from one title and break this item and bring to the whole company what should the whole Community for that matter um deal would it be you know as far as something like the community could do with this um would maybe some documentation around like real world cases like the one gerasi mentioned um do you think that might be helpful if you could share like similar stories with your team it it might I mean we we try to do that already I I think it's like a combination of everything that's mentioned here maybe like I don't want to say like what we're doing isn't working I think it is working it's just like progress is slower than I maybe would prefer you know um I mean yeah I think like you know having a collection of like good news stories that you know or whatever you want to call it like uh you know like hey I work at company X and we were able to solve this problem and you know reduce mean time resolution blah blah blah blah like yeah I do think like that that you know you know if people are like looking to get into the space I think those are great and I think like having those examples of like specifics um you know like as my role here as like in the observability group for my company like I go and I read blogs and stuff but I don't think like everybody at my company would do that like maybe they are reading blogs specific to like the.net runtime or something you know I don't know um so yeah I think it would it would help um I just I don't know what the best I guess format would be I'm just wondering would would it be beneficial to have like periodic like some sort of forum um where like every month or whatever or every quarter where folks from organizations that are looking to adopt open Telemetry practices open Telemetry in general um have a chance to like interact with folks in the community for for like a pointed q a um to like just to sort of you know ease their concerns like or or would you all feel like this is too much of an overlap of what this group already does I'm just wondering um because I I do feel like just putting putting folks who are kind of like not really sure about this um in touch with with other folks who have gone through this or you know have some expertise in in the area I can like put them at ease I don't know my first thought is like it does have some overlap with this group um I mean I think I don't know maybe I would need to think about that a bit more well one thing like at Rhys mentioned like she's recording this uh meeting now which I think is good like people can go back and like get insight into the past which before I think was like a there's some good topics here that are brought up that that like aren't necessarily easy to get answers because there aren't really answers like they're they're more gray if you will um I don't know I would have to think about that yeah um no I think it's it's a very valid um observator concern um and yeah I would like to Noodle on this more as well um and that said I just realized well I realized a few minutes ago that I haven't been time boxing this so I apologize um but it sounds like um we might be good on this topic Dan was there anything else you wanted to add before we move to the next one no I'm good okay can I hop in real quick just share one last thought of course um this is kind of just a more General approach for trying out our organization but we're kind of trying to drive change across various laterals through the idea of like a service catalog and scorecard for services where various like subject matter experts can Define like a set of standards in a particular domain that should apply for services and kind of provide a rubric for those things so what does like you know sufficient or like get give a service of grading in a particular set of criteria right c a A plus right and you don't have to be responsible for implementing what is like an a look like in your service but you're allowed to you know you can define those standards and then the actual service owners that are responsible for that service can consult that rubric that scorecard kind of grade their own Services according and then kind of refer to whatever standardized documentation you provide as a subject matter expert for figuring out okay how do I take my service that is maybe a a great C and observability to like a a grade A right and so in that kind of a situation you're setting forth the standards you're providing a clear rubric that allows service owners to kind of grade themselves and you're providing more information if they're looking to kind of level up uh their service so this is kind of something that is very new at our organization we're going to try to drive change in a bunch of different places things like SRA practices how you tune your services and kubernetes code quality test coverage observability stuff so we're just gonna try that out and see if that makes it kind of easier to gamify making strides um and just improving services and things like that so I don't know if that's helpful at all but it kind of separates implementation from like leading the way and improving something all right Derek I don't know if you can see this but Dan has put a thumbs up oh cool yep alrighty so since the next two have equals how more votes we'll just go with the uh in order how are you dealing with clock slash time drift yeah these are actually all mine but today um yeah um yeah so like I uh I don't know I noticed well I noticed because our backend doesn't accept data that's like in the future I think it's 10 minutes in the future um obviously people are creating data points from the future because there are times on their servers are not correct um one I guess are people experiencing this problem like the naive side of me wants to just say well like they should just fix their servers which I do agree with um if you are experiencing this do you do you notify services do you implement something in the collector to to understand that it's happening uh uh yeah that so um I can probably talk a little bit about how jiggers deals with that um and the way that eager does or used to do is um it tries to detect well so I guess the first realization is that uh clock skew is going to happen um and you just have to account for it there is no way for clocks with synchrons or synchronized especially on a microservices architecture um and one way of of dealing with that is younger chance is um it first assumes that um you don't have asynchronous processing so one Trace is uh very synchronous so the the parent span or the very first span um and uh at most when the the the the is the last span finishes you know so and if that's not the case then Jaeger by default will try to adjust the parent spans of that span uh to be as big or as as long as the longest span that it has um it did generate a lot of confusion among users especially for users who do have a synchrance processing so much that we ended up doing a flag to disable this behavior on the eager side on the eager collector side and I think we at some point thought about not having that that behavior by default so only users who would know what they're doing would then enable the clock skew I can't remember what is the feature name but uh they wouldn't have this uh clock um fixing a feature enabled um on The Collector side we're not doing anything that I know of perhaps Alex Cannon can share uh if he knows whether we are doing something like that there but uh and I guess the short answer is um the owner of the of the the system that generates the Telemetry data is in a way better position to understand the clock drift then uh any any general purpose tool like The Collector so the collector cannot in a it's not in a very good position to detect and and fix this issue for you um so yeah great thank you um so like I don't know for to me at least and maybe this is not true but um like there's sort of like two use cases here there's like like a lower mild kind of clock skew where it's like I don't know a couple seconds or something and yeah sure maybe it makes the the tracing Vision like look a little awkward or something and then there's like like the really bad you know ones that are like in this case like I I know they were like 10 minutes off um so I I was thinking of like creating a processor that detects some you know incoming data point that is like some some like distance I don't know what the right word is some you know if it's more than five minutes or some threshold uh like just I don't know creating create a data point that like captures you know the service name or something and and like you know just just at least like identifies it like hey this is a this is something that's like way out of sync with what we consider real time I don't know if like something like that would be useful um right now our back end there's no good way to like correlate like the air that our back end throws with like the data that's coming through so I don't even know like what data is missing unless a service owner were to reach out to me um it could be specific to my back end maybe my use case um I know so I was just like maybe just like having more visibility into the issue would like be a first step does that sound like a weird use case like I don't know creating a processor that would kind of detect that I think it's definitely an interesting use case if you know you could have a processor that just detects things that are out in the future um I I would guess that the the hard part is you know you still wouldn't catch all of your clock drifts I guess you would catch a very obvious ones that you kind of know about um that you've seen in the past but yeah so I I guess then yeah that's like I guess because in my case I'm like dropping data because the back end's rejecting it it would be good to know that but if there was like say it was a 10 minute threshold and it was I don't know three minutes delayed and I didn't want to detect that or I couldn't detect that now it still cause issues with the interpretation of the data uh but it would be like a secondary concern like maybe I should just Shore up like the data loss if you will and then I don't know just be better at you know having having that like like drawsi is that how you pronounce your name having like the service owners sort of interpret the drift or the skew because they're that the experts like like you said are you um are you talking specifically about metrics or are you talking about any Telemetry data type um I would have to double check I want to say the data that I was looking at in this case was spans [Music] um but in my opinion it would be for for any type yeah so for metrics there's some metrics um um stores like you know every every single permittee is based storage uh will block new metrics that are from the past so not from the future I suppose but from the past um so you would have a a warning on some logs already because of that before using Primitives um for for traces uh it might be relatively easy to to find it out if the clock drift can be detected by looking at a trace right so by looking at the spans of a Trace and I guess it would not be too hard to to make a a processor that detects that now for logs that um for logs I don't know I I wouldn't even know if we would want to have something like that for logs um because you generate such a huge amount of logs on one specific server and supposedly all of the logs on that server are having the same timestamps for things are a lot at the same time um and uh so perhaps just yeah I don't know perhaps this data it's hard to think about logs in this case because it would be having a bunch of logs at once uh and then you have to compare those logs with a batch of logs from another server right coming from another server on The Collector so your your comparison base would be huge yeah I guess my my take there would be just do the simplest thing that would work which in this case if the log time stamps are still in the future you could you could flag it you know you could create a metric that would at least capture that information but again this would only capture clock drift that's in the future nothing nothing clear would happen if it was like drifting in the past yeah makes sense okay cool thanks for the input guys I'm good with this one recent other people don't have comments excellence alrighty best practices for bifurcating data in a pipeline for example have a filter processor with positive and negative condition Clarity around Fan in yeah so this is also mine and I don't really know how to phrase this in a in a simple way um maybe I'll tell you a really weird case use case that I have and [Music] yeah anyway so like I have data flowing um let's say I have an otlp I have a collector I have an otlp receiver I have some processors configured and then I have uh a back end let's call it back end a and then I have another pipeline also otlp receiver well let's just say we're using metrics here um some processors in backend B so I want to like have um so the same set of data is flowing into both of these pipelines and then for like pipeline one I want say just to have histograms go to backend a and for pipeline two I wanted to have non-histograms go to backend B is configuring two Pipelines um like that the right thing to do um I was trying to like it works like I have this configured it works I was thinking about like is this the most CPU or memory efficient in terms of like uh I'm not I'm not I guess like the fan and fan out of how like the receivers and and stuff work in the in the in the exporters work I was trying to like you know understand if I'm doing it in like the optimal way does that question make sense at all is that a use case anyone else has um I hope that made sense there's a new type of component for The Collector that is coming up and that's called the connector so it's it's uh so the collector right now has uh four types of components right extensions and then pipeline specific components like receivers processors and exporters and there's going to be a fourth one or a fifth uh which is connectors so connectors they can act as receivers and exporters so you have one pipeline that receives otlp for instance then there is some processing and um you end up with a with an exporter and exporter for that pipeline is going to be a connector now the connector then connects with another pipeline um and as a receiver right so it's an exporter in one Pipeline and a receiver on the next pipeline um so what you can do is you can you can translate signals so you can translate its bands to metrics for instance but in your case here it would work uh in a way that you have like one receiver and then uh two exporters or two connectors as part of the same pipeline one that is going to connect this data or this this Pipeline with one that is histogram specific and one that is going to connect with a non-histogram specific with the other one and then each one of those connectors would then be able to filter what is interesting to be passed through this disconnector and block everything else so it ended up with uh three pipelines in there and so one that receives the raw data one that deals with a histogram data and one that deals with other data so this is what we have planned for the future so the basic building blocks for that are are merged already so um from what I remember so Alex can correct me if I'm wrong um but uh the basic building blocks are there and it's it's we're all looking forward to seeing connectors being ready to be used because we have so many use cases in mind to implement now what what we have today for that specific case is the routing processor so we have a routing processor and it it doesn't really work the way that you that you that you want here but it it works in a very similar way so what we do is we route the data points based on their characteristics and we rush them to specific exporters but we do that by making another network connection so it's very inefficient but it works so those are the two possible solutions for that with the router I'll obviously look into this thank you I didn't know what it is maybe this is specific use case but what would be the advantage of like the router solution versus like having a filter processor that like you know having the two pipelines and having the filter processor just like get rid of a you know part of like the part A of the set or Part B of the set would there be a advantage to you either one of those I guess that would be a third solution yeah I guess um that's what I'm doing right now um and it's working uh there were some bugs in the filter processor that have been resolved so now it seems to be working perfectly as far as I know it just I I just felt like I was because I had like two receivers receiving the same data like I was doubling up on the memory that you know my collector was using uh I haven't done any profiling but maybe that's something I should do um but I'll definitely look at the at the routing processor um actually take a look at the connectors because I I've made a very similar question to the to the author of The connectors on the pr that they introduced and I think I think indeed the the answer is that the connector is using connectors you wouldn't only have one receiver and only only on the costume only one copy of the of the data point in memory it only keeps one copy of the data point to memory let me try to find a PR and you can get more information from there okay awesome that would be great thank you that sounds like a great feature I'll definitely follow that thank you yeah that's neat I haven't heard about the connectors so I'm interested to learn more as well well jurassi is getting us that PR does anyone else have anything they'd like to add or ask well we still have 10 minutes on the clock I I do if no one else does that would if someone else obviously go first since I've asked enough I think you're in the clarity and go ahead okay um I asked this a couple of these sessions ago um I was trying to I'm trying to find I had opened a GitHub issue um it was very large in scope so I don't think I have any responses on it um it was it was it was about like understanding more about um um I'll just pop this in the zoom chat uh if you're curious um it was more about just like figuring out like how to scale my collector deployment correctly in dressy I saw that you had like a post yesterday that you posted on one of the channels that talks a little bit about that um there were some other things I had put in here like um understanding like when the num consumers setting uh for instance should be modified um I'm curious if you guys have any like insight into like when I should be I guess horizontally scaling my pod versus when I should be modifying the configuration of uh an individual pod if that or an individual collector if that makes sense like how do I what wonder what criteria do I use to determine one verse versus the other does that make any sense it's probably a loaded question as well so are you asking like at what point should the configuration be relegated to the Pod versus to The Collector uh so like I have it like an HPA configured so like as my traffic increases you know presumably I spin up new pods um but why are there settings like the number of consumers like should I be is that is that purely for non you know deployments that can't automatically scale um more so like when when do I when do I create more collectors or when do I change collector specific configuration um I'm sorry I I missed the very beginning of the question because I was looking for for the issue I found the issue and I linked here and I linked directly to the comment that I've made that is close to your question um you can read then a dance and search that um but coming coming back to this question here I'm and forgive me if I'm if I'm missing soft some context from the very beginning of the question but um the way that I that I would I would say this the way that I would tackle is um if you only have stateless connect or or components in your collector you can then just dot use a specific metric and scale up or scale adding more more replicas would be the solution um the other answer to that is you know your workload um and you would probably have to think about that not only as in the scaling to attend the demands uh like like the the load that I have but also scaling to improve my high availability so if a node goes down what am I going to what what are the effects across my observability pipeline so how would I want to isolate failures on my observability pipeline so is it better to have one per namespace is that and is that good enough or um or should I have a a one per tenant or perhaps a a even within tenants in my cluster perhaps I should have different layers of collectors um because you know failing one specific branch of your collection of collectors isn't not going to be so critical as something that is going to fail only you know if you have everything going through one collector then it's really going to be a an epic failure at one point um I guess there's no easy answer to that um yeah I I know there's no like one size fits all I guess I was just trying to like okay like so obviously like I have some like bait like some bass throughput like you know that I want to kind of I want to serve like you know I get a certain amount of data points per second or whatever uh base and then like I get spikes right so I have to like have a minimum deployment that can handle those spikes yeah but that but then there are like some connections that are I think like like have like sticky sessions so I need to be careful about like if one of those things like burst real high then like a given you know a given pod or given collector if you will will will like I don't know have to be able to to handle that all right um so in most of the cases you're going to use grpc for the connection between collectors or between your workload and a collector um and the good thing about Char PC is when a connection fails uh it will attempt to connect to another um to another backhand automatically right so if you have your deployment done correctly and correctly here means on kubernetes you wouldn't be having a a headless surveys and your clients would be then connecting to the Headless service so that the the client has a list of known backends up front so whenever one of them fails it just fails over to the next one and uh so so that's one way of of um dealing with the sticky problem they sticky session problems so most other connections they are long-lived when we talk about the observability pipeline so there are grpc connections and grpc connections they are long-lived by Design which also means that if you have like only three collectors at the very beginning of your life cycle of your cluster for instance and then you keep adding more collectors to that but you don't increase the number of clients then you're not going to see any effects until the clients reconnect uh due to some failure right so you're not going to see any advantage uh the moments that you scale up you're only going to see advantages the moment things start to fail and then you start seeing the other collectors receive some traffic or when you have new clients so new clients are going to be load balanced through the to the new nodes so then you start seeing that um yeah so I guess that's there's that uh one other thing you may consider is um charting your pipeline based on the type of processing that it's doing all right so one pattern that we've seen before and uh that I've recorded at some point is having one pipeline per data point so you have one one Matrix pipeline you have one logs Pipeline and you have one traces pipeline because the types of workloads or you know the workload for metrics is way different than the workload for for a traces um the way that they work is really different so you might want to split by that and you also might want to split by the type of processing that you do on on the Telemetry data that has been generated by your workload so if you have more pii information being generated by nodes or by pods on one specific namespace then it makes sense to have one collector on that namespace with a specific um processor to remove this pii and then goes to the next layer of collectors that deals with more generic data yeah so you're only affecting like you're only slowing down for like that subset right exactly yeah okay yeah so yeah okay cool awesome and then I know we're like almost that time so just like harping on one specific setting that num consumers thing like is there when when would I ever want to change that do you know I'm sorry um consumers as in yeah there's on the otlp exporter there's a maybe this is two technical maybe I should just open an issue just for this uh there's a property called num consumers I think it's on otlp exporter um it seemed like I got better at the root book when I increased it um and I was just wondering what like why wouldn't I increase that like what trade-off am I sacrificing okay um sorry I see now so this is about it's about the signing queue uh this is a blocking queue and um the way that it works is um whenever things are being sent from from one place to another from from an exporter and into a back end um it's placed on a queue and then things are are picked from the queue like from like workers and uh this is you know number of consumers is basically the number of workers that are picking things up from the queue now um I'm not the author of this component here of this Helper but um I we had a similar component on Jager and I can tell you by experience that we don't actually know what is the optimal value for that you have to find that out by yourself um a and it's complicated because um in in Java for instance uh the number of workers would be typically uh closely related to the number of processors in a specific like CPU processors in a machine ACP using a machine now with Goal it's not like that so one one worker thread is not a thread it's a good routine which is not related to an OS threat a Linux thread so there's no relation between or no very explicit relation between a number of consumers with the number of of uh processors on a specific laptop or a machine server bare metal um so you have to play with that number I I think I kind of played with this information on on an article recently um and the idea is that the more um if you have backhands that are are taking very long to answer to you having more consumers here means that you have more HTTP connections with the backend that you're sending data to and it might be a problem with the back end that you're you're I mean if you're the backend is having trouble with a specific amount of connections and you're adding more connections you're adding more load to a server that is already overloaded in that case you want to decrease the number of consumers but what it means is it is processing less data um um simultaneously concurrently so what is effectively means is it it increases the concurrency of data being sent to the remote server so in theory it is related to the CPU number of CPUs that you have uh but at the same time I think it is more important to the back end that we are talking to okay yeah so awesome that's like super helpful I I think the main takeaway for me is like I should probably be doing some more specific testing around like my expected you know like how much data I'm receiving and expecting to send to my back ends and just you know trying to optimize for my specific use case rather than like having a generic uh this type of situation you know you should set it to X Y or Z absolutely okay awesome really really appreciate the Insight thank you Brad thank you thank you all all or a few minutes a few minutes or so I want to be respect to respect everyone's time um Dan thank you so much for all your questions um Alex gerasi it was great to have you on and um if anyone has any questions about these sessions or anything else they are curious myself Adriana as well as Rin are all on the end user working group so feel free to reach out to any of us on cnco slack um and for I I just have to drop real quick but um Alex and C wanted to see the kitten I'm just gonna do a quick kitten kitten show this is Taco thank you hi taco and all right thank you all so much [Music]

